{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"ai/","title":"TiDB for AI","text":"<p>TiDB is an open-source, distributed SQL database designed for modern AI applications, offering seamless scalability, real-time analytics, and unified storage for vectors, documents, knowledge graphs, operational data and more.</p> Python <p>TiDB provide a Python SDK and a series of integrations with popular AI frameworks to help developers build AI applications efficiently.</p> <p>To install the TiDB Python SDK, run the following command:</p> <pre><code>pip install pytidb\n</code></pre> <p>Integrations:</p> <ul> <li>AI Frameworks: LlamaIndex, LangChain</li> <li>ORM Libraries: SQLAlchemy, Django-ORM, Peewee</li> <li>AI Services: Bedrock</li> <li>Embedding Models/Services: JinaAI</li> </ul>"},{"location":"ai/#next-steps","title":"Next Steps","text":"<ul> <li>\ud83d\udcd6 Explore Concepts to understand the fundamentals of building modern AI applications</li> <li>\ud83d\ude80 Follow our Quickstart Guide to begin building your first AI application with TiDB</li> </ul>"},{"location":"ai/quickstart/","title":"Quickstart","text":"<p>In this guide, you will learn how to get started with vector search in TiDB using Python SDK. Follow along to build your first AI application working with TiDB.</p>"},{"location":"ai/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Go tidbcloud.com to create a TiDB Cloud Starter cluster for free or using tiup playground to a TiDB Self-Managed cluster for local testing.</li> </ul>"},{"location":"ai/quickstart/#installation","title":"Installation","text":"<p>pytidb is the official Python SDK for TiDB, designed to help developers build AI applications efficiently.</p> <p>To install the Python SDK, run the following command:</p> <pre><code>pip install pytidb\n</code></pre> <p>To use built-in embedding function, install the <code>models</code> extension (alternative):</p> <pre><code>pip install \"pytidb[models]\"\n</code></pre>"},{"location":"ai/quickstart/#connect-to-database","title":"Connect to database","text":"TiDB Cloud StarterTiDB Self-Managed <p>You can get these connection parameters from the TiDB Cloud console:</p> <ol> <li>Navigate to the Clusters page, and then click the name of your target cluster to go to its overview page.</li> <li>Click Connect in the upper-right corner. A connection dialog is displayed, with connection parameters listed.</li> </ol> <p>For example, if the connection parameters are displayed as follows:</p> <pre><code>HOST:     gateway01.us-east-1.prod.shared.aws.tidbcloud.com\nPORT:     4000\nUSERNAME: 4EfqPF23YKBxaQb.root\nPASSWORD: abcd1234\nDATABASE: test\nCA:       /etc/ssl/cert.pem\n</code></pre> <p>The corresponding Python code to connect to the TiDB Cloud Starter cluster would be as follows:</p> <pre><code>from pytidb import TiDBClient\n\nclient = TiDBClient.connect(\n    host=\"gateway01.us-east-1.prod.shared.aws.tidbcloud.com\",\n    port=4000,\n    username=\"4EfqPF23YKBxaQb.root\",\n    password=\"abcd1234\",\n    database=\"test\",\n)\n</code></pre> <p>Note: The preceding example is for demonstration purposes only. You need to fill in the parameters with your own values and keep them secure.</p> <p>Here is a basic example for connecting to a self-managed TiDB cluster:</p> <pre><code>from pytidb import TiDBClient\n\nclient = TiDBClient.connect(\n    host=\"localhost\",\n    port=4000,\n    username=\"root\",\n    password=\"\",\n    database=\"test\",\n    ensure_db=True,\n)\n</code></pre> <p>Tip: Please modify the connection parameters according to your actual deployment.</p> <p>Once connected, you can use the <code>client</code> object to operate tables, query data, and more. </p>"},{"location":"ai/quickstart/#create-an-embedding-function","title":"Create an embedding function","text":"<p>When working with embedding models, you can leverage the embedding function to automatically vectorize your data at both insertion and query stages. It natively supports popular embedding models like OpenAI, Jina AI, Hugging Face, Sentence Transformers, and others.</p> OpenAIJina AI <p>Go OpenAI platform to create your API key for embedding.</p> <pre><code>from pytidb.embeddings import EmbeddingFunction\n\ntext_embed = EmbeddingFunction(\n    model_name=\"openai/text-embedding-3-small\",\n    api_key=\"&lt;your-openai-api-key&gt;\",\n)\n</code></pre> <p>Go Jina AI to create your API key for embedding.</p> <pre><code>from pytidb.embeddings import EmbeddingFunction\n\ntext_embed = EmbeddingFunction(\n    model_name=\"jina/jina-embeddings-v3\",\n    api_key=\"&lt;your-jina-api-key&gt;\",\n)\n</code></pre>"},{"location":"ai/quickstart/#create-a-table","title":"Create a table","text":"<p>As an example, create a table named <code>chunks</code> with the following columns:</p> <ul> <li><code>id</code> (int): the ID of the chunk.</li> <li><code>text</code> (text): the text content of the chunk.</li> <li><code>text_vec</code> (vector): the vector embeddings of the text.</li> <li><code>user_id</code> (int): the ID of the user who created the chunk.</li> </ul> Python <pre><code>from pytidb.schema import TableModel, Field, VectorField\n\nclass Chunk(TableModel):\n    id: int | None = Field(default=None, primary_key=True)\n    text: str = Field()\n    text_vec: list[float] = text_embed.VectorField(source_field=\"text\")\n    user_id: int = Field()\n\ntable = client.create_table(schema=Chunk, if_exists=\"overwrite\")\n</code></pre> <p>Once created, you can use the <code>table</code> object to insert data, search data, and more.</p>"},{"location":"ai/quickstart/#insert-data","title":"Insert Data","text":"<p>Now let's add some sample data to our table. </p> <pre><code>table.bulk_insert([\n    # \ud83d\udc47 The text will be automatically embedded and populated into the `text_vec` field.\n    Chunk(text=\"PyTiDB is a Python library for developers to connect to TiDB.\", user_id=2),\n    Chunk(text=\"LlamaIndex is a framework for building AI applications.\", user_id=2),\n    Chunk(text=\"OpenAI is a company and platform that provides AI models service and tools.\", user_id=3),\n])\n</code></pre>"},{"location":"ai/quickstart/#search-for-nearest-neighbors","title":"Search for nearest neighbors","text":"<p>To search for nearest neighbors of a given query, you can use the <code>table.search()</code> method, it will perform a vector search by default.</p> <pre><code>table.search(\n    # \ud83d\udc47 Pass the query text directly, it will be embedded to a query vector automatically.\n    \"A library for my artificial intelligence software\"\n)\n.limit(3).to_list()\n</code></pre> <p>In this example, vector search compares the query vector with the stored vectors in the <code>text_vec</code> field of the <code>chunks</code> table and returns the top 3 most semantically relevant results based on similarity scores.</p> <p>The closer <code>_distance</code> means the more similar the two vectors are.</p> Expected output<pre><code>[\n    {\n        'id': 2,\n        'text': 'LlamaIndex is a framework for building AI applications.',\n        'text_vec': [...],\n        'user_id': 2,\n        '_distance': 0.5719928358786761,\n        '_score': 0.4280071641213239\n    },\n    {\n        'id': 3,\n        'text': 'OpenAI is a company and platform that provides AI models service and tools.',\n        'text_vec': [...],\n        'user_id': 3,\n        '_distance': 0.603133726213383,\n        '_score': 0.396866273786617\n    },\n    {\n        'id': 1,\n        'text': 'PyTiDB is a Python library for developers to connect to TiDB.',\n        'text_vec': [...],\n        'user_id': 2,\n        '_distance': 0.6202191842385758,\n        '_score': 0.3797808157614242\n    }\n]\n</code></pre>"},{"location":"ai/quickstart/#delete-data","title":"Delete data","text":"<p>To delete a specific row from the table, you can use the <code>table.delete()</code> method:</p> <pre><code>table.delete({\n    \"id\": 1\n})\n</code></pre>"},{"location":"ai/quickstart/#drop-table","title":"Drop table","text":"<p>When you no longer need a table, you can drop it using the <code>client.drop_table()</code> method:</p> <pre><code>client.drop_table(\"chunks\")\n</code></pre>"},{"location":"ai/quickstart/#next-steps","title":"Next steps","text":"<ul> <li>Learn more details about Vector Search, Fulltext Search and Hybrid Search in TiDB.</li> </ul>"},{"location":"ai/concepts/vector-search/","title":"Vector Search","text":"<p>Vector search offers a powerful solution for semantic similarity searches across diverse data types, such as documents, images, audio, and video. It allows developers to leverage their MySQL expertise to build scalable applications enriched with generative AI capabilities, simplifying the integration of advanced search functionality.</p> <p>Note</p> <p>The vector search feature is only available for TiDB Self-Managed clusters and TiDB Cloud Starter clusters.</p>"},{"location":"ai/concepts/vector-search/#concepts","title":"Concepts","text":"<p>Vector search is a search method that prioritizes the meaning of your data to deliver relevant results.</p> <p>Unlike traditional full-text search, which relies on exact keyword matching and word frequency, vector search converts various data types (such as text, images, or audio) into high-dimensional vectors and queries based on the similarity between these vectors. This search method captures the semantic meaning and contextual information of the data, leading to a more precise understanding of user intent.</p> <p>Even when the search terms do not exactly match the content in the database, vector search can still provide results that align with the user's intent by analyzing the semantics of the data.</p> <p>For example, a full-text search for \"a swimming animal\" only returns results containing these exact keywords. In contrast, vector search can return results for other swimming animals, such as fish or ducks, even if these results do not contain the exact keywords.</p>"},{"location":"ai/concepts/vector-search/#vector-embedding","title":"Vector embedding","text":"<p>A vector embedding, also known as an embedding, is a sequence of numbers that represents real-world objects in a high-dimensional space. It captures the meaning and context of unstructured data, such as documents, images, audio, and videos.</p> <p>Vector embeddings are essential in machine learning and serve as the foundation for semantic similarity searches.</p> <p>TiDB introduces Vector data types and Vector search index designed to optimize the storage and retrieval of vector embeddings, enhancing their use in AI applications. You can store vector embeddings in TiDB and perform vector search queries to find the most relevant data using these data types.</p>"},{"location":"ai/concepts/vector-search/#embedding-model","title":"Embedding model","text":"<p>Embedding models are algorithms that transform data into vector embeddings.</p> <p>Choosing an appropriate embedding model is crucial for ensuring the accuracy and relevance of semantic search results. For unstructured text data, you can find top-performing text embedding models on the Massive Text Embedding Benchmark (MTEB) Leaderboard.</p> <p>To learn how to generate vector embeddings for your specific data types, refer to integration tutorials or examples of embedding models.</p>"},{"location":"ai/concepts/vector-search/#how-vector-search-works","title":"How vector search works","text":"<p>After converting raw data into vector embeddings and storing them in TiDB, your application can execute vector search queries to find the data most semantically or contextually relevant to a user's query.</p> <p>TiDB vector search identifies the top-k nearest neighbor (KNN) vectors by using a distance function to calculate the distance between the given vector and vectors stored in the database. The vectors closest to the given vector in the query represent the most similar data in meaning.</p> <p></p> <p>As a relational database with integrated vector search capabilities, TiDB enables you to store data and their corresponding vector representations (that is, vector embeddings) together in one database. You can choose any of the following ways for storage:</p> <ul> <li>Store data and their corresponding vector representations in different columns of the same table.</li> <li>Store data and their corresponding vector representation in different tables. In this way, you need to use <code>JOIN</code> queries to combine the tables when retrieving data.</li> </ul>"},{"location":"ai/concepts/vector-search/#use-cases","title":"Use cases","text":""},{"location":"ai/concepts/vector-search/#retrieval-augmented-generation-rag","title":"Retrieval-Augmented Generation (RAG)","text":"<p>Retrieval-Augmented Generation (RAG) is an architecture designed to optimize the output of Large Language Models (LLMs). By using vector search, RAG applications can store vector embeddings in the database and retrieve relevant documents as additional context when the LLM generates responses, thereby improving the quality and relevance of the answers.</p>"},{"location":"ai/concepts/vector-search/#semantic-search","title":"Semantic search","text":"<p>Semantic search is a search technology that returns results based on the meaning of a query, rather than simply matching keywords. It interprets the meaning across different languages and various types of data (such as text, images, and audio) using embeddings. Vector search algorithms then use these embeddings to find the most relevant data that satisfies the user's query.</p>"},{"location":"ai/concepts/vector-search/#recommendation-engine","title":"Recommendation engine","text":"<p>A recommendation engine is a system that proactively suggests content, products, or services that are relevant and personalized to users. It accomplishes this by creating embeddings that represent user behavior and preferences. These embeddings help the system identify similar items that other users have interacted with or shown interest in. This increases the likelihood that the recommendations will be both relevant and appealing to the user.</p>"},{"location":"ai/examples/","title":"Demo Gallery","text":"Demo Gallery <p>       Explore hands-on demos showcasing how TiDB empowers AI applications. Get started quickly with TiDB Cloud Starter to build your own AI-powered solutions.      </p> Categories Featured Getting Started Search &amp; Retrieval AI Applications \u2b50 Featured Image Search <p>                 Build an image search application using multimodal embeddings for both text-to-image and image-to-image search.               </p> RAG <p>                 Build a RAG application that combines document retrieval with language generation.               </p> Memory <p>                 Implement conversation memory for chatbots and conversational AI applications.               </p> \ud83d\ude80 Getting Started \u2699\ufe0f Basic Usage <p>                 Learn fundamental PyTiDB operations including database connection, table creation, and data manipulation.               </p> \ud83e\udd16 Auto Embedding <p>                 Automatically generate embeddings for your text data using built-in embedding models.               </p> \ud83d\udd0d Search &amp; Retrieval Vector Search <p>                 Implement semantic search using vector embeddings to find similar content.               </p> Fulltext Search <p>                 Perform traditional text search using MySQL fulltext search capabilities.               </p> Hybrid Search <p>                 Combine vector search and fulltext search for more comprehensive results.               </p> Image Search <p>                 Build an image search application using multimodal embeddings for both text-to-image and image-to-image search.               </p> \ud83e\udd16 AI Applications RAG <p>                 Build a RAG application that combines document retrieval with language generation.               </p> Memory <p>                 Implement conversation memory for chatbots and conversational AI applications.               </p> \ud83d\udcac Text2SQL <p>                 Convert natural language queries into SQL statements using AI models.               </p> Ready to build your AI application? <p>           Start your AI journey with TiDB Cloud Starter. Follow our quickstart guide to build your first AI-powered application in minutes, or explore specific examples for your use case.         </p> Try TiDB Cloud Starter View Quickstart Guide"},{"location":"ai/examples/auto-embedding-with-pytidb/","title":"Auto Embedding Demo","text":"<p>This example showcases how to use the auto embedding feature with PyTiDB Client.</p> <ul> <li>Connect to TiDB with PyTiDB Client</li> <li>Define a table with a VectorField configured for automatic embedding</li> <li>Insert plain text data, embeddings are populated automatically in the background</li> <li>Run vector searches with natural language queries, embedding happens transparently</li> </ul>"},{"location":"ai/examples/auto-embedding-with-pytidb/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>A TiDB Cloud Starter cluster: Create a free cluster here: tidbcloud.com \u2197\ufe0f</li> </ul>"},{"location":"ai/examples/auto-embedding-with-pytidb/#how-to-run","title":"How to run","text":"<p>Step 1: Clone the repository</p> <pre><code>git clone https://github.com/pingcap/pytidb.git\ncd pytidb/examples/auto_embedding/\n</code></pre> <p>Step 2: Install the required packages</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -r reqs.txt\n</code></pre> <p>Step 3: Set up environment to connect to database</p> <p>Go to TiDB Cloud console to get the connection parameters and set up the environment variable like this:</p> <pre><code>cat &gt; .env &lt;&lt;EOF\nTIDB_HOST={gateway-region}.prod.aws.tidbcloud.com\nTIDB_PORT=4000\nTIDB_USERNAME={prefix}.root\nTIDB_PASSWORD={password}\nTIDB_DATABASE=test\n\n# Using TiDB Cloud Free embedding model by default, which is no required to set up any API key\nEMBEDDING_PROVIDER=tidbcloud_free\nEOF\n</code></pre> <p>Step 4: Run the demo</p> <pre><code>python main.py\n</code></pre> <p>Expected output:</p> <pre><code>=== Define embedding function ===\nEmbedding function (model id: tidbcloud_free/amazon/titan-embed-text-v2) defined\n\n=== Define table schema ===\nTable created\n\n=== Truncate table ===\nTable truncated\n\n=== Insert sample data ===\nInserted 3 chunks\n\n=== Perform vector search ===\nid: 1, text: TiDB is a distributed database that supports OLTP, OLAP, HTAP and AI workloads., distance: 0.30373281240458805\nid: 2, text: PyTiDB is a Python library for developers to connect to TiDB., distance: 0.422506501973434\nid: 3, text: LlamaIndex is a Python library for building AI-powered applications., distance: 0.5267239638442787\n</code></pre>"},{"location":"ai/examples/auto-embedding-with-pytidb/#related-resources","title":"Related Resources","text":"<ul> <li>Source Code: View on GitHub</li> <li> <p>Category: Getting-Started</p> </li> <li> <p>Description: Automatically generate embeddings for your text data using built-in embedding models.</p> </li> </ul> <p>\ud83c\udfe0 Back to Demo Gallery </p>"},{"location":"ai/examples/basic-with-pytidb/","title":"Basic CRUD Demo","text":"<p>This example demonstrates basic CRUD (Create, Read, Update, Delete) operations with PyTiDB.</p> <ul> <li>Use PyTiDB Client to connect to TiDB</li> <li>Create a table with text, vector, and JSON columns</li> <li>Perform basic CRUD operations on data</li> </ul>"},{"location":"ai/examples/basic-with-pytidb/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>A TiDB Cloud Starter cluster: Create a free cluster here: tidbcloud.com \u2197\ufe0f</li> </ul>"},{"location":"ai/examples/basic-with-pytidb/#how-to-run","title":"How to run","text":"<p>Step 1: Clone the repository to local</p> <pre><code>git clone https://github.com/pingcap/pytidb.git\ncd pytidb/examples/basic/\n</code></pre> <p>Step 2: Install the required packages</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -r reqs.txt\n</code></pre> <p>Step 3: Set up environment to connect to database</p> <p>Go to TiDB Cloud console and get the connection parameters, then set up the environment variable like this:</p> <pre><code>cat &gt; .env &lt;&lt;EOF\nTIDB_HOST={gateway-region}.prod.aws.tidbcloud.com\nTIDB_PORT=4000\nTIDB_USERNAME={prefix}.root\nTIDB_PASSWORD={password}\nTIDB_DATABASE=test\nEOF\n</code></pre> <p>Step 4: Run the demo</p> <pre><code>python main.py\n</code></pre> <p>Expected output:</p> <pre><code>=== CREATE TABLE ===\nTable created\n\n=== TRUNCATE TABLE ===\nTable truncated\n\n=== CREATE ===\nCreated 3 items\n\n=== READ ===\nID: 1, Content: TiDB is a distributed SQL database, Metadata: {'category': 'database'}\nID: 2, Content: GPT-4 is a large language model, Metadata: {'category': 'llm'}\nID: 3, Content: LlamaIndex is a Python library for building AI-powered applications, Metadata: {'category': 'rag'}\n\n=== UPDATE ===\nUpdated item #1\nAfter update - ID: 1, Content: TiDB Cloud Starter is a fully-managed, auto-scaling cloud database service, Metadata: {'category': 'dbass'}\n\n=== DELETE ===\nDeleted item #2\n\n=== FINAL STATE ===\nID: 1, Content: TiDB Cloud Starter is a fully-managed, auto-scaling cloud database service, Metadata: {'category': 'dbass'}\nID: 3, Content: LlamaIndex is a Python library for building AI-powered applications, Metadata: {'category': 'rag'}\n\n=== COUNT ROWS ===\nNumber of rows: 2\n\n=== DROP TABLE ===\nTable dropped\n\nBasic CRUD operations completed!\n</code></pre>"},{"location":"ai/examples/basic-with-pytidb/#related-resources","title":"Related Resources","text":"<ul> <li>Source Code: View on GitHub</li> <li> <p>Category: Getting-Started</p> </li> <li> <p>Description: Learn fundamental PyTiDB operations including database connection, table creation, and data manipulation.</p> </li> </ul> <p>\ud83c\udfe0 Back to Demo Gallery </p>"},{"location":"ai/examples/fulltext-search-with-pytidb/","title":"Fulltext Search Example","text":"<p>This example demonstrates how to build a E-commerce product search application using TiDB's full-text search feature with multilingual support. Users can search for products by keywords in their preferred language.</p> <p> <p>E-commerce product search with full-text search</p> </p>"},{"location":"ai/examples/fulltext-search-with-pytidb/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>A TiDB Cloud Starter cluster: Create a free cluster here: tidbcloud.com \u2197\ufe0f</li> </ul>"},{"location":"ai/examples/fulltext-search-with-pytidb/#how-to-run","title":"How to run","text":"<p>Step 1: Clone the repository to local</p> <pre><code>git clone https://github.com/pingcap/pytidb.git\ncd pytidb/examples/fulltext_search/;\n</code></pre> <p>Step 2: Install the required packages and setup environment</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -r reqs.txt\n</code></pre> <p>Step 3: Set up environment to connect to database</p> <p>Go to the TiDB Cloud console, create a new cluster if you don't have one, and then get the connection parameters on the connection dialog.</p> <pre><code>cat &gt; .env &lt;&lt;EOF\nTIDB_HOST={gateway-region}.prod.aws.tidbcloud.com\nTIDB_PORT=4000\nTIDB_USERNAME={prefix}.root\nTIDB_PASSWORD={password}\nTIDB_DATABASE=pytidb_fulltext_demo\nEOF\n</code></pre> <p>Step 4: Run the Streamlit app</p> <pre><code>streamlit run app.py\n</code></pre> <p>Step 5: open the browser and visit <code>http://localhost:8501</code></p>"},{"location":"ai/examples/fulltext-search-with-pytidb/#related-resources","title":"Related Resources","text":"<ul> <li>Source Code: View on GitHub</li> <li> <p>Category: Search</p> </li> <li> <p>Description: Perform traditional text search using MySQL fulltext search capabilities.</p> </li> </ul> <p>\ud83c\udfe0 Back to Demo Gallery </p>"},{"location":"ai/examples/hybrid-search-with-pytidb/","title":"Hybrid Search Demo","text":"<p>In this demo, we will show you how to use hybrid search to combine vector search and full-text search on a set of documents.</p> <p> <p>TiDB Hybrid Search Demo</p> </p>"},{"location":"ai/examples/hybrid-search-with-pytidb/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>TiDB database instance (\ud83d\udc49 Create a free TiDB Serverless Cluster)</li> <li>OpenAI API key (Go to OpenAI to get the API key)</li> </ul> <p>Note</p> <p>Currently, full-text search is only available for the following product option and region:</p> <ul> <li>TiDB Cloud Starter: Frankfurt (eu-central-1), Singapore (ap-southeast-1)</li> </ul>"},{"location":"ai/examples/hybrid-search-with-pytidb/#how-to-run","title":"How to run","text":"<p>Step 1: Clone the repository</p> <pre><code>git clone https://github.com/pingcap/pytidb.git\ncd pytidb/examples/hybrid_search;\n</code></pre> <p>Step 2: Install the required packages and setup environment</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -r reqs.txt\n</code></pre> <p>Step 3: Set up environment to connect to storage</p> <p>If you are using TiDB Cloud, you can find the connection parameters in the TiDB Cloud console.</p> <pre><code>cat &gt; .env &lt;&lt;EOF\nTIDB_HOST=localhost\nTIDB_PORT=4000\nTIDB_USERNAME=root\nTIDB_PASSWORD=\nTIDB_DATABASE=pytidb_hybrid_demo\nOPENAI_API_KEY=&lt;your-openai-api-key&gt;\nEOF\n</code></pre> <p>Step 4: Run the demo</p> <p>Option 1: Run the Streamlit app</p> <p>If you want to check the demo with a web UI, you can run the following command:</p> <pre><code>streamlit run app.py\n</code></pre> <p>Open the browser and visit <code>http://localhost:8501</code></p> <p>Option 2: Run the demo script</p> <p>If you want to check the demo with a script, you can run the following command:</p> <pre><code>python example.py\n</code></pre> <p>Expected output:</p> <pre><code>=== CONNECT TO TIDB ===\nConnected to TiDB.\n\n=== CREATE TABLE ===\nTable created.\n\n=== INSERT SAMPLE DATA ===\nInserted 3 rows.\n\n=== PERFORM HYBRID SEARCH ===\nSearch results:\n[\n    {\n        \"_distance\": 0.4740166257687124,\n        \"_match_score\": 1.6804268,\n        \"_score\": 0.03278688524590164,\n        \"id\": 60013,\n        \"text\": \"TiDB is a distributed database that supports OLTP, OLAP, HTAP and AI workloads.\"\n    },\n    {\n        \"_distance\": 0.6428459116216618,\n        \"_match_score\": 0.78427225,\n        \"_score\": 0.03200204813108039,\n        \"id\": 60015,\n        \"text\": \"LlamaIndex is a Python library for building AI-powered applications.\"\n    },\n    {\n        \"_distance\": 0.641581407158715,\n        \"_match_score\": null,\n        \"_score\": 0.016129032258064516,\n        \"id\": 60014,\n        \"text\": \"PyTiDB is a Python library for developers to connect to TiDB.\"\n    }\n]\n</code></pre>"},{"location":"ai/examples/hybrid-search-with-pytidb/#related-resources","title":"Related Resources","text":"<ul> <li>Source Code: View on GitHub</li> <li> <p>Category: Search</p> </li> <li> <p>Description: Combine vector search and fulltext search for more comprehensive results.</p> </li> </ul> <p>\ud83c\udfe0 Back to Demo Gallery </p>"},{"location":"ai/examples/image-search-with-pytidb/","title":"Pet Image Search Demo","text":"<p>This example showcases how to build a powerful image search application by combining TiDB's vector search capabilities with multimodal embedding models.</p> <p>With just a few lines of code, you can create an intelligent search system that understands both text and images.</p> <ul> <li>\ud83d\udd0d Text-to-Image Search: Find the perfect pet photos by describing what you're looking for in natural language - from \"fluffy orange cat\"</li> <li>\ud83d\uddbc\ufe0f Image-to-Image Search: Upload a photo and instantly discover visually similar pets based on breed, color, pose and more</li> </ul> <p> <p>Pet image search via multimodal embeddings</p> </p>"},{"location":"ai/examples/image-search-with-pytidb/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>A TiDB Cloud Starter cluster: Create a free cluster here: tidbcloud.com \u2197\ufe0f</li> <li>Jina AI API Key: Get your free API key at jina.ai Embeddings \u2197\ufe0f</li> </ul>"},{"location":"ai/examples/image-search-with-pytidb/#how-to-run","title":"How to run","text":"<p>Step 1: Clone the repository to local</p> <pre><code>git clone https://github.com/pingcap/pytidb.git\ncd pytidb/examples/image_search/\n</code></pre> <p>Step 2: Install the required packages</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # Windows: .venv\\Scripts\\activate\npip install -r reqs.txt\n</code></pre> <p>Step 3: Set up environment variables</p> <p>Go to TiDB Cloud console and get the connection parameters, then set up the environment variable like this:</p> <pre><code>cat &gt; .env &lt;&lt;EOF\nTIDB_HOST={gateway-region}.prod.aws.tidbcloud.com\nTIDB_PORT=4000\nTIDB_USERNAME={prefix}.root\nTIDB_PASSWORD={password}\nTIDB_DATABASE=test\n\nJINA_AI_API_KEY={your-jina-ai-api-key}\nEOF\n</code></pre> <p>Step 3: Download and extract the dataset</p> <p>In this demo, we will use the Oxford Pets dataset to load pet images to the database for search.</p> <p>For Linux/MacOS:</p> <pre><code># Download the dataset\ncurl -L -o oxford_pets.tar.gz \"https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz\"\n\n# Extract the dataset\nmkdir -p oxford_pets\ntar -xzf oxford_pets.tar.gz -C oxford_pets\n</code></pre> <p>Step 4: Run the app</p> <pre><code>streamlit run app.py\n</code></pre> <p>Open <code>http://localhost:8501</code> in your browser.</p> <p>Step 5: Load data</p> <p>In the sample app, you can click the Load Sample Data button to load some sample data to the database.</p> <p>Or if you want to load all the data in the Oxford Pets dataset, click the Load All Data button.</p> <p>Step 6: Search</p> <ol> <li>Select the Search type in the sidebar</li> <li>Input a text description of the pet you're looking for, or upload a photo of a dog or cat</li> <li>Click the Search button</li> </ol>"},{"location":"ai/examples/image-search-with-pytidb/#related-resources","title":"Related Resources","text":"<ul> <li>Source Code: View on GitHub</li> <li> <p>Category: Search</p> </li> <li> <p>Description: Build an image search application using multimodal embeddings for both text-to-image and image-to-image search.</p> </li> </ul> <p>\ud83c\udfe0 Back to Demo Gallery </p>"},{"location":"ai/examples/memory-with-pytidb/","title":"AI Agent Memory Demo","text":"<p>This example showcases how to build an intelligent AI agent with persistent memory powered by TiDB's vector search capabilities.</p> <p>With just a few lines of code, you can create a conversational AI that remembers past interactions and builds context over time.</p> <ul> <li>\ud83e\udde0 Persistent Memory: The AI remembers conversations across sessions and user interactions</li> <li>\ud83d\udcac Interactive Chat: Both web interface and command-line options for flexible interaction</li> <li>\ud83d\udc64 Multi-User Support: Different users can have separate memory contexts</li> <li>\ud83d\udd0d Real-Time Memory Viewing: Visual display of all stored memories in the web interface</li> </ul> <p> <p>AI Agent with memory powered by TiDB</p> </p>"},{"location":"ai/examples/memory-with-pytidb/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>A TiDB Cloud Starter cluster: Create a free cluster here: tidbcloud.com \u2197\ufe0f</li> <li>OpenAI API Key: Get your API key at OpenAI Platform \u2197\ufe0f</li> </ul>"},{"location":"ai/examples/memory-with-pytidb/#how-to-run","title":"How to run","text":"<p>Step 1: Clone the repository to local</p> <pre><code>git clone https://github.com/pingcap/pytidb.git\ncd pytidb/examples/memory/\n</code></pre> <p>Step 2: Install the required packages</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # Windows: .venv\\Scripts\\activate\npip install -r reqs.txt\n</code></pre> <p>Step 3: Set up environment variables</p> <p>Go to TiDB Cloud console and get the connection parameters, then set up the environment variable like this:</p> <pre><code>cat &gt; .env &lt;&lt;EOF\nTIDB_HOST={gateway-region}.prod.aws.tidbcloud.com\nTIDB_PORT=4000\nTIDB_USERNAME={prefix}.root\nTIDB_PASSWORD={password}\nTIDB_DATABASE=test\n\nOPENAI_API_KEY={your-openai-api-key}\nEOF\n</code></pre> <p>Step 4: Run the application</p> <p>Choose one of the following options:</p> <p>Option 1: Launch Web Application:</p> <pre><code>streamlit run app.py\n</code></pre> <p>Visit <code>http://localhost:8501</code> in your browser and follow the instructions in the Interact with memory in Web Application section to start interacting with the memory-enabled AI assistant.</p> <p>Option 2: Run Command Line Application:</p> <pre><code>python main.py\n</code></pre> <p>Follow the instructions in the Interact with memory in Command Line Application section to start interacting with the memory-enabled AI assistant.</p>"},{"location":"ai/examples/memory-with-pytidb/#interact-with-memory-in-web-application","title":"Interact with memory in Web Application","text":"<p>In the web application, you can interact with the AI assistant, the user interface includes:</p> <ul> <li>Sidebar: User settings and chat list.</li> <li>Main chat area: Chat interface with the AI assistant.</li> <li>Memory viewer: Real-time memory viewer showing stored facts.</li> </ul> <p>You can follow the following steps to check how the memory works:</p> <ol> <li>Introduce yourself in the default chat session. For example, \"Hello, I am John. I work as a software engineer and love guitar.\"</li> <li>You can see the information you provided in the memory viewer.</li> <li>Click New chat in the sidebar to start a new chat session.</li> <li>Ask \"Who am I?\" in the new chat session. The AI will recall your information from previous conversations.</li> </ol>"},{"location":"ai/examples/memory-with-pytidb/#interact-with-memory-in-command-line-application","title":"Interact with memory in Command Line Application","text":"<p>In the command line application, you can interact with the AI assistant and introduce yourself.</p> <p>Example conversation:</p> <pre><code>Chat with AI (type 'exit' to quit)\nYou: Hello, I am Mini256.\nAI: Hello, Mini256! How can I assist you today?\nYou: I am working at PingCAP.\nAI: That's great to hear, Mini256! PingCAP is known for its work on distributed databases, particularly TiDB. How's your experience been working there?\nYou: I am developing pytidb (A Python SDK for TiDB) which helps developers easy to connect to TiDB.\nAI: That sounds like a great project, Mini256! Developing a Python SDK for TiDB can make it much easier for developers to integrate with TiDB and interact with it using Python. If you need any advice on best practices, libraries to use, or specific features to implement, feel free to ask!\nYou: exit\nGoodbye!\n</code></pre> <p>After the first conversation, the AI assistant will remember the information you provided and use it to answer future questions.</p> <p>Now, you can start a new chat session and ask the AI assistant \"Who am I?\".</p> <p>Example conversation in another chat session:</p> <pre><code>Chat with AI (type 'exit' to quit)\nYou: Who am I?\nAI: You are Mini256, and you work at PingCAP, where you are developing pytidb, a Python SDK for TiDB to assist developers in easily connecting to TiDB.\nYou: exit\nGoodbye!\n</code></pre> <p>As you can see, the AI assistant remembers you across sessions!</p>"},{"location":"ai/examples/memory-with-pytidb/#related-resources","title":"Related Resources","text":"<ul> <li>Source Code: View on GitHub</li> <li> <p>Category: Ai-Apps</p> </li> <li> <p>Description: Implement conversation memory for chatbots and conversational AI applications.</p> </li> </ul> <p>\ud83c\udfe0 Back to Demo Gallery </p>"},{"location":"ai/examples/rag-with-pytidb/","title":"RAG Example","text":"<p>This example demonstrates how to use PyTiDB to build a minimal RAG application.</p> <ul> <li>Use Ollama to deploy local embedding model and LLM model</li> <li>Use Streamlit to build a Web UI for the RAG application</li> <li>Use PyTiDB to build a minimal RAG application</li> </ul> <p> <p>RAG application built with PyTiDB</p> </p>"},{"location":"ai/examples/rag-with-pytidb/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>A TiDB Cloud Starter cluster: Create a free cluster here: tidbcloud.com \u2197\ufe0f</li> <li>Ollama: You can install it from Ollama \u2197\ufe0f</li> </ul>"},{"location":"ai/examples/rag-with-pytidb/#how-to-run","title":"How to run","text":"<p>Step 1: Prepare the inference API</p> <p>Pull the embedding and LLM model via ollama CLI:</p> <pre><code>ollama pull mxbai-embed-large\nollama pull gemma3:4b\nollama run gemma3:4b\n</code></pre> <p>Test the <code>/embed</code> and <code>/generate</code> endpoints to make sure they are running:</p> <pre><code>curl http://localhost:11434/api/embed -d '{\n  \"model\": \"mxbai-embed-large\",\n  \"input\": \"Llamas are members of the camelid family\"\n}'\n</code></pre> <pre><code>curl http://localhost:11434/api/generate -d '{\n  \"model\": \"gemma3:4b\",\n  \"prompt\": \"Hello, Who are you?\"\n}'\n</code></pre> <p>Step 2: Clone the repository to local</p> <pre><code>git clone https://github.com/pingcap/pytidb.git\ncd pytidb/examples/rag/;\n</code></pre> <p>Step 3: Install the required packages and setup environment</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -r reqs.txt\n</code></pre> <p>Step 4: Set up environment to connect to database</p> <p>Go to TiDB Cloud console and get the connection parameters, then set up the environment variable like this:</p> <pre><code>cat &gt; .env &lt;&lt;EOF\nTIDB_HOST={gateway-region}.prod.aws.tidbcloud.com\nTIDB_PORT=4000\nTIDB_USERNAME={prefix}.root\nTIDB_PASSWORD={password}\nTIDB_DATABASE=test\nEOF\n</code></pre> <p>Step 5: Run the Streamlit app</p> <pre><code>streamlit run main.py\n</code></pre> <p>Step 6: Open the browser and visit <code>http://localhost:8501</code></p>"},{"location":"ai/examples/rag-with-pytidb/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ai/examples/rag-with-pytidb/#502-bad-gateway-error","title":"<code>502 Bad Gateway</code> Error","text":"<p>Try to disable the global proxy settings.</p>"},{"location":"ai/examples/rag-with-pytidb/#related-resources","title":"Related Resources","text":"<ul> <li>Source Code: View on GitHub</li> <li> <p>Category: Ai-Apps</p> </li> <li> <p>Description: Build a RAG application that combines document retrieval with language generation.</p> </li> </ul> <p>\ud83c\udfe0 Back to Demo Gallery </p>"},{"location":"ai/examples/text2sql-with-pytidb/","title":"Text2SQL Demo","text":"<p>This demo showcases an AI-powered interface that converts natural language questions into SQL queries and executes them against TiDB. Built with PyTiDB, OpenAI GPT, and Streamlit, it provides a seamless way to interact with your database using plain English.</p>"},{"location":"ai/examples/text2sql-with-pytidb/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>A TiDB Cloud Starter cluster: Create a free cluster here: tidbcloud.com \u2197\ufe0f</li> <li>OpenAI API Key: Get your API key at OpenAI Platform \u2197\ufe0f</li> </ul>"},{"location":"ai/examples/text2sql-with-pytidb/#how-to-run","title":"How to run","text":"<p>Step 1: Clone the repository</p> <pre><code>git clone https://github.com/pingcap/pytidb.git\ncd pytidb/examples/text2sql/;\n</code></pre> <p>Step 2: Install the required packages</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -r reqs.txt\n</code></pre> <p>Step 3: Run the Streamlit app</p> <pre><code>streamlit run app.py\n</code></pre> <p>Step 4: Run streamlit app</p> <p>Open the browser and visit <code>http://localhost:8501</code></p> <ul> <li>Input OpenAI API key in left sidebar</li> <li>Input the TiDB Cloud connection string in left sidebar, the format is <code>mysql+pymysql://root@localhost:4000/test</code></li> </ul>"},{"location":"ai/examples/text2sql-with-pytidb/#related-resources","title":"Related Resources","text":"<ul> <li>Source Code: View on GitHub</li> <li> <p>Category: Ai-Apps</p> </li> <li> <p>Description: Convert natural language queries into SQL statements using AI models.</p> </li> </ul> <p>\ud83c\udfe0 Back to Demo Gallery </p>"},{"location":"ai/examples/vector-search-with-pytidb/","title":"Vector Search Example","text":"<p>This example demonstrates how to build a semantic search application using TiDB and local embedding models. It leverages vector search to find similar items based on meaning, not just keywords. The app uses Streamlit for the web UI and Ollama for local embedding generation.</p> <p> <p>Semantic search with vector embeddings</p> </p>"},{"location":"ai/examples/vector-search-with-pytidb/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>A TiDB Cloud Starter cluster: Create a free cluster here: tidbcloud.com \u2197\ufe0f</li> <li>Ollama: You can install it from Ollama \u2197\ufe0f</li> </ul>"},{"location":"ai/examples/vector-search-with-pytidb/#how-to-run","title":"How to run","text":"<p>Step 1: Start the embedding service with Ollama</p> <p>Pull the embedding model:</p> <pre><code>ollama pull mxbai-embed-large\n</code></pre> <p>Test the embedding service to make sure it is running:</p> <pre><code>curl http://localhost:11434/api/embed -d '{\n  \"model\": \"mxbai-embed-large\",\n  \"input\": \"Llamas are members of the camelid family\"\n}'\n</code></pre> <p>Step 2: Clone the repository to local</p> <pre><code>git clone https://github.com/pingcap/pytidb.git\ncd pytidb/examples/vector_search/\n</code></pre> <p>Step 3: Install the required packages and set up the environment</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -r reqs.txt\n</code></pre> <p>Step 4: Set up environment to connect to TiDB</p> <p>Go to TiDB Cloud console and get the connection parameters, then set up the environment variable like this:</p> <pre><code>cat &gt; .env &lt;&lt;EOF\nTIDB_HOST={gateway-region}.prod.aws.tidbcloud.com\nTIDB_PORT=4000\nTIDB_USERNAME={prefix}.root\nTIDB_PASSWORD={password}\nTIDB_DATABASE=pytidb_vector_search\nEOF\n</code></pre> <p>Step 5: Run the Streamlit app</p> <pre><code>streamlit run app.py\n</code></pre> <p>Step 6: Open your browser and visit <code>http://localhost:8501</code></p>"},{"location":"ai/examples/vector-search-with-pytidb/#related-resources","title":"Related Resources","text":"<ul> <li>Source Code: View on GitHub</li> <li> <p>Category: Search</p> </li> <li> <p>Description: Implement semantic search using vector embeddings to find similar content.</p> </li> </ul> <p>\ud83c\udfe0 Back to Demo Gallery </p>"},{"location":"ai/guides/auto-embedding/","title":"Auto Embedding","text":"<p>Auto embedding is a feature that allows you to automatically generate vector embeddings for text data.</p> <p>Tip</p> <p>To check the complete example code, please refer to the auto embedding example.</p>"},{"location":"ai/guides/auto-embedding/#basic-usage","title":"Basic Usage","text":"<p>In this example, we use TiDB Cloud hosted embedding models for demonstration, for other providers, please check the Supported Providers list.</p>"},{"location":"ai/guides/auto-embedding/#step-1-define-a-embedding-function","title":"Step 1. Define a embedding function","text":"Python <p>Define a embedding function to generate vector embeddings for text data.</p> <pre><code>from pytidb.embeddings import EmbeddingFunction\n\nembed_func = EmbeddingFunction(\n    model_name=\"tidbcloud_free/amazon/titan-embed-text-v2\",\n)\n</code></pre>"},{"location":"ai/guides/auto-embedding/#step-2-create-a-table-and-a-vector-field","title":"Step 2. Create a table and a vector field","text":"Python <p>Use <code>embed_func.VectorField()</code> to create a vector field on the table.</p> <p>To enable auto embedding, you need to set <code>source_field</code> to the field that you want to embed.</p> <pre><code>from pytidb.schema import TableModel, Field\nfrom pytidb.datatype import TEXT\n\nclass Chunk(TableModel):\n    id: int = Field(primary_key=True)\n    text: str = Field(sa_type=TEXT)\n    text_vec: list[float] = embed_func.VectorField(source_field=\"text\")\n\ntable = client.create_table(schema=Chunk, if_exists=\"overwrite\")\n</code></pre> <p>You don't need to specify the <code>dimensions</code> parameter, it will be automatically determined by the embedding model.</p> <p>However, you can specify the <code>dimensions</code> parameter to override the default dimension.</p>"},{"location":"ai/guides/auto-embedding/#step-3-insert-some-sample-data","title":"Step 3. Insert some sample data","text":"Python <p>Insert some sample data into the table.</p> <pre><code>table.bulk_insert([\n    Chunk(text=\"TiDB is a distributed database that supports OLTP, OLAP, HTAP and AI workloads.\"),\n    Chunk(text=\"PyTiDB is a Python library for developers to connect to TiDB.\"),\n    Chunk(text=\"LlamaIndex is a Python library for building AI-powered applications.\"),\n])\n</code></pre> <p>When inserting data, the <code>text_vec</code> field will be automatically populated with the vector embeddings generated based on the <code>text</code> field.</p>"},{"location":"ai/guides/auto-embedding/#step-4-perform-a-vector-search","title":"Step 4. Perform a vector search","text":"Python <p>You can pass the query text to the <code>search()</code> method directly, the query text will be embedded and then used for vector search.</p> <pre><code>table.search(\"HTAP database\").limit(3).to_list()\n</code></pre>"},{"location":"ai/guides/connect/","title":"Connect to database","text":"<p>In this guide, we will introduce how to connect to a TiDB database using the TiDB client.</p>"},{"location":"ai/guides/connect/#install-the-dependencies","title":"Install the dependencies","text":"<p>pytidb is a Python client built upon SQLAlchemy, it provides a series of high-level APIs to help developers store and search vector embeddings without writing raw SQL.</p> <p>To install the Python client, run the following command:</p> <pre><code>pip install pytidb\n</code></pre>"},{"location":"ai/guides/connect/#connect-with-connection-parameters","title":"Connect with connection parameters","text":"<p>Choose the steps based on your deployment type:</p> TiDB Cloud StarterTiDB Self-Managed <p>You can create a starter cluster in the TiDB Cloud, and then get the connection parameters from the web console.</p> <ol> <li>Navigate to the Clusters page, and then click the name of your target cluster to go to its overview page.</li> <li>Click Connect in the upper-right corner. A connection dialog is displayed, with connection parameters listed.</li> <li>Copy the connection parameters to your code or environment variables.</li> </ol> <p>Example code:</p> main.py<pre><code>from pytidb import TiDBClient\n\ndb = TiDBClient.connect(\n    host=\"{gateway-region}.prod.aws.tidbcloud.com\",\n    port=4000,\n    username=\"{prefix}.root\",\n    password=\"{password}\",\n    database=\"test\",\n)\n</code></pre> <p>Tip</p> <p>For TiDB Cloud Starter, TLS connection to the database is required when using Public Endpoint. TiDB Client will automatically enable TLS connection for starter clusters.</p> <p>You can follow Quick Start with TiDB Self-Managed to deploy a TiDB cluster for testing.</p> <p>Example code:</p> main.py<pre><code>from pytidb import TiDBClient\n\ndb = TiDBClient.connect(\n    host=\"{tidb_server_host}\",\n    port=4000,\n    username=\"root\",\n    password=\"{password}\",\n    database=\"test\",\n)\n</code></pre> <p>Tip</p> <p>If you are using <code>tiup playground</code> to deploy a TiDB cluster for testing, the default host is <code>127.0.0.1</code> and the default password is empty.</p> <p>Once connected, you can use the <code>db</code> object to operate tables, query data, and more.</p>"},{"location":"ai/guides/connect/#connect-with-connection-string","title":"Connect with connection string","text":"<p>If you prefer to use a connection string (database URL), you can follow the format based on your deployment type:</p> TiDB Cloud StarterTiDB Self-Managed <p>You can create a starter cluster in the TiDB Cloud, and then get the connection parameters from the web console.</p> <ol> <li>Navigate to the Clusters page, and then click the name of your target cluster to go to its overview page.</li> <li>Click Connect in the upper-right corner. A connection dialog is displayed with the connection parameters listed.</li> <li>Copy the connection parameters and construct the connection string as the format below.</li> </ol> main.py<pre><code>from pytidb import TiDBClient\n\ndb = TiDBClient.connect(\n    database_url=\"mysql+pymysql://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}?ssl_verify_cert=true&amp;ssl_verify_identity=true\",\n)\n</code></pre> <p>Note</p> <p>For TiDB Cloud Starter, TLS connection to the database is required when using Public Endpoint, so you need to set <code>ssl_verify_cert=true&amp;ssl_verify_identity=true</code> in the connection string.</p> <p>You can follow the format below to construct the connection string:</p> main.py<pre><code>from pytidb import TiDBClient\n\ndb = TiDBClient.connect(\n    database_url=\"mysql+pymysql://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\",\n)\n</code></pre> <p>Tip</p> <p>If you are using <code>tiup playground</code> to deploy a TiDB cluster for testing, the connection string is: </p> <pre><code>mysql+pymysql://root:@127.0.0.1:4000/test\n</code></pre>"},{"location":"ai/guides/connect/#connect-with-sqlalchemy-db-engine","title":"Connect with SQLAlchemy DB engine","text":"<p>If your application already has an existing SQLAlchemy database engine, you can reuse the engine through the <code>db_engine</code> parameter:</p> main.py<pre><code>from pytidb import TiDBClient\n\ndb = TiDBClient(db_engine=db_engine)\n</code></pre>"},{"location":"ai/guides/connect/#next-steps","title":"Next Steps","text":"<p>After connecting to your TiDB database, you can explore the following guides to learn how to work with your data:</p> <ul> <li>Working with Tables: Learn how to define and manage tables in TiDB.</li> <li>Vector Search: Perform semantic search using vector embeddings.</li> <li>Fulltext Search: Retrieve documents using keyword-based search.</li> <li>Hybrid Search: Combine vector and full-text search for more relevant results.</li> </ul>"},{"location":"ai/guides/filtering/","title":"Filtering","text":"<p>As a relational database, TiDB supports a rich set of SQL operators and allows flexible combinations of filtering conditions that enable you to query your data precisely.</p>"},{"location":"ai/guides/filtering/#overview","title":"Overview","text":"<p>You can not only apply filtering on scalar fields but also on JSON fields. Filtering on JSON fields is often used for metadata filtering in vector search.</p> Python <p>For PyTiDB, you can apply filtering by passing a filters parameter to the <code>table.query()</code>, <code>table.delete()</code>, <code>table.update()</code>, and <code>table.search()</code> methods.</p> <p>The filters parameter supports two formats: Dictionary Filters and SQL String Filters.</p>"},{"location":"ai/guides/filtering/#dictionary-filters","title":"Dictionary Filters","text":"Python <p>PyTiDB allows you to define filter conditions using a Python dictionary with operators as the filters parameter.</p> <p>The dictionary structure of filters is as follows:</p> <pre><code>{\n    \"&lt;key&gt;\": {\n        \"&lt;operator&gt;\": &lt;value&gt;\n    },\n    ...\n}\n</code></pre> <ul> <li><code>&lt;key&gt;</code>: The key can be a column name, a JSON path expression to access a JSON field (see Metadata filtering), or a logical operator.</li> <li><code>&lt;operator&gt;</code>: The operator can be a compare operator or an inclusion operator.</li> <li><code>&lt;value&gt;</code>: The value can be a scalar value, an array, it depends on the operator.</li> </ul> <p>Example: Filter records where <code>created_at</code> is greater than 2024-01-01</p> <pre><code>table.query({\n    # The `created_at` is a scalar field with DATETIME type\n    \"created_at\": {\n        \"$gt\": \"2024-01-01\"\n    }\n})\n</code></pre> <p>Example: Filter records where <code>meta.category</code> is in the array [\"tech\", \"science\"]</p> <pre><code>results = (\n    table.search(\"some query\", search_type=\"vector\")\n        .filter({\n            # The `meta` is a JSON field, and its value is a JSON object like {\"category\": \"tech\"}\n            \"meta.category\": {\n                \"$in\": [\"tech\", \"science\"]\n            }\n        })\n        .limit(10)\n        .to_list()\n)\n</code></pre>"},{"location":"ai/guides/filtering/#compare-operators","title":"Compare operators","text":"<p>You can use the following compare operators to filter records:</p> Operator Description <code>$eq</code> Equal to value <code>$ne</code> Not equal to value <code>$gt</code> Greater than value <code>$gte</code> Greater than or equal to value <code>$lt</code> Less than value <code>$lte</code> Less than or equal to value <p>Example: filter records where <code>user_id</code> is equal to 1</p> <pre><code>{\n    \"user_id\": {\n        \"$eq\": 1\n    }\n}\n</code></pre> <p>You can omit the <code>$eq</code> operator. The following query is equivalent to the above:</p> <pre><code>{\n    \"user_id\": 1\n}\n</code></pre>"},{"location":"ai/guides/filtering/#inclusion-operators","title":"Inclusion operators","text":"<p>You can use the following inclusion operators to filter records:</p> Operator Description <code>$in</code> In array (string, int, or float) <code>$nin</code> Not in array (string, int, float) <p>Example: Filter records where <code>category</code> is in the array [\"tech\", \"science\"]</p> <pre><code>{\n    \"category\": {\n        \"$in\": [\"tech\", \"science\"]\n    }\n}\n</code></pre>"},{"location":"ai/guides/filtering/#logical-operators","title":"Logical operators","text":"<p>You can use the logical operators <code>$and</code> and <code>$or</code> to combine multiple filters.</p> Operator Description <code>$and</code> Returns results that match all filters in the list <code>$or</code> Returns results that match any filter in the list <p>Syntax for using <code>$and</code> or <code>$or</code>:</p> <pre><code>{\n    \"$and|$or\": [\n        {\n            \"field_name\": {\n                &lt;operator&gt;: &lt;value&gt;\n            }\n        },\n        {\n            \"field_name\": {\n                &lt;operator&gt;: &lt;value&gt;\n            }\n        }\n        ...\n    ]\n}\n</code></pre> <p>Example: using <code>$and</code> to combine multiple filters:</p> <pre><code>{\n    \"$and\": [\n        {\n            \"created_at\": {\n                \"$gt\": \"2024-01-01\"\n            }\n        },\n        {\n            \"meta.category\": {\n                \"$in\": [\"tech\", \"science\"]\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"ai/guides/filtering/#sql-string-filters","title":"SQL String Filters","text":"Python <p>You can also use a SQL string as the <code>filters</code> parameter. The string should be a valid SQL <code>WHERE</code> clause (without the <code>WHERE</code> keyword) using TiDB's SQL syntax.</p> <p>Example: Filter records where <code>created_at</code> is greater than 2024-01-01</p> <pre><code>results = table.query(\n    filters=\"created_at &gt; '2024-01-01'\",\n    limit=10\n).to_list()\n</code></pre> <p>Example: Filter records where the JSON field <code>meta.category</code> equals 'tech'</p> <pre><code>results = table.query(\n    filters=\"meta-&gt;&gt;'$.category' = 'tech'\",\n    limit=10\n).to_list()\n</code></pre> <p>You can combine multiple conditions using <code>AND</code>, <code>OR</code>, and parentheses, and use any SQL operators supported by TiDB.</p> <p>Warning</p> <p>When using SQL string filters with dynamic user input, always validate the input to prevent SQL injection vulnerabilities.</p>"},{"location":"ai/guides/fulltext-search/","title":"Full-text Search","text":"<p>Full-text search enables you to find documents or data by matching keywords or phrases within the entire text content. It is widely used in search engines, document management, e-commerce, and any scenario where users need to search large volumes of unstructured or semi-structured text.</p> <p>TiDB provides full-text search capabilities for massive datasets with high performance and built-in multilingual support.</p> <p>Note</p> <p>Full-text search is currently in the early stages with limited accessibility. It is only available for TiDB Cloud Starter in the following regions:</p> <ul> <li>Frankfurt (eu-central-1)</li> <li>Singapore (ap-southeast-1)</li> </ul> <p>If you have feedback or need help, feel free to reach out to us on Discord.</p> <p>Tip</p> <p>For a complete example of full-text search, see the E-commerce product search demo.</p>"},{"location":"ai/guides/fulltext-search/#basic-usage","title":"Basic Usage","text":""},{"location":"ai/guides/fulltext-search/#step-1-create-table-and-full-text-index","title":"Step 1. Create Table and Full-text Index","text":"PythonSQL <p>You can use <code>FullTextField</code> to define a text field with full-text search enabled. The <code>fts_parser</code> parameter in Python corresponds to the <code>WITH PARSER</code> clause in SQL.</p> <p>For example, the following code creates a table with a full-text index on the <code>title</code> column:</p> <pre><code>from pytidb.schema import TableModel, Field, FullTextField\n\nclass Item(TableModel):\n    __tablename__ = \"items\"\n    id: int = Field(primary_key=True)\n    title: str = FullTextField(fts_parser=\"MULTILINGUAL\")\n\ntable = client.create_table(schema=Item, if_exists=\"overwrite\")\n</code></pre> <p>The <code>fts_parser</code> parameter specifies the parser for the full-text index. Supported values:</p> <ul> <li><code>STANDARD</code>: Fast, works for English content, splits words by spaces and punctuation.</li> <li><code>MULTILINGUAL</code> (default): Supports multiple languages, including English, Chinese, Japanese, and Korean.</li> </ul> <p>Create a table with a full-text index:</p> <pre><code>CREATE TABLE items(\n    id INT PRIMARY KEY,\n    title TEXT,\n    FULLTEXT INDEX (title) WITH PARSER MULTILINGUAL\n);\n</code></pre> <p>You can also add a full-text index to an existing table with a separate statement:</p> <pre><code>CREATE TABLE items(\n    id INT PRIMARY KEY,\n    title TEXT\n);\n\nALTER TABLE items ADD FULLTEXT INDEX (title)\nWITH PARSER MULTILINGUAL ADD_COLUMNAR_REPLICA_ON_DEMAND;\n</code></pre> <p>The following parsers are supported in the <code>WITH PARSER &lt;PARSER_NAME&gt;</code> clause:</p> <ul> <li><code>STANDARD</code>: Fast, works for English content, splits words by spaces and punctuation.</li> <li><code>MULTILINGUAL</code>: Supports multiple languages, including English, Chinese, Japanese, and Korean.</li> </ul>"},{"location":"ai/guides/fulltext-search/#step-2-insert-sample-data","title":"Step 2. Insert Sample Data","text":"<p>For demonstration purposes, the following sample data covers English, Japanese, and Chinese text.</p> PythonSQL <p>You can use the <code>bulk_insert</code> method to insert sample data into the table.</p> <pre><code>table.bulk_insert([\n    Item(id=1, title=\"Bluetooth Earphones, HiFi sound, 48h battery, Fast charge, Low latency\"),\n    Item(id=2, title=\"Bluetooth 5.3 Headphones, Noise Cancelling, Immersive sound, Comfortable\"),\n    Item(id=3, title=\"IPX7 Waterproof Earbuds, Sport ready, Touch control, High-quality music\"),\n    Item(id=4, title=\"Sports Earbuds, Secure fit, Sweatproof, Long battery, Workout support\"),\n    Item(id=5, title=\"Wired Headphones, Studio-grade, HD sound, Comfortable, Pro music experience\"),\n    Item(id=6, title=\"Bluetooth\u30a4\u30e4\u30db\u30f3 HiFi\u97f3\u8cea 48h\u30d0\u30c3\u30c6\u30ea\u30fc \u6025\u901f\u5145\u96fb \u4f4e\u9045\u5ef6\"),\n    Item(id=7, title=\"Bluetooth5.3\u30d8\u30c3\u30c9\u30db\u30f3 \u30ce\u30a4\u30ba\u30ad\u30e3\u30f3\u30bb\u30ea\u30f3\u30b0 \u6ca1\u5165\u30b5\u30a6\u30f3\u30c9 \u5feb\u9069\u88c5\u7740\"),\n    Item(id=8, title=\"IPX7\u9632\u6c34\u30a4\u30e4\u30db\u30f3 \u30b9\u30dd\u30fc\u30c4\u5bfe\u5fdc \u30bf\u30c3\u30c1\u64cd\u4f5c \u9ad8\u97f3\u8cea\u97f3\u697d\"),\n    Item(id=9, title=\"\u30b9\u30dd\u30fc\u30c4\u30a4\u30e4\u30db\u30f3 \u5b89\u5b9a\u88c5\u7740 \u9632\u6c57 \u9577\u6301\u3061\u30d0\u30c3\u30c6\u30ea\u30fc \u30ef\u30fc\u30af\u30a2\u30a6\u30c8\u5bfe\u5fdc\"),\n    Item(id=10, title=\"\u6709\u7dda\u30d8\u30c3\u30c9\u30db\u30f3 \u30b9\u30bf\u30b8\u30aa\u7d1a HD\u30b5\u30a6\u30f3\u30c9 \u5feb\u9069\u88c5\u7740 \u30d7\u30ed\u97f3\u697d\u4f53\u9a13\"),\n    Item(id=11, title=\"\u65e0\u7ebf\u84dd\u7259\u8033\u673a HiFi\u97f3\u8d28 48\u5c0f\u65f6\u8d85\u957f\u7eed\u822a \u5feb\u901f\u5145\u7535 \u4f4e\u5ef6\u8fdf\"),\n    Item(id=12, title=\"\u84dd\u72595.3\u964d\u566a\u5934\u6234\u5f0f\u8033\u673a \u675c\u6bd4\u5168\u666f\u58f0 \u6c89\u6d78\u97f3\u6548 \u8212\u9002\u4f69\u6234 \u7545\u4eab\u9759\u8c27\u97f3\u4e50\u65f6\u5149\"),\n    Item(id=13, title=\"IPX7\u9632\u6c34\u771f\u65e0\u7ebf\u8033\u673a \u8fd0\u52a8\u65e0\u5fe7 \u667a\u80fd\u89e6\u63a7 \u968f\u65f6\u7545\u542c\u9ad8\u54c1\u8d28\u97f3\u4e50\"),\n    Item(id=14, title=\"\u8fd0\u52a8\u4e13\u7528\u8033\u673a \u7a33\u56fa\u4f69\u6234 \u9632\u6c57\u8bbe\u8ba1 \u8d85\u957f\u7eed\u822a \u4f4e\u5ef6\u8fdf\u97f3\u9891 \u9ad8\u6e05\u901a\u8bdd\"),\n    Item(id=15, title=\"\u5f55\u97f3\u5ba4\u7ea7\u6709\u7ebf\u8033\u673a \u9ad8\u6e05\u97f3\u8d28 \u8212\u9002\u4f69\u6234 \u53ef\u62c6\u5378\u7ebf\u6750 \u591a\u8bbe\u5907\u517c\u5bb9 \u964d\u566a\u9ea6\u514b\u98ce\"),\n])\n</code></pre> <p>You can use the <code>INSERT INTO</code> statement to insert the sample data into the table.</p> <pre><code>INSERT INTO items (id, title) VALUES\n    (1, 'Bluetooth Earphones, HiFi sound, 48h battery, Fast charge, Low latency'),\n    (2, 'Bluetooth 5.3 Headphones, Noise Cancelling, Immersive sound, Comfortable'),\n    (3, 'IPX7 Waterproof Earbuds, Sport ready, Touch control, High-quality music'),\n    (4, 'Sports Earbuds, Secure fit, Sweatproof, Long battery, Workout support'),\n    (5, 'Wired Headphones, Studio-grade, HD sound, Comfortable, Pro music experience'),\n    (6, 'Bluetooth\u30a4\u30e4\u30db\u30f3 HiFi\u97f3\u8cea 48h\u30d0\u30c3\u30c6\u30ea\u30fc \u6025\u901f\u5145\u96fb \u4f4e\u9045\u5ef6'),\n    (7, 'Bluetooth5.3\u30d8\u30c3\u30c9\u30db\u30f3 \u30ce\u30a4\u30ba\u30ad\u30e3\u30f3\u30bb\u30ea\u30f3\u30b0 \u6ca1\u5165\u30b5\u30a6\u30f3\u30c9 \u5feb\u9069\u88c5\u7740'),\n    (8, 'IPX7\u9632\u6c34\u30a4\u30e4\u30db\u30f3 \u30b9\u30dd\u30fc\u30c4\u5bfe\u5fdc \u30bf\u30c3\u30c1\u64cd\u4f5c \u9ad8\u97f3\u8cea\u97f3\u697d'),\n    (9, '\u30b9\u30dd\u30fc\u30c4\u30a4\u30e4\u30db\u30f3 \u5b89\u5b9a\u88c5\u7740 \u9632\u6c57 \u9577\u6301\u3061\u30d0\u30c3\u30c6\u30ea\u30fc \u30ef\u30fc\u30af\u30a2\u30a6\u30c8\u5bfe\u5fdc'),\n    (10, '\u6709\u7ebf\u30d8\u30c3\u30c9\u30db\u30f3 \u30b9\u30bf\u30b8\u30aa\u7ea7 HD\u30b5\u30a6\u30f3\u30c9 \u5feb\u9002\u88c5\u7740 \u30d7\u30ed\u97f3\u697d\u4f53\u9a13'),\n    (11, '\u65e0\u7ebf\u84dd\u7259\u8033\u673a HiFi\u97f3\u8d28 48\u5c0f\u65f6\u8d85\u957f\u7eed\u822a \u5feb\u901f\u5145\u7535 \u4f4e\u5ef6\u8fdf'),\n    (12, '\u84dd\u72595.3\u964d\u566a\u5934\u6234\u5f0f\u8033\u673a \u675c\u6bd4\u5168\u666f\u58f0 \u6c89\u6d78\u97f3\u6548 \u8212\u9002\u4f69\u6234 \u7545\u4eab\u9759\u8c27\u97f3\u4e50\u65f6\u5149'),\n    (13, 'IPX7\u9632\u6c34\u771f\u65e0\u7ebf\u8033\u673a \u8fd0\u52a8\u65e0\u5fe7 \u667a\u80fd\u89e6\u63a7 \u968f\u65f6\u7545\u542c\u9ad8\u54c1\u8d28\u97f3\u4e50'),\n    (14, '\u8fd0\u52a8\u4e13\u7528\u8033\u673a \u7a33\u56fa\u4f69\u6234 \u9632\u6c57\u8bbe\u8ba1 \u8d85\u957f\u7eed\u822a \u4f4e\u5ef6\u8fdf\u97f3\u9891 \u9ad8\u6e05\u901a\u8bdd'),\n    (15, '\u5f55\u97f3\u5ba4\u7ea7\u6709\u7ebf\u8033\u673a \u9ad8\u6e05\u97f3\u8d28 \u8212\u9002\u4f69\u6234 \u53ef\u62c6\u5378\u7ebf\u6750 \u591a\u8bbe\u5907\u517c\u5bb9 \u964d\u566a\u9ea6\u514b\u98ce');\n</code></pre>"},{"location":"ai/guides/fulltext-search/#step-3-perform-a-full-text-search","title":"Step 3. Perform a Full-text Search","text":"PythonSQL <p>To perform a full-text search with pytidb, use the <code>search</code> method and set the <code>search_type</code> parameter to <code>\"fulltext\"</code>.</p> <p>Example: Search for the 3 most relevant documents</p> <pre><code>results = table.search(\"Bluetooth Headphones\", search_type=\"fulltext\").limit(3).to_list()\nprint(json.dumps(results, indent=2, ensure_ascii=False))\n</code></pre> Execution result<pre><code>[\n    {\n        \"id\": 2,\n        \"title\": \"Bluetooth 5.3 Headphones, Noise Cancelling, Immersive sound, Comfortable\",\n        \"_match_score\": 3.7390857,\n        \"_score\": 3.7390857\n    },\n    {\n        \"id\": 5,\n        \"title\": \"Wired Headphones, Studio-grade, HD sound, Comfortable, Pro music experience\",\n        \"_match_score\": 1.9798478,\n        \"_score\": 1.9798478\n    },\n    {\n        \"id\": 1,\n        \"title\": \"Bluetooth Earphones, HiFi sound, 48h battery, Fast charge, Low latency\",\n        \"_match_score\": 1.620981,\n        \"_score\": 1.620981\n    }\n]\n</code></pre> <p>The results are sorted by relevance, with the most relevant documents listed first.</p> <p>The <code>_match_score</code> (or <code>_score</code>) field indicates the relevance score of each document, calculated using the BM25 algorithm\u2014a widely used ranking function in information retrieval.</p> <p>Example: Search for the 3 most relevant documents in another language</p> <pre><code>results = table.search(\"\u84dd\u7259\u8033\u673a\", search_type=\"fulltext\").limit(3).to_list()\nprint(json.dumps(results, indent=2, ensure_ascii=False))\n</code></pre> Execution result<pre><code>[\n    {\n        \"id\": 11,\n        \"title\": \"\u65e0\u7ebf\u84dd\u7259\u8033\u673a HiFi\u97f3\u8d28 48\u5c0f\u65f6\u8d85\u957f\u7eed\u822a \u5feb\u901f\u5145\u7535 \u4f4e\u5ef6\u8fdf\",\n        \"_match_score\": 3.000002,\n        \"_score\": 3.000002\n    },\n    {\n        \"id\": 12,\n        \"title\": \"\u84dd\u72595.3\u964d\u566a\u5934\u6234\u5f0f\u8033\u673a \u675c\u6bd4\u5168\u666f\u58f0 \u6c89\u6d78\u97f3\u6548 \u8212\u9002\u4f69\u6234 \u7545\u4eab\u9759\u8c27\u97f3\u4e50\u65f6\u5149\",\n        \"_match_score\": 2.5719738,\n        \"_score\": 2.5719738\n    },\n    {\n        \"id\": 14,\n        \"title\": \"\u8fd0\u52a8\u4e13\u7528\u8033\u673a \u7a33\u56fa\u4f69\u6234 \u9632\u6c57\u8bbe\u8ba1 \u8d85\u957f\u7eed\u822a \u4f4e\u5ef6\u8fdf\u97f3\u9891 \u9ad8\u6e05\u901a\u8bdd\",\n        \"_match_score\": 1.1418362,\n        \"_score\": 1.1418362\n    }\n]\n</code></pre> <p>To perform a full-text search, use the <code>fts_match_word()</code> function.</p> <p>Example: Search for the 3 most relevant documents</p> <pre><code>SELECT *, fts_match_word(\"Bluetooth Headphones\", title) AS score\nFROM items\nWHERE fts_match_word(\"Bluetooth Headphones\", title)\nORDER BY score DESC\nLIMIT 3;\n</code></pre> Execution result<pre><code>+----+-----------------------------------------------------------------------------+-----------+\n| id | title                                                                       | score     |\n+----+-----------------------------------------------------------------------------+-----------+\n|  2 | Bluetooth 5.3 Headphones, Noise Cancelling, Immersive sound, Comfortable    | 3.7390857 |\n|  5 | Wired Headphones, Studio-grade, HD sound, Comfortable, Pro music experience | 1.9798478 |\n|  1 | Bluetooth Earphones, HiFi sound, 48h battery, Fast charge, Low latency      |  1.620981 |\n+----+-----------------------------------------------------------------------------+-----------+\n</code></pre> <p>The results are ordered by relevance, with the most relevant documents first.</p> <p>Example: Search for the 3 most relevant documents in another language</p> <pre><code>SELECT *, fts_match_word(\"\u84dd\u7259\u8033\u673a\", title) AS score\nFROM items\nWHERE fts_match_word(\"\u84dd\u7259\u8033\u673a\", title)\nORDER BY score DESC\nLIMIT 3;\n</code></pre> Execution result<pre><code>+----+------------------------------------------------------------------+-----------+\n| id | title                                                            | score     |\n+----+------------------------------------------------------------------+-----------+\n| 11 | \u65e0\u7ebf\u84dd\u7259\u8033\u673a HiFi\u97f3\u8d28 48\u5c0f\u65f6\u8d85\u957f\u7eed\u822a \u5feb\u901f\u5145\u7535 \u4f4e\u5ef6\u8fdf                    |  3.000002 |\n| 12 | \u84dd\u72595.3\u964d\u566a\u5934\u6234\u5f0f\u8033\u673a \u675c\u6bd4\u5168\u666f\u58f0 \u6c89\u6d78\u97f3\u6548 \u8212\u9002\u4f69\u6234 \u7545\u4eab\u9759\u8c27\u97f3\u4e50\u65f6\u5149        | 2.5719738 |\n| 14 | \u8fd0\u52a8\u4e13\u7528\u8033\u673a \u7a33\u56fa\u4f69\u6234 \u9632\u6c57\u8bbe\u8ba1 \u8d85\u957f\u7eed\u822a \u4f4e\u5ef6\u8fdf\u97f3\u9891 \u9ad8\u6e05\u901a\u8bdd               | 1.1418362 |\n+----+------------------------------------------------------------------+-----------+\n</code></pre>"},{"location":"ai/guides/fulltext-search/#see-also","title":"See Also","text":"<p>In Retrieval-Augmented Generation (RAG) scenarios, it is often beneficial to utilize both full-text search and vector search for optimal results.</p> <ul> <li>Learn how to combine these approaches in the hybrid search guide.</li> <li>For more on vector search, see the vector search guide.</li> </ul>"},{"location":"ai/guides/hybrid-search/","title":"Hybrid Search","text":"<p>Hybrid search is a technique that combines multiple search algorithms to deliver more accurate and relevant results.</p> <p>TiDB supports both semantic search (also known as vector search) and keyword-based search (full-text search). By leveraging the strengths of both approaches, you can achieve superior search results through hybrid search.</p> <p> </p> <p>Tip</p> <p>For a complete example of hybrid search, refer to the hybrid-search example.</p>"},{"location":"ai/guides/hybrid-search/#basic-usage","title":"Basic Usage","text":""},{"location":"ai/guides/hybrid-search/#step-1-define-an-embedding-function","title":"Step 1. Define an Embedding Function","text":"<p>Define an embedding function to generate vector representations of text data.</p> <pre><code>from pytidb.embeddings import EmbeddingFunction\n\nembed_fn = EmbeddingFunction(\n    model_name=\"openai/text-embedding-3-small\",\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n</code></pre>"},{"location":"ai/guides/hybrid-search/#step-2-create-a-table-with-vector-and-full-text-indexes","title":"Step 2. Create a Table with Vector and Full-Text Indexes","text":"Python <p>After you have connected to your TiDB database using <code>TiDBClient</code> and get the <code>client</code> instance:</p> <p>You can now create a table with both a <code>FullTextField</code> and a <code>VectorField</code> to store the text data and its vector embedding.</p> <p>Example:</p> <pre><code>from pytidb.schema import TableModel, Field, FullTextField\n\nclass Chunk(TableModel):\n    __tablename__ = \"chunks_for_hybrid_search\"\n    id: int = Field(primary_key=True)\n    text: str = FullTextField()\n    text_vec: list[float] = embed_fn.VectorField(source_field=\"text\")\n\ntable = client.create_table(schema=Chunk, if_exists=\"overwrite\")\n</code></pre> <p>In this example, PyTiDB will automatically create a full-text index on the <code>text</code> column and a vector index on the <code>text_vec</code> column.</p>"},{"location":"ai/guides/hybrid-search/#step-3-insert-sample-data","title":"Step 3. Insert Sample Data","text":"Python <p>Use the <code>bulk_insert()</code> method to insert sample data into the table.</p> <pre><code>table.truncate()\ntable.bulk_insert([\n    Chunk(\n        text=\"TiDB is a distributed database that supports OLTP, OLAP, HTAP and AI workloads.\",\n    ),\n    Chunk(\n        text=\"PyTiDB is a Python library for developers to connect to TiDB.\",\n    ),\n    Chunk(\n        text=\"LlamaIndex is a Python library for building AI-powered applications.\",\n    ),\n])\n</code></pre> <p>The <code>text_vec</code> field is automatically populated with the vector embedding of the text data via the Auto Embedding feature.</p>"},{"location":"ai/guides/hybrid-search/#step-4-perform-hybrid-search","title":"Step 4. Perform Hybrid Search","text":"<p>To enable hybrid search, set the <code>search_type</code> parameter to <code>hybrid</code> when calling the <code>search()</code> method.</p> <pre><code>results = (\n    table.search(\n        \"AI database\", search_type=\"hybrid\"\n    )\n    .limit(3)\n    .to_list()\n)\n\nfor item in results:\n    item.pop(\"text_vec\")\nprint(json.dumps(results, indent=4, sort_keys=True))\n</code></pre> <p>The search results contain three special fields:</p> <ul> <li><code>_distance</code>: The distance between the query vector and the vector data in the table, as returned by the vector search.</li> <li><code>_match_score</code>: The match score between the query and the text field, as returned by the full-text search.</li> <li><code>_score</code>: The final score of the search result, calculated by the fusion algorithm.</li> </ul> Output<pre><code>[\n    {\n        \"_distance\": 0.4740166257687124,\n        \"_match_score\": 1.6804268,\n        \"_score\": 0.03278688524590164,\n        \"id\": 60013,\n        \"text\": \"TiDB is a distributed database that supports OLTP, OLAP, HTAP and AI workloads.\"\n    },\n    {\n        \"_distance\": 0.6428459116216618,\n        \"_match_score\": 0.78427225,\n        \"_score\": 0.03200204813108039,\n        \"id\": 60015,\n        \"text\": \"LlamaIndex is a Python library for building AI-powered applications.\"\n    },\n    {\n        \"_distance\": 0.641581407158715,\n        \"_match_score\": null,\n        \"_score\": 0.016129032258064516,\n        \"id\": 60014,\n        \"text\": \"PyTiDB is a Python library for developers to connect to TiDB.\"\n    }\n]\n</code></pre>"},{"location":"ai/guides/hybrid-search/#fusion-methods","title":"Fusion Methods","text":"<p>Fusion methods combine results from vector (semantic) and full-text (keyword) searches into a single, unified ranking. This ensures that the final results leverage both semantic relevance and keyword matching.</p> <p>PyTiDB supports two fusion methods:</p> <ul> <li><code>rrf</code>: Reciprocal Rank Fusion (default)</li> <li><code>weighted</code>: Weighted Score Fusion</li> </ul> <p>You can select the fusion method that best fits your use case to optimize hybrid search results.</p>"},{"location":"ai/guides/hybrid-search/#reciprocal-rank-fusion-rrf","title":"Reciprocal Rank Fusion (RRF)","text":"<p>Reciprocal Rank Fusion (RRF) is an algorithm that evaluates search results by leveraging the rank of documents in multiple result sets.</p> <p>For more details, see the RRF paper.</p> Python <p>Enable reciprocal rank fusion by specifying the <code>method</code> parameter as <code>\"rrf\"</code> in the <code>.fusion()</code> method.</p> <pre><code>results = (\n    table.search(\n        \"AI database\", search_type=\"hybrid\"\n    )\n    .fusion(method=\"rrf\")\n    .limit(3)\n    .to_list()\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>k</code>: A constant (default: 60) to prevent division by zero and control the impact of high-ranked documents.</li> </ul>"},{"location":"ai/guides/hybrid-search/#weighted-score-fusion","title":"Weighted Score Fusion","text":"<p>Weighted Score Fusion combines vector search and full-text search scores using weighted sum:</p> <pre><code>final_score = vs_weight * vector_score + fts_weight * fulltext_score\n</code></pre> Python <p>Enable weighted score fusion by specifying the <code>method</code> parameter as <code>\"weighted\"</code> in the <code>.fusion()</code> method.</p> <p>For example, to give more weight to vector search, set the <code>vs_weight</code> parameter to 0.7 and the <code>fts_weight</code> parameter to 0.3:</p> <pre><code>results = (\n    table.search(\n        \"AI database\", search_type=\"hybrid\"\n    )\n    .fusion(method=\"weighted\", vs_weight=0.7, fts_weight=0.3)\n    .limit(3)\n    .to_list()\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>vs_weight</code>: The weight of the vector search score.</li> <li><code>fts_weight</code>: The weight of the full-text search score.</li> </ul>"},{"location":"ai/guides/hybrid-search/#rerank-method","title":"Rerank Method","text":"<p>Hybrid search also supports reranking using reranker-specific models. </p> Python <p>Use the <code>rerank()</code> method to specify a reranker that sorts search results by relevance between the query and the documents.</p> <p>Example: Using JinaAI Reranker to rerank the hybrid search results</p> <pre><code>reranker = Reranker(\n    # Use the `jina-reranker-m0` model\n    model_name=\"jina_ai/jina-reranker-m0\",\n    api_key=\"{your-jinaai-api-key}\"\n)\n\nresults = (\n    table.search(\n        \"AI database\", search_type=\"hybrid\"\n    )\n    .fusion(method=\"rrf\", k=60)\n    .rerank(reranker, \"text\")\n    .limit(3)\n    .to_list()\n)\n</code></pre> <p>To check other reranker models, see the Reranking guide.</p>"},{"location":"ai/guides/image-search/","title":"Image search","text":"<p>Image search helps you find similar images by comparing their visual content, not just text or metadata. This feature is useful for e-commerce, content moderation, digital asset management, and any scenario where you need to search for or deduplicate images based on appearance.</p> <p>TiDB enables image search using vector search. With automatic embedding, you can generate image embeddings from image URLs, PIL images, or keyword text using a multimodal embedding model. TiDB then efficiently searches for similar vectors at scale.</p> <p>Tip</p> <p>For a complete example of image search, see the Pet image search demo.</p>"},{"location":"ai/guides/image-search/#basic-usage","title":"Basic usage","text":""},{"location":"ai/guides/image-search/#step-1-define-an-embedding-function","title":"Step 1. Define an embedding function","text":"<p>To generate image embeddings, you need an embedding model that supports image input.</p> <p>For demonstration, you can use Jina AI's multimodal embedding model to generate image embeddings.</p> <p>Go to Jina AI to create an API key, then initialize the embedding function as follows:</p> <pre><code>from pytidb.embeddings import EmbeddingFunction\n\nimage_embed = EmbeddingFunction(\n    # Or another provider/model that supports multimodal input\n    model_name=\"jina_ai/jina-embedding-v4\",\n    api_key=\"{your-jina-api-key}\",\n    multimodal=True,\n)\n</code></pre>"},{"location":"ai/guides/image-search/#step-2-create-a-table-and-vector-field","title":"Step 2. Create a table and vector field","text":"<p>Use <code>VectorField()</code> to define a vector field for storing image embeddings. Set the <code>source_field</code> parameter to specify the field that stores image URLs.</p> <pre><code>from pytidb.schema import TableModel, Field\n\nclass ImageItem(TableModel):\n    __tablename__ = \"image_items\"\n    id: int = Field(primary_key=True)\n    image_uri: str = Field()\n    image_vec: list[float] = image_embed.VectorField(\n        source_field=\"image_uri\"\n    )\n\ntable = client.create_table(schema=ImageItem, if_exists=\"overwrite\")\n</code></pre>"},{"location":"ai/guides/image-search/#step-3-insert-image-data","title":"Step 3. Insert image data","text":"<p>When you insert data, the <code>image_vec</code> field is automatically populated with the embedding generated from the <code>image_uri</code>.</p> <pre><code>table.bulk_insert([\n    ImageItem(image_uri=\"https://example.com/image1.jpg\"),\n    ImageItem(image_uri=\"https://example.com/image2.jpg\"),\n    ImageItem(image_uri=\"https://example.com/image3.jpg\"),\n])\n</code></pre>"},{"location":"ai/guides/image-search/#step-4-perform-image-search","title":"Step 4. Perform image search","text":"<p>Image search is a type of vector search. Automatic embedding lets you input an image URL, PIL image, or keyword text directly. All these inputs are converted to vector embeddings for similarity matching.</p>"},{"location":"ai/guides/image-search/#option-1-search-by-image-url","title":"Option 1: Search by image URL","text":"<p>Search for similar images by providing an image URL:</p> <pre><code>results = table.search(\"https://example.com/query.jpg\").limit(3).to_list()\n</code></pre> <p>The client converts the input image URL into a vector. TiDB then finds and returns the most similar images by comparing their vectors.</p>"},{"location":"ai/guides/image-search/#option-2-search-by-pil-image","title":"Option 2: Search by PIL image","text":"<p>You can also search for similar images by providing an image file or bytes:</p> <pre><code>from PIL import Image\n\nimage = Image.open(\"/path/to/query.jpg\")\n\nresults = table.search(image).limit(3).to_list()\n</code></pre> <p>The client converts the PIL image object into a Base64 string before sending it to the embedding model.</p>"},{"location":"ai/guides/image-search/#option-3-search-by-keyword-text","title":"Option 3: Search by keyword text","text":"<p>You can also search for similar images by providing keyword text. </p> <p>For example, if you are working on a pet image dataset, you can search for similar images by keywords like \"orange tabby cat\" or \"golden retriever puppy\".</p> <pre><code>results = table.search(\"orange tabby cat\").limit(3).to_list()\n</code></pre> <p>The keyword text will be converted to a vector embedding that captures the semantic meaning by the multimodal embedding model, and then a vector search will be performed to find the images whose embeddings are most similar to the keyword embedding.</p>"},{"location":"ai/guides/image-search/#see-also","title":"See also","text":"<ul> <li>Automatic embedding guide</li> <li>Vector search guide</li> <li>Pet image search demo</li> </ul>"},{"location":"ai/guides/joins/","title":"Multiple Table Joins","text":"<p>As a relational database, TiDB allows you to store diverse data in tables with different structures (for example: <code>chunks</code>, <code>documents</code>, <code>users</code>, <code>chats</code>) in a single database. Moreover, you can use joins to combine data from multiple tables to perform complex queries.</p>"},{"location":"ai/guides/joins/#basic-usage","title":"Basic Usage","text":""},{"location":"ai/guides/joins/#step-1-create-tables-and-insert-sample-data","title":"Step 1. Create tables and insert sample data","text":"PythonSQL <p>Assuming you have already connected to the TiDB database via TiDBClient:</p> <p>Create a <code>documents</code> table and insert some sample data:</p> <pre><code>from pytidb import Session\nfrom pytidb.schema import TableModel, Field\nfrom pytidb.sql import select\n\nclass Document(TableModel):\n    __tablename__ = \"documents\"\n    id: int = Field(primary_key=True)\n    title: str = Field(max_length=255)\n\nclient.create_table(schema=Document, if_exists=\"overwrite\")\nclient.table(\"documents\").truncate()\nclient.table(\"documents\").bulk_insert([\n    Document(id=1, title=\"The Power of Positive Thinking\"),\n    Document(id=2, title=\"The Happiness Advantage\"),\n    Document(id=3, title=\"The Art of Happiness\"),\n])\n</code></pre> <p>Create a <code>chunks</code> table and insert some sample data:</p> <pre><code>class Chunk(TableModel):\n    __tablename__ = \"chunks\"\n    id: int = Field(primary_key=True)\n    text: str = Field(max_length=255)\n    document_id: int = Field(foreign_key=\"documents.id\")\n\nclient.create_table(schema=Chunk, if_exists=\"overwrite\")\nclient.table(\"chunks\").truncate()\nclient.table(\"chunks\").bulk_insert([\n    Chunk(id=1, text=\"Positive thinking can change your life\", document_id=1),\n    Chunk(id=2, text=\"Happiness leads to success\", document_id=2),\n    Chunk(id=3, text=\"Finding joy in everyday moments\", document_id=3),\n])\n</code></pre> <p>Create a <code>documents</code> table and insert some sample data:</p> <pre><code>CREATE TABLE documents (\n    id INT PRIMARY KEY,\n    title VARCHAR(255) NOT NULL\n);\n\nINSERT INTO documents (id, title) VALUES \n    (1, 'The Power of Positive Thinking'),\n    (2, 'The Happiness Advantage'),\n    (3, 'The Art of Happiness');\n</code></pre> <p>Create a <code>chunks</code> table and insert some sample data:</p> <pre><code>CREATE TABLE chunks (\n    id INT PRIMARY KEY,\n    text VARCHAR(255) NOT NULL,\n    document_id INT NOT NULL,\n    FOREIGN KEY (document_id) REFERENCES documents(id)\n);\n\nINSERT INTO chunks (id, text, document_id) VALUES \n    (1, 'Positive thinking can change your life', 1),\n    (2, 'Happiness leads to success', 2),\n    (3, 'Finding joy in everyday moments', 3);\n</code></pre>"},{"location":"ai/guides/joins/#step-2-perform-a-join-query","title":"Step 2. Perform a join query","text":"PythonSQL <pre><code>with Session(client.db_engine) as db_session:\n    query = (\n        select(Chunk)\n        .join(Document, Chunk.document_id == Document.id)\n        .where(Document.title == \"The Power of Positive Thinking\")\n    )\n    chunks = db_session.exec(query).all()\n\n[(c.id, c.text, c.document_id) for c in chunks]\n</code></pre> <p>Perform a join query to combine data from the <code>chunks</code> and <code>documents</code> tables:</p> <pre><code>SELECT c.id, c.text, c.document_id\nFROM chunks c\nJOIN documents d ON c.document_id = d.id\nWHERE d.title = 'The Power of Positive Thinking';\n</code></pre>"},{"location":"ai/guides/raw-queries/","title":"Raw Queries","text":""},{"location":"ai/guides/raw-queries/#operate-data-with-raw-sql","title":"Operate data with raw SQL","text":"<p>You can use <code>client.execute()</code> method to execute <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code> and other data manipulation SQL statements.</p> <pre><code>client.execute(\"INSERT INTO chunks(text, user_id) VALUES ('sample text', 5)\")\n</code></pre>"},{"location":"ai/guides/raw-queries/#sql-injection-prevention","title":"SQL injection prevention","text":"<p>Both of the <code>execute</code> and <code>query</code> methods are support the Parameterized SQL feature, which help you avoid SQL injection while building dynamic SQL statements.</p> <pre><code>client.execute(\n    \"INSERT INTO chunks(text, user_id) VALUES (:text, :user_id)\",\n    {\n        \"text\": \"sample text\",\n        \"user_id\": 6,\n    },\n)\n</code></pre>"},{"location":"ai/guides/raw-queries/#query-data-with-rawsql","title":"Query data with rawSQL","text":"<p>You can use <code>client.query()</code> method to execute <code>SELECT</code>, <code>SHOW</code> and other query SQL statements.</p>"},{"location":"ai/guides/raw-queries/#output-query-result","title":"Output query result","text":"<p>The <code>client.query()</code> method will return a <code>SQLQueryResult</code> instance with some helper methods:</p> <ul> <li><code>to_pydantic()</code></li> <li><code>to_list()</code></li> <li><code>to_pandas()</code></li> <li><code>to_rows()</code></li> <li><code>scalar()</code></li> </ul>"},{"location":"ai/guides/raw-queries/#as-pydantic-model","title":"As Pydantic model","text":"<p>The <code>to_pydantic()</code> method will return a list of Pydantic models.</p> <pre><code>client.query(\"SELECT id, text, user_id FROM chunks\").to_pydantic()\n</code></pre>"},{"location":"ai/guides/raw-queries/#as-sqlalchemy-result-rows","title":"As SQLAlchemy result rows","text":"<p>The <code>to_rows()</code> method will return a list of tuple, every tuple represent of one row of data.</p> <pre><code>client.query(\"SHOW TABLES;\").to_rows()\n</code></pre>"},{"location":"ai/guides/raw-queries/#as-list-of-dict","title":"As list of dict","text":"<p>The <code>to_list()</code> method will convert the query result into a list of dict.</p> <pre><code>client.query(\n    \"SELECT id, text, user_id FROM chunks WHERE user_id = :user_id\",\n    {\n        \"user_id\": 3\n    }\n).to_list()\n</code></pre>"},{"location":"ai/guides/raw-queries/#as-pandas-dataframe","title":"As pandas DataFrame","text":"<p>The <code>to_pandas()</code> method to convert the query result to a <code>pandas.DataFrame</code>, which is displayed as human-friendly style on the notebook:</p> <pre><code>client.query(\"SELECT id, text, user_id FROM chunks\").to_pandas()\n</code></pre>"},{"location":"ai/guides/raw-queries/#as-scalar-value","title":"As scalar value","text":"<p>The <code>scalar()</code> method will return the first column of the first row of the result set.</p> <pre><code>client.query(\"SELECT COUNT(*) FROM chunks;\").scalar()\n</code></pre>"},{"location":"ai/guides/reranking/","title":"Reranking","text":"<p>Reranking is a technique used to improve the relevance and accuracy of search results by re-evaluating and reordering them using a dedicated rerank model.</p> <p>The search process works in two stages:</p> <ol> <li>Initial Retrieval: Vector search identifies the top <code>k</code> most similar documents from the collection</li> <li>Reranking: A reranking model evaluates these <code>k</code> documents based on the relevance between the query and the documents and reorders them to produce the final top <code>n</code> results (where <code>n</code> \u2264 <code>k</code>)</li> </ol> <p>This two-stage retrieval approach significantly improves both document relevance and accuracy.</p>"},{"location":"ai/guides/reranking/#basic-usage","title":"Basic Usage","text":"Python <p>PyTiDB provides the <code>Reranker</code> class that allows you to use reranker models from multiple third-party providers.</p> <ol> <li> <p>Create a reranker instance</p> <pre><code>from pytidb.rerankers import Reranker\n\nreranker = Reranker(model_name=\"{provider}/{model_name}\")\n</code></pre> </li> <li> <p>Apply reranker via <code>.rerank()</code> method</p> <pre><code>table.search(\"{query}\").rerank(reranker, \"{field_to_rerank}\").limit(3)\n</code></pre> </li> </ol>"},{"location":"ai/guides/reranking/#supported-providers","title":"Supported Providers","text":"<p>Here are some examples to use reranker models from third-party providers.</p>"},{"location":"ai/guides/reranking/#jina-ai","title":"Jina AI","text":"<p>To enable reranker provided by JinaAI, go to their website to create a API key.</p> <p>For example:</p> <pre><code>jinaai = Reranker(\n    # Using the `jina-reranker-m0` model\n    model_name=\"jina_ai/jina-reranker-m0\",\n    api_key=\"{your-jinaai-api-key}\"\n)\n</code></pre>"},{"location":"ai/guides/tables/","title":"Working with tables","text":"<p>TiDB uses tables to organize and store collections of related data. It provides flexible schema definition capabilities, allowing you to structure your tables according to your specific requirements.</p> <p>A table can contain multiple columns with different data types to store various kinds of data. Supported data types include text, numbers, vectors, binary data (<code>BLOB</code>), JSON, and more.</p> <p>Tip</p> <p>For a complete working example, see the basic example in our repository.</p>"},{"location":"ai/guides/tables/#create-a-table","title":"Create a table","text":""},{"location":"ai/guides/tables/#using-tablemodel","title":"Using TableModel","text":"<p>TiDB provides a <code>TableModel</code> class that represents the schema of a table. This class is compatible with the Pydantic Model and allows you to define the table structure in a declarative way.</p> <p>In the following example, you create a table named <code>items</code> with these columns:</p> <ul> <li><code>id</code>: a primary key column with an integer type</li> <li><code>content</code>: a text type column</li> <li><code>embedding</code>: a vector type column with 3 dimensions</li> <li><code>meta</code>: a JSON type column</li> </ul> PythonSQL <p>After you connect to the database using PyTiDB and obtain a <code>client</code> instance, you can create a table with the <code>create_table</code> method.</p> <pre><code>from pytidb.schema import TableModel, Field, VectorField\nfrom pytidb.datatype import TEXT, JSON\n\nclass Item(TableModel):\n    __tablename__ = \"items\"\n\n    id: int = Field(primary_key=True)\n    content: str = Field(sa_type=TEXT)\n    embedding: list[float] = VectorField(dimensions=3)\n    meta: dict = Field(sa_type=JSON, default_factory=dict)\n\ntable = client.create_table(schema=Item, if_exists=\"overwrite\")\n</code></pre> <p>The <code>create_table</code> method accepts these parameters:</p> <ul> <li><code>schema</code>: The <code>TableModel</code> class that defines your table structure.</li> <li><code>if_exists</code>: The creation mode of the table.<ul> <li><code>raise</code> (default): Creates the table if it does not exist; raises an error if it already exists.</li> <li><code>skip</code>: Creates the table if it does not exist; does nothing if it already exists.</li> <li><code>overwrite</code>: Drops the existing table and creates a new one. This is useful for testing and development, but not recommended for production environments.</li> </ul> </li> </ul> <p>Once the table is created, you can use the <code>table</code> object to insert, update, delete, and query data.</p> <p>Use the <code>CREATE TABLE</code> statement to create a table.</p> <pre><code>CREATE TABLE items (\n    id INT PRIMARY KEY,\n    content TEXT,\n    embedding VECTOR(3),\n    meta JSON\n);\n</code></pre>"},{"location":"ai/guides/tables/#add-data-to-a-table","title":"Add data to a table","text":""},{"location":"ai/guides/tables/#with-tablemodel","title":"With TableModel","text":"<p>You can use a <code>TableModel</code> instance to represent a record and insert it into the table.</p> <p>To insert a single record:</p> PythonSQL <p>Use the <code>table.insert()</code> method to insert a single record into the table.</p> <pre><code>table.insert(\n    Item(\n        id=1,\n        content=\"TiDB is a distributed SQL database\",\n        embedding=[0.1, 0.2, 0.3],\n        meta={\"category\": \"database\"},\n    )\n)\n</code></pre> <p>Use the <code>INSERT INTO</code> statement to insert a single record into the table.</p> <pre><code>INSERT INTO items(id, content, embedding, meta)\nVALUES (1, 'TiDB is a distributed SQL database', '[0.1, 0.2, 0.3]', '{\"category\": \"database\"}');\n</code></pre> <p>To insert multiple records:</p> PythonSQL <p>Use the <code>table.bulk_insert()</code> method to insert multiple records into the table.</p> <pre><code>table.bulk_insert([\n    Item(\n        id=2,\n        content=\"GPT-4 is a large language model\",\n        embedding=[0.4, 0.5, 0.6],\n        meta={\"category\": \"llm\"},\n    ),\n    Item(\n        id=3,\n        content=\"LlamaIndex is a Python library for building AI-powered applications\",\n        embedding=[0.7, 0.8, 0.9],\n        meta={\"category\": \"rag\"},\n    ),\n])\n</code></pre> <p>Use the <code>INSERT INTO</code> statement to insert multiple records into the table.</p> <pre><code>INSERT INTO items(id, content, embedding, meta)\nVALUES\n    (2, 'GPT-4 is a large language model', '[0.4, 0.5, 0.6]', '{\"category\": \"llm\"}'),\n    (3, 'LlamaIndex is a Python library for building AI-powered applications', '[0.7, 0.8, 0.9]', '{\"category\": \"rag\"}');\n</code></pre>"},{"location":"ai/guides/tables/#with-dict","title":"With Dict","text":"<p>You can also use <code>dict</code> to represent records and insert them into the table. This approach is more flexible and doesn't require to use a <code>TableModel</code> to insert data.</p> <p>To insert a single record:</p> PythonSQL <p>Use the <code>table.insert()</code> method with a dictionary to insert a single record into the table.</p> <pre><code>table.insert({\n    \"id\": 1,\n    \"content\": \"TiDB is a distributed SQL database\",\n    \"embedding\": [0.1, 0.2, 0.3],\n    \"meta\": {\"category\": \"database\"},\n})\n</code></pre> <p>Use the <code>INSERT INTO</code> statement to insert a single record into the table.</p> <pre><code>INSERT INTO items(id, content, embedding, meta)\nVALUES (1, 'TiDB is a distributed SQL database', '[0.1, 0.2, 0.3]', '{\"category\": \"database\"}');\n</code></pre>"},{"location":"ai/guides/tables/#save-data-to-a-table","title":"Save data to a table","text":"<p>The <code>save</code> method provides a convenient way to insert or update a single record. If a record with the specified primary key does not exist, it creates a new record. If the record already exists, it overwrites the entire record.</p> <p>Note</p> <p>If a record ID already exists, <code>table.save()</code> function overwrites the entire record. To change only part of a record, use <code>table.update()</code>.</p> PythonSQL <p>Use the <code>table.save()</code> method to save a single record to the table.</p> <p>Example: Save a new record</p> <pre><code>saved_record = table.save(\n    Item(\n        id=4,\n        content=\"Vector databases enable AI applications\",\n        embedding=[1.0, 1.1, 1.2],\n        meta={\"category\": \"vector-db\"},\n    )\n)\n</code></pre> <p>Example: Save an existing record (overwrites the entire record)</p> <pre><code># This overwrites the entire record with id=1\nupdated_record = table.save(\n    Item(\n        id=1,  # Existing ID\n        content=\"Updated content for TiDB\",\n        embedding=[0.2, 0.3, 0.4],\n        meta={\"category\": \"updated\"},\n    )\n)\n</code></pre> <p>Use the <code>INSERT ... ON DUPLICATE KEY UPDATE</code> statement to save a record.</p> <p>Example: Save a new record or update if it exists</p> <pre><code>INSERT INTO items(id, content, embedding, meta)\nVALUES (4, 'Vector databases enable AI applications', '[1.0, 1.1, 1.2]', '{\"category\": \"vector-db\"}')\nON DUPLICATE KEY UPDATE\n    content = VALUES(content),\n    embedding = VALUES(embedding),\n    meta = VALUES(meta);\n</code></pre>"},{"location":"ai/guides/tables/#query-data-from-a-table","title":"Query data from a table","text":"<p>To fetch records from a table:</p> PythonSQL <p>Use the <code>table.query()</code> method to fetch the records from the table.</p> <p>Example: Fetch the first 10 records</p> <pre><code>result = table.query(limit=10).to_list()\n</code></pre> <p>Use the <code>SELECT</code> statement to fetch the records from the table.</p> <p>Example: Fetch the first 10 records</p> <pre><code>SELECT * FROM items LIMIT 10;\n</code></pre> <p>To fetch records based on query conditions:</p> PythonSQL <p>Pass the <code>filters</code> parameter to the <code>table.query()</code> method.</p> <pre><code>result = table.query(\n    filters={\"meta.category\": \"database\"},\n    limit=10\n).to_list()\n</code></pre> <p>Use the <code>WHERE</code> clause to filter records.</p> <p>Example: Fetch the 10 records with category \"database\"</p> <pre><code>SELECT * FROM items WHERE meta-&gt;&gt;'$.category' = 'database' LIMIT 10;\n</code></pre> <p>For a complete list of supported filter operations and examples, refer to the filtering guide.</p>"},{"location":"ai/guides/tables/#update-data-in-a-table","title":"Update data in a table","text":"PythonSQL <p>Use the <code>table.update()</code> method to update records with filters.</p> <p>Example: Update the record whose <code>id</code> equals 1</p> <pre><code>table.update(\n    values={\n        \"content\": \"TiDB Cloud Starter is a fully managed, auto-scaling cloud database service\"\n        \"embedding\": [0.1, 0.2, 0.4],\n        \"meta\": {\"category\": \"dbass\"},\n    },\n    filters={\n        \"id\": 1\n    },\n)\n</code></pre> <p>Use the <code>UPDATE</code> statement to update records with filters.</p> <p>Example: Update the record whose <code>id</code> equals 1</p> <pre><code>UPDATE items\nSET\n    content = 'TiDB Cloud Starter is a fully managed, auto-scaling cloud database service',\n    embedding = '[0.1, 0.2, 0.4]',\n    meta = '{\"category\": \"dbass\"}'\nWHERE\n    id = 1;\n</code></pre>"},{"location":"ai/guides/tables/#delete-from-a-table","title":"Delete from a table","text":"PythonSQL <p>Use the <code>table.delete()</code> method to delete records with filters.</p> <p>Example: Delete the record where <code>id</code> equals 2</p> <pre><code>table.delete(\n    filters={\n        \"id\": 2\n    }\n)\n</code></pre> <p>Use the <code>DELETE</code> statement to delete records with filters.</p> <p>Example: Delete the record where <code>id</code> equals 2</p> <pre><code>DELETE FROM items WHERE id = 2;\n</code></pre>"},{"location":"ai/guides/tables/#truncate-a-table","title":"Truncate a table","text":"PythonSQL <p>To remove all data from the table but keep the table structure, use the <code>table.truncate()</code> method.</p> <pre><code>table.truncate()\n</code></pre> <p>To check that the table is truncated, verify that it contains 0 rows.</p> <pre><code>table.rows()\n</code></pre> <p>To remove all data from the table but keep the table structure, use the <code>TRUNCATE TABLE</code> statement.</p> <pre><code>TRUNCATE TABLE items;\n</code></pre> <p>To check that the table is truncated, verify that it contains 0 rows.</p> <pre><code>SELECT COUNT(*) FROM items;\n</code></pre>"},{"location":"ai/guides/tables/#drop-a-table","title":"Drop a table","text":"PythonSQL <p>To permanently remove a table from the database, use the <code>client.drop_table()</code> method.</p> <pre><code>client.drop_table(\"items\")\n</code></pre> <p>To check that the table is removed from the database:</p> <pre><code>client.table_names()\n</code></pre> <p>To permanently remove a table from the database, use the <code>DROP TABLE</code> statement.</p> <pre><code>DROP TABLE items;\n</code></pre> <p>To check that the table is removed from the database:</p> <pre><code>SHOW TABLES;\n</code></pre>"},{"location":"ai/guides/transaction/","title":"Transaction","text":"<p>TiDB supports ACID transactions, which ensure data consistency and reliability.</p>"},{"location":"ai/guides/transaction/#basic-usage","title":"Basic Usage","text":"Python <pre><code>with client.session() as session:\n    initial_total_balance = session.query(\"SELECT SUM(balance) FROM players\").scalar()\n\n    # Transfer 10 coins from player 1 to player 2\n    session.execute(\"UPDATE players SET balance = balance - 10 WHERE id = 1\")\n    session.execute(\"UPDATE players SET balance = balance + 10 WHERE id = 2\")\n\n    session.commit()\n    # or session.rollback()\n\n    final_total_balance = session.query(\"SELECT SUM(balance) FROM players\").scalar()\n    assert final_total_balance == initial_total_balance\n</code></pre>"},{"location":"ai/guides/transaction/#see-also","title":"See also","text":"<ul> <li>TiDB Develop Guide - Transaction</li> <li>TiDB Docs- SQL Reference - Transactions</li> </ul>"},{"location":"ai/guides/vector-search/","title":"Vector Search","text":"<p>Vector search uses semantic similarity to help you find the most relevant records, even if your query does not explicitly include all the keywords.</p> <p>Tip</p> <p>For a complete example of vector search, see the vector-search example.</p>"},{"location":"ai/guides/vector-search/#basic-usage","title":"Basic Usage","text":"<p>This section shows you how to use vector search in your application in minimal steps. Before you start, you need to connect to the database.</p>"},{"location":"ai/guides/vector-search/#step-1-create-a-table-with-a-vector-field","title":"Step 1. Create a table with a vector field","text":"PythonSQL <p>You can use <code>client.create_table()</code> to create a table and use <code>VectorField</code> to define a vector field.</p> <p>In this example, we create a table named <code>documents</code> with four columns:</p> <ul> <li><code>id</code>: The primary key of the table.</li> <li><code>text</code>: The text content of the document.</li> <li><code>text_vec</code>: The vector embedding of the text content.</li> <li><code>meta</code>: The metadata of the document, which is a JSON object.</li> </ul> <pre><code>from pytidb.schema import TableModel, Field, VectorField\nfrom pytidb.datatype import TEXT, JSON\n\nclass Document(TableModel):\n    __tablename__ = \"documents\"\n\n    id: int = Field(primary_key=True)\n    text: str = Field(sa_type=TEXT)\n    text_vec: list[float] = VectorField(dimensions=3)\n    meta: dict = Field(sa_type=JSON, default_factory=dict)\n\ntable = client.create_table(schema=Document, if_exists=\"overwrite\")\n</code></pre> <p>The <code>VectorField</code> class accepts the following parameters:</p> <ul> <li><code>dimensions</code>: The number of dimensions of the vector. Once specified, only vectors with this exact dimension can be stored in this field.</li> <li><code>index</code>: Whether to create a vector index for the vector field. Defaults to <code>True</code>.</li> <li><code>distance_metric</code>: The distance metric to use for the vector index. Supported values:<ul> <li><code>DistanceMetric.COSINE</code> (default): Cosine distance metric, suitable for measuring text similarity</li> <li><code>DistanceMetric.L2</code>: L2 distance metric, suitable for capturing overall difference</li> </ul> </li> </ul> <p>You can use the <code>CREATE TABLE</code> statement to create a table and using <code>VECTOR</code> type to define a vector column.</p> <pre><code>CREATE TABLE documents (\n    id INT PRIMARY KEY,\n    text TEXT,\n    text_vec VECTOR(3),\n    VECTOR INDEX `vec_idx_text_vec`((VEC_COSINE_DISTANCE(`text_vec`)))\n);\n</code></pre> <p>In this example:</p> <ul> <li>The <code>text_vec</code> column is defined as a <code>VECTOR</code> type with 3 dimensions, it means that the vector to be stored in this column must have 3 dimensions.</li> <li>A vector index is created using the <code>VEC_COSINE_DISTANCE</code> function to optimize vector search performance</li> </ul> <p>TiDB supports two distance functions for vector indexes:</p> <ul> <li><code>VEC_COSINE_DISTANCE</code>: Calculates the cosine distance between two vectors</li> <li><code>VEC_L2_DISTANCE</code>: Calculates L2 distance (Euclidean distance) between two vectors</li> </ul>"},{"location":"ai/guides/vector-search/#step-2-insert-vector-data-into-the-table","title":"Step 2. Insert vector data into the table","text":"<p>For demonstration purposes, insert some text and their corresponding vector embeddings into the table. In this example, we use simple 3-dimensional vectors.</p> <p>We insert three documents:</p> <ul> <li><code>dog</code> with the vector embedding <code>[1, 2, 1]</code></li> <li><code>fish</code> with the vector embedding <code>[1, 2, 4]</code></li> <li><code>tree</code> with the vector embedding <code>[1, 0, 0]</code></li> </ul> PythonSQL <pre><code>table.bulk_insert([\n    Document(text=\"dog\", text_vec=[1,2,1], meta={\"category\": \"animal\"}),\n    Document(text=\"fish\", text_vec=[1,2,4], meta={\"category\": \"animal\"}),\n    Document(text=\"tree\", text_vec=[1,0,0], meta={\"category\": \"plant\"}),\n])\n</code></pre> <pre><code>INSERT INTO documents (id, text, text_vec, meta)\nVALUES\n    (1, 'dog', '[1,2,1]', '{\"category\": \"animal\"}'),\n    (2, 'fish', '[1,2,4]', '{\"category\": \"animal\"}'),\n    (3, 'tree', '[1,0,0]', '{\"category\": \"plant\"}');\n</code></pre> <p>Tip</p> <p>In real-world applications, vector embeddings are usually generated by an embedding model.</p> <p>For convenience, pytidb provides an auto embedding feature that can automatically generate vector embeddings for your text fields when you insert, update, or search\u2014no manual processing needed.</p> <p>For details, see the Auto Embedding guide.</p>"},{"location":"ai/guides/vector-search/#step-3-perform-vector-search","title":"Step 3. Perform vector search","text":"<p>Vector search uses vector distance metrics to measure the similarity and relevance between vectors. The closer the distance, the more relevant the record. To find the most relevant documents in the table, you need to specify a query vector. </p> <p>In this example, we assume the query is <code>A swimming animal</code> and its vector embedding is <code>[1, 2, 3]</code>.</p> PythonSQL <p>You can use the <code>table.search()</code> method to perform vector search, which uses <code>search_mode=\"vector\"</code> by default.</p> <pre><code>table.search([1, 2, 3]).limit(3).to_list()\n</code></pre> Execution result<pre><code>[\n    {\"id\": 2, \"text\": \"fish\", \"text_vec\": [1,2,4], \"_distance\": 0.00853986601633272},\n    {\"id\": 1, \"text\": \"dog\", \"text_vec\": [1,2,1], \"_distance\": 0.12712843905603044},\n    {\"id\": 3, \"text\": \"tree\", \"text_vec\": [1,0,0], \"_distance\": 0.7327387580875756},\n]\n</code></pre> <p>The result shows that the most relevant document is <code>fish</code> with a distance of <code>0.00853986601633272</code>.</p> <p>You can use the <code>ORDER BY &lt;distance_function&gt;(&lt;column_name&gt;, &lt;query_vector&gt;) LIMIT &lt;n&gt;</code> clause in the <code>SELECT</code> statement to get the n nearest neighbors of the query vector.</p> <p>In this example, we use the <code>vec_cosine_distance</code> function to calculate the cosine distance between the vectors stored in the <code>text_vec</code> column and the provided query vector <code>[1, 2, 3]</code>.</p> <pre><code>SELECT id, text, vec_cosine_distance(text_vec, '[1,2,3]') AS distance\nFROM documents\nORDER BY distance\nLIMIT 3;\n</code></pre> Execution result<pre><code>+----+----------+---------------------+\n| id | text     | distance            |\n+----+----------+---------------------+\n|  2 | fish     | 0.00853986601633272 |\n|  1 | dog      | 0.12712843905603044 |\n|  3 | tree     |  0.7327387580875756 |\n+----+----------+---------------------+\n3 rows in set (0.15 sec)\n</code></pre> <p>The result shows that the most relevant document is <code>fish</code> with a distance of <code>0.00853986601633272</code>.</p>"},{"location":"ai/guides/vector-search/#distance-metrics","title":"Distance metrics","text":"<p>Distance metrics are a measure of the similarity between a pair of vectors. Currently, TiDB supports the following distance metrics:</p> PythonSQL <p>The <code>table.search()</code> API supports the following distance metrics:</p> Metric Name Description Best For <code>DistanceMetric.COSINE</code> Calculates the cosine distance between two vectors (default). Measures the angle between vectors. Text embeddings, semantic search <code>DistanceMetric.L2</code> Calculates the L2 distance (Euclidean distance) between two vectors. Measures the straight-line distance. Image features <p>To change the distance metric used for vector search, use the <code>.distance_metric()</code> method.</p> <p>Example: Use the L2 distance metric</p> <pre><code>from pytidb.schema import DistanceMetric\n\nresults = (\n    table.search([1, 2, 3])\n        .distance_metric(DistanceMetric.L2)\n        .limit(10)\n        .to_list()\n)\n</code></pre> <p>In SQL, you can use the following built-in functions to calculate vector distances directly in your queries:</p> Function Name Description <code>VEC_L2_DISTANCE</code> Calculates L2 distance (Euclidean distance) between two vectors <code>VEC_COSINE_DISTANCE</code> Calculates the cosine distance between two vectors <code>VEC_NEGATIVE_INNER_PRODUCT</code> Calculates the negative of the inner product between two vectors <code>VEC_L1_DISTANCE</code> Calculates L1 distance (Manhattan distance) between two vectors"},{"location":"ai/guides/vector-search/#distance-threshold","title":"Distance threshold","text":"<p>The <code>table.search()</code> API allows you to set a distance threshold to control the similarity of the returned results. By specifying this threshold, you can exclude less similar vectors and return only those that meet your relevance criteria.</p> PythonSQL <p>Use the <code>.distance_threshold()</code> method to set a maximum distance for the search results. Only records with a distance less than the threshold are returned.</p> <p>Example: Only return documents with a distance less than 0.5</p> <pre><code>results = table.search([1, 2, 3]).distance_threshold(0.5).limit(10).to_list()\n</code></pre> <p>In SQL, use the <code>HAVING</code> clause with a distance function to filter results by distance:</p> <p>Example: Only return documents with a distance less than 0.1</p> <pre><code>SELECT id, text, vec_cosine_distance(text_vec, '[1,2,3]') AS distance\nFROM documents\nHAVING distance &lt; 0.1\nORDER BY distance\nLIMIT 10;\n</code></pre>"},{"location":"ai/guides/vector-search/#distance-range","title":"Distance range","text":"<p>The <code>table.search()</code> API also supports specifying a distance range to further refine the results.</p> PythonSQL <p>Use the <code>.distance_range()</code> method to set both minimum and maximum distance values. Only records with a distance within this range are returned.</p> <p>Example: Only return documents with a distance between 0.01 and 0.05</p> <pre><code>results = table.search([1, 2, 3]).distance_range(0.01, 0.05).limit(10).to_list()\n</code></pre> <p>To specify a distance range in SQL, use <code>BETWEEN</code> or other comparison operators in the <code>HAVING</code> clause:</p> <p>Example: Only return documents with a distance between 0.01 and 0.05</p> <pre><code>SELECT id, text, vec_l2_distance(text_vec, '[1,2,3]') AS distance\nFROM documents\nHAVING distance BETWEEN 0.01 AND 0.05\nORDER BY distance\nLIMIT 10;\n</code></pre>"},{"location":"ai/guides/vector-search/#metadata-filtering","title":"Metadata filtering","text":"<p>As a relational database, TiDB supports a rich set of SQL operators and allows flexible combinations of filtering conditions.</p> <p>For vector search in TiDB, you can apply metadata filtering on scalar fields (e.g., integers, strings) or JSON fields.</p> <p>Typically, vector search combined with metadata filtering operates in two modes:</p> <ul> <li>Post-filtering: In a two-stage retrieval process, TiDB first performs vector search to retrieve the top-k candidate results from the entire vector space, then applies the filter to this candidate set. The vector search stage typically leverages a vector index for efficiency.</li> <li>Pre-filtering: The filter is applied before vector search. If the filter is highly selective and the filtered field is indexed with a scalar index, this approach can significantly reduce the search space and improve performance.</li> </ul>"},{"location":"ai/guides/vector-search/#post-filtering","title":"Post-filtering","text":"PythonSQL <p>Use the <code>.filter()</code> method with a filter dictionary to apply filtering to vector search.</p> <p>By default, the <code>table.search()</code> API uses post-filtering mode to maximize search performance with the vector index.</p> <p>Example: Vector search with post-filtering</p> <pre><code>results = (\n    table.search([1, 2, 3])\n        # The `meta` is a JSON field, and its value is a JSON object\n        # like {\"category\": \"animal\"}\n        .filter({\"meta.category\": \"animal\"})\n        .num_candidate(50)\n        .limit(10)\n        .to_list()\n)\n</code></pre> <p>Tip</p> <p>When using a vector index, if the final <code>limit</code> is very small, the accuracy of the results may decrease. You can use the <code>.num_candidate()</code> method to control how many candidates to retrieve from the vector index during the vector search phase, without changing the <code>limit</code> parameter.</p> <p>A higher <code>num_candidate</code> value generally improves recall but may reduce query performance. Adjust this value based on your dataset and accuracy requirements.</p> <p>Currently, vector indexes are only effective in strict ANN (Approximate Nearest Neighbor) queries, such as:</p> <pre><code>SELECT * FROM &lt;table&gt; ORDER BY &lt;distance_func&gt;(&lt;column&gt;) LIMIT &lt;n&gt;\n</code></pre> <p>In other words, you cannot use a <code>WHERE</code> clause together with a vector index in the same query.</p> <p>If you need to combine vector search with additional filtering conditions, you can use the post-filtering pattern. In this approach, the ANN query will be divided into two parts:</p> <ul> <li>The inner query performs the vector search using the vector index.</li> <li>The outer query applies the <code>WHERE</code> condition to filter the results.</li> </ul> <pre><code>SELECT *\nFROM (\n    SELECT id, text, meta, vec_cosine_distance(text_vec, '[1,2,3]') AS distance\n    FROM documents\n    ORDER BY distance\n    LIMIT 50\n) candidates\nWHERE meta-&gt;&gt;'$.category' = 'animal'\nORDER BY distance\nLIMIT 10;\n</code></pre> <p>Tip</p> <p>The post-filtering pattern may lead to false positives \u2014 for example, the inner query may retrieve the top 50 most similar records, but none of them match the <code>WHERE</code> condition.</p> <p>To mitigate this, you can increase the <code>LIMIT</code> value (e.g., 50) in the inner query to fetch more candidates, improving the chances of returning enough valid results after filtering.</p> <p>For supported SQL operators, see Operators in the TiDB Cloud documentation.</p>"},{"location":"ai/guides/vector-search/#pre-filtering","title":"Pre-filtering","text":"PythonSQL <p>To enable pre-filtering, set the <code>prefilter</code> parameter to <code>True</code> in the <code>.filter()</code> method.</p> <p>Example: Vector search with pre-filtering</p> <pre><code>results = (\n    table.search([1, 2, 3])\n        .filter({\"meta.category\": \"animal\"}, prefilter=True)\n        .limit(10)\n        .to_list()\n)\n</code></pre> <p>For supported filter operators, see Filtering.</p> <p>In SQL, use the <code>-&gt;&gt;</code> operator or <code>JSON_EXTRACT</code> to access JSON fields in the <code>WHERE</code> clause:</p> <pre><code>SELECT id, text, meta, vec_cosine_distance(text_vec, '[1,2,3]') AS distance\nFROM documents\nWHERE meta-&gt;&gt;'$.category' = 'animal'\nORDER BY distance\nLIMIT 10;\n</code></pre> <p>For supported SQL operators, see Operators in the TiDB Cloud documentation.</p>"},{"location":"ai/guides/vector-search/#multiple-vector-fields","title":"Multiple vector fields","text":"<p>TiDB supports defining multiple vector columns in a single table, allowing you to store and search different types of vector embeddings. </p> <p>For example, you can store both text embeddings and image embeddings in the same table, making it convenient to manage multi-modal data.</p> PythonSQL <p>You can define multiple vector fields in the schema and perform vector search on the specified vector field by using the <code>.vector_column()</code> method.</p> <p>Example: Specify the vector field to search on</p> <pre><code># Create a table with multiple vector fields\nclass RichTextDocument(TableModel):\n    __tablename__ = \"rich_text_documents\"\n    id: int = Field(primary_key=True)\n    text: str = Field(sa_type=TEXT)\n    text_vec: list[float] = VectorField(dimensions=3)\n    image_url: str\n    image_vec: list[float] = VectorField(dimensions=3)\n\ntable = client.create_table(schema=RichTextDocument, if_exists=\"overwrite\")\n\n# Insert sample data ...\n\n# Search using image vector field\nresults = (\n    table.search([1, 2, 3])\n        .vector_column(\"image_vec\")\n        .distance_metric(DistanceMetric.COSINE)\n        .limit(10)\n        .to_list()\n)\n</code></pre> <p>You can create multiple vector columns in a table and search them using suitable distance functions:</p> <pre><code>-- Create a table with multiple vector fields\nCREATE TABLE rich_text_documents (\n    id BIGINT PRIMARY KEY,\n    text TEXT,\n    text_vec VECTOR(3),\n    image_url VARCHAR(255),\n    image_vec VECTOR(3)\n);\n\n-- Insert sample data ...\n\n-- Search using text vector\nSELECT id, image_url, vec_l2_distance(image_vec, '[4,5,6]') AS image_distance\nFROM rich_text_documents\nORDER BY image_distance\nLIMIT 10;\n</code></pre>"},{"location":"ai/guides/vector-search/#output-search-results","title":"Output search results","text":"Python <p>The <code>table.search()</code> API lets you convert search results into several common data processing formats:</p>"},{"location":"ai/guides/vector-search/#as-sqlalchemy-result-rows","title":"As SQLAlchemy result rows","text":"<p>To work with raw SQLAlchemy result rows, use:</p> <pre><code>table.search([1, 2, 3]).limit(10).to_rows()\n</code></pre>"},{"location":"ai/guides/vector-search/#as-a-list-of-python-dictionaries","title":"As a list of Python dictionaries","text":"<p>For easier manipulation in Python, convert the results to a list of dictionaries:</p> <pre><code>table.search([1, 2, 3]).limit(10).to_list()\n</code></pre>"},{"location":"ai/guides/vector-search/#as-a-pandas-dataframe","title":"As a pandas DataFrame","text":"<p>To display results in a user-friendly table\u2014especially useful in Jupyter notebooks\u2014convert them to a pandas DataFrame:</p> <pre><code>table.search([1, 2, 3]).limit(10).to_pandas()\n</code></pre>"},{"location":"ai/guides/vector-search/#as-a-list-of-pydantic-model-instances","title":"As a list of Pydantic model instances","text":"<p>The <code>TableModel</code> class can also be used as a Pydantic model to represent data entities. To work with results as Pydantic model instances, use:</p> <pre><code>table.search([1, 2, 3]).limit(10).to_pydantic()\n</code></pre>"},{"location":"ai/integrations/embedding-cohere/","title":"Integrate TiDB Vector Search with Cohere Embeddings API","text":"<p>This tutorial demonstrates how to use Cohere to generate text embeddings, store them in TiDB vector storage, and perform semantic search.</p> <p>Info</p> <p>Currently, Server-Side Auto Embedding is only available on TiDB Cloud Starter clusters in the following AWS regions:</p> <ul> <li><code>Frankfurt (eu-central-1)</code></li> <li><code>Oregon (us-west-2)</code></li> <li><code>N. Virginia (us-east-1)</code></li> </ul>"},{"location":"ai/integrations/embedding-cohere/#cohere-embeddings","title":"Cohere Embeddings","text":"<p>Cohere offers multilingual embedding models for search, RAG, and classification. The latest <code>embed-v4.0</code> model supports text, images, and mixed content. You can use the Cohere Embeddings API with TiDB through the AI SDK or native SQL functions for automatic embedding generation.</p>"},{"location":"ai/integrations/embedding-cohere/#supported-models","title":"Supported Models","text":"Model Name Dimensions Max Input Tokens Description <code>cohere/embed-v4.0</code> 256, 512, 1024, 1536 (default) 128k Latest multimodal model supporting text, images, and mixed content (PDFs) <code>cohere/embed-english-v3.0</code> 1024 512 High-performance English embedding model optimized for search and classification <code>cohere/embed-multilingual-v3.0</code> 1024 512 Multilingual model supporting 100+ languages <code>cohere/embed-english-light-v3.0</code> 384 512 Lightweight English model for faster processing with similar performance <code>cohere/embed-multilingual-light-v3.0</code> 384 512 Lightweight multilingual model for faster processing with similar performance <p>For a complete list of supported models and detailed specifications, see the Cohere Embeddings Documentation.</p>"},{"location":"ai/integrations/embedding-cohere/#usage-example","title":"Usage example","text":"<p>This example demonstrates creating a vector table, inserting documents, and performing similarity search using Cohere embedding models.</p>"},{"location":"ai/integrations/embedding-cohere/#step-1-connect-to-the-database","title":"Step 1: Connect to the database","text":"PythonSQL <pre><code>from pytidb import TiDBClient\n\ntidb_client = TiDBClient.connect(\n    host=\"{gateway-region}.prod.aws.tidbcloud.com\",\n    port=4000,\n    username=\"{prefix}.root\",\n    password=\"{password}\",\n    database=\"{database}\",\n    ensure_db=True,\n)\n</code></pre> <pre><code>mysql -h {gateway-region}.prod.aws.tidbcloud.com \\\n    -P 4000 \\\n    -u {prefix}.root \\\n    -p{password} \\\n    -D {database}\n</code></pre>"},{"location":"ai/integrations/embedding-cohere/#step-2-configure-the-api-key","title":"Step 2: Configure the API key","text":"<p>Create your API key from the Cohere Dashboard and bring your own key (BYOK) to use the embedding service.</p> PythonSQL <p>Configure the API key for the Cohere embedding provider using the TiDB Client:</p> <pre><code>tidb_client.configure_embedding_provider(\n    provider=\"cohere\",\n    api_key=\"{your-cohere-api-key}\",\n)\n</code></pre> <p>Set the API key for the Cohere embedding provider using SQL:</p> <pre><code>SET @@GLOBAL.TIDB_EXP_EMBED_COHERE_API_KEY = \"{your-cohere-api-key}\";\n</code></pre>"},{"location":"ai/integrations/embedding-cohere/#step-3-create-a-vector-table","title":"Step 3: Create a vector table","text":"<p>Create a table with a vector field that uses the <code>cohere/embed-v4.0</code> model to generate 1536-dimensional vectors (default dimension):</p> PythonSQL <pre><code>from pytidb.schema import TableModel, Field\nfrom pytidb.embeddings import EmbeddingFunction\nfrom pytidb.datatype import TEXT\n\nclass Document(TableModel):\n    __tablename__ = \"sample_documents\"\n    id: int = Field(primary_key=True)\n    content: str = Field(sa_type=TEXT)\n    embedding: list[float] = EmbeddingFunction(\n        model_name=\"cohere/embed-v4.0\"\n    ).VectorField(source_field=\"content\")\n\ntable = tidb_client.create_table(schema=Document, if_exists=\"overwrite\")\n</code></pre> <pre><code>CREATE TABLE sample_documents (\n    `id`        INT PRIMARY KEY,\n    `content`   TEXT,\n    `embedding` VECTOR(1536) GENERATED ALWAYS AS (EMBED_TEXT(\n        \"cohere/embed-v4.0\",\n        `content`\n    )) STORED\n);\n</code></pre>"},{"location":"ai/integrations/embedding-cohere/#step-4-insert-data-into-the-table","title":"Step 4: Insert data into the table","text":"PythonSQL <p>Use the <code>table.insert()</code> or <code>table.bulk_insert()</code> API to add data:</p> <pre><code>documents = [\n    Document(id=1, content=\"Python: High-level programming language for data science and web development.\"),\n    Document(id=2, content=\"Python snake: Non-venomous constrictor found in tropical regions.\"),\n    Document(id=3, content=\"Python framework: Django and Flask are popular web frameworks.\"),\n    Document(id=4, content=\"Python libraries: NumPy and Pandas for data analysis.\"),\n    Document(id=5, content=\"Python ecosystem: Rich collection of packages and tools.\"),\n]\ntable.bulk_insert(documents)\n</code></pre> <p>Insert data using the <code>INSERT INTO</code> statement:</p> <pre><code>INSERT INTO sample_documents (id, content)\nVALUES\n    (1, \"Python: High-level programming language for data science and web development.\"),\n    (2, \"Python snake: Non-venomous constrictor found in tropical regions.\"),\n    (3, \"Python framework: Django and Flask are popular web frameworks.\"),\n    (4, \"Python libraries: NumPy and Pandas for data analysis.\"),\n    (5, \"Python ecosystem: Rich collection of packages and tools.\");\n</code></pre>"},{"location":"ai/integrations/embedding-cohere/#step-5-search-for-similar-documents","title":"Step 5: Search for similar documents","text":"PythonSQL <p>Use the <code>table.search()</code> API to perform vector search:</p> <pre><code>results = table.search(\"How to learn Python programming?\") \\\n    .limit(2) \\\n    .to_list()\nprint(results)\n</code></pre> <p>Use the <code>VEC_EMBED_COSINE_DISTANCE</code> function to perform vector search based on cosine distance metric:</p> <pre><code>SELECT\n    `id`,\n    `content`,\n    VEC_EMBED_COSINE_DISTANCE(embedding, \"How to learn Python programming?\") AS _distance\nFROM sample_documents\nORDER BY _distance ASC\nLIMIT 2;\n</code></pre>"},{"location":"ai/integrations/embedding-gemini/","title":"Integrate TiDB Vector Search with Google Gemini Embeddings API","text":"<p>This tutorial demonstrates how to use Google Gemini to generate embeddings for text and image data, store them in TiDB vector storage, and perform semantic search.</p> <p>Info</p> <p>Currently, Server-Side Auto Embedding is only available on TiDB Cloud Starter clusters in the following AWS regions:</p> <ul> <li><code>Frankfurt (eu-central-1)</code></li> <li><code>Oregon (us-west-2)</code></li> <li><code>N. Virginia (us-east-1)</code></li> </ul>"},{"location":"ai/integrations/embedding-gemini/#google-gemini-embeddings","title":"Google Gemini Embeddings","text":"<p>The Gemini API provides text embedding models that generate embeddings for words, phrases, sentences, and code. These embeddings enable advanced natural language processing (NLP) tasks such as semantic search, classification, and clustering. By using context-aware embeddings, you can achieve more accurate results than with traditional keyword-based methods.</p>"},{"location":"ai/integrations/embedding-gemini/#supported-models","title":"Supported Models","text":"Model Name Dimensions (recommended) Max Input Tokens Description <code>gemini-embedding-001</code> 128\u20133072 (768, 1536, 3072) 2048 Text and code embeddings <p>For a complete list of supported models and detailed specifications, see the Google Gemini Embeddings Documentation.</p>"},{"location":"ai/integrations/embedding-gemini/#usage-example","title":"Usage example","text":"<p>This example demonstrates creating a vector table, inserting documents, and performing similarity search using Google Gemini embedding models.</p>"},{"location":"ai/integrations/embedding-gemini/#step-1-connect-to-the-database","title":"Step 1: Connect to the database","text":"PythonSQL <pre><code>from pytidb import TiDBClient\n\ntidb_client = TiDBClient.connect(\n    host=\"{gateway-region}.prod.aws.tidbcloud.com\",\n    port=4000,\n    username=\"{prefix}.root\",\n    password=\"{password}\",\n    database=\"{database}\",\n    ensure_db=True,\n)\n</code></pre> <pre><code>mysql -h {gateway-region}.prod.aws.tidbcloud.com \\\n    -P 4000 \\\n    -u {prefix}.root \\\n    -p{password} \\\n    -D {database}\n</code></pre>"},{"location":"ai/integrations/embedding-gemini/#step-2-configure-the-api-key","title":"Step 2: Configure the API key","text":"<p>Create your API key from the Google AI Studio and bring your own key (BYOK) to use the embedding service.</p> PythonSQL <p>Configure the API key for the Google Gemini embedding provider using the TiDB Client:</p> <pre><code>tidb_client.configure_embedding_provider(\n    provider=\"google_gemini\",\n    api_key=\"{your-google-api-key}\",\n)\n</code></pre> <p>Set the API key for the Google Gemini embedding provider using SQL:</p> <pre><code>SET @@GLOBAL.TIDB_EXP_EMBED_GEMINI_API_KEY = \"{your-google-api-key}\";\n</code></pre>"},{"location":"ai/integrations/embedding-gemini/#step-3-create-a-vector-table","title":"Step 3: Create a vector table","text":"<p>Create a table with a vector field that uses the <code>gemini-embedding-001</code> model to generate 3072-dimensional vectors (default):</p> PythonSQL <pre><code>from pytidb.schema import TableModel, Field\nfrom pytidb.embeddings import EmbeddingFunction\nfrom pytidb.datatype import TEXT\n\nclass Document(TableModel):\n    __tablename__ = \"sample_documents\"\n    id: int = Field(primary_key=True)\n    content: str = Field(sa_type=TEXT)\n    embedding: list[float] = EmbeddingFunction(\n        model_name=\"gemini-embedding-001\"\n    ).VectorField(source_field=\"content\")\n\ntable = tidb_client.create_table(schema=Document, if_exists=\"overwrite\")\n</code></pre> <pre><code>CREATE TABLE sample_documents (\n    `id`        INT PRIMARY KEY,\n    `content`   TEXT,\n    `embedding` VECTOR(3072) GENERATED ALWAYS AS (EMBED_TEXT(\n        \"gemini-embedding-001\",\n        `content`\n    )) STORED\n);\n</code></pre>"},{"location":"ai/integrations/embedding-gemini/#step-4-insert-data-into-the-table","title":"Step 4: Insert data into the table","text":"PythonSQL <p>Use the <code>table.insert()</code> or <code>table.bulk_insert()</code> API to add data:</p> <pre><code>documents = [\n    Document(id=1, content=\"Java: Object-oriented language for cross-platform development.\"),\n    Document(id=2, content=\"Java coffee: Bold Indonesian beans with low acidity.\"),\n    Document(id=3, content=\"Java island: Densely populated, home to Jakarta.\"),\n    Document(id=4, content=\"Java's syntax is used in Android apps.\"),\n    Document(id=5, content=\"Dark roast Java beans enhance espresso blends.\"),\n]\ntable.bulk_insert(documents)\n</code></pre> <p>Insert data using the <code>INSERT INTO</code> statement:</p> <pre><code>INSERT INTO sample_documents (id, content)\nVALUES\n    (1, \"Java: Object-oriented language for cross-platform development.\"),\n    (2, \"Java coffee: Bold Indonesian beans with low acidity.\"),\n    (3, \"Java island: Densely populated, home to Jakarta.\"),\n    (4, \"Java's syntax is used in Android apps.\"),\n    (5, \"Dark roast Java beans enhance espresso blends.\");\n</code></pre>"},{"location":"ai/integrations/embedding-gemini/#step-5-search-for-similar-documents","title":"Step 5: Search for similar documents","text":"PythonSQL <p>Use the <code>table.search()</code> API to perform vector search:</p> <pre><code>results = table.search(\"How to start learning Java programming?\") \\\n    .limit(2) \\\n    .to_list()\nprint(results)\n</code></pre> <p>Use the <code>VEC_EMBED_COSINE_DISTANCE</code> function to perform vector search based on cosine distance metric:</p> <pre><code>SELECT\n    `id`,\n    `content`,\n    VEC_EMBED_COSINE_DISTANCE(embedding, \"How to start learning Java programming?\") AS _distance\nFROM sample_documents\nORDER BY _distance ASC\nLIMIT 2;\n</code></pre>"},{"location":"ai/integrations/embedding-gemini/#custom-embedding-dimensions","title":"Custom embedding dimensions","text":"<p>The <code>gemini-embedding-001</code> model supports flexible vector dimensions through Matryoshka Representation Learning (MRL). You can specify the desired dimensions in your embedding function:</p> PythonSQL <pre><code># For 1536 dimensions\nembedding: list[float] = EmbeddingFunction(\n    model_name=\"gemini-embedding-001\",\n    dimensions=1536\n).VectorField(source_field=\"content\")\n\n# For 768 dimensions\nembedding: list[float] = EmbeddingFunction(\n    model_name=\"gemini-embedding-001\", \n    dimensions=768\n).VectorField(source_field=\"content\")\n</code></pre> <pre><code>-- For 1536 dimensions\n`embedding` VECTOR(1536) GENERATED ALWAYS AS (EMBED_TEXT(\n    \"gemini-embedding-001\",\n    `content`,\n    '{\"embedding_config\": {\"output_dimensionality\": 1536}}'\n)) STORED\n\n-- For 768 dimensions  \n`embedding` VECTOR(768) GENERATED ALWAYS AS (EMBED_TEXT(\n    \"gemini-embedding-001\",\n    `content`,\n    '{\"embedding_config\": {\"output_dimensionality\": 768}}'\n)) STORED\n</code></pre> <p>Choose the appropriate dimensions based on your performance requirements and storage constraints. Higher dimensions provide better accuracy but require more storage and computational resources.</p>"},{"location":"ai/integrations/embedding-huggingface/","title":"Integrate TiDB Vector Search with Hugging Face Embeddings","text":"<p>This tutorial demonstrates how to use Hugging Face models to generate text embeddings, store them in TiDB vector storage, and perform semantic search.</p> <p>Info</p> <p>Currently, Server-Side Auto Embedding is only available on TiDB Cloud Starter clusters in the following AWS regions:</p> <ul> <li><code>Frankfurt (eu-central-1)</code></li> <li><code>Oregon (us-west-2)</code></li> <li><code>N. Virginia (us-east-1)</code></li> </ul>"},{"location":"ai/integrations/embedding-huggingface/#hugging-face-embeddings","title":"Hugging Face Embeddings","text":"<p>Hugging Face provides access to a vast collection of pre-trained embedding models through the Hugging Face Hub. You can integrate these models with TiDB using the AI SDK, which enables automatic embedding generation from various transformer-based models.</p>"},{"location":"ai/integrations/embedding-huggingface/#supported-models","title":"Supported Models","text":"<p>Hugging Face supports a wide range of embedding models. Here are some popular examples:</p> Model Name Dimensions Max Input Tokens Description <code>sentence-transformers/all-MiniLM-L6-v2</code> 384 256 Fast, lightweight model for general-purpose embeddings <code>sentence-transformers/all-mpnet-base-v2</code> 768 384 High-quality embeddings with good performance <code>sentence-transformers/all-MiniLM-L12-v2</code> 384 256 Balanced model between speed and quality <code>BAAI/bge-small-en-v1.5</code> 384 512 Multilingual model optimized for semantic search <code>BAAI/bge-base-en-v1.5</code> 768 512 Higher quality multilingual embeddings <code>sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2</code> 384 256 Multilingual model for semantic similarity across languages <code>sentence-transformers/paraphrase-multilingual-mpnet-base-v2</code> 768 384 High-quality multilingual model based on MPNet architecture <code>bert-base-uncased</code> 768 512 Google's BERT base model with 12 layers and 12 attention heads <code>distilbert-base-uncased</code> 768 512 Lightweight BERT model with ~60% fewer parameters, 60% faster inference <p>For a complete list of supported models and detailed specifications, see the Hugging Face Model Hub.</p>"},{"location":"ai/integrations/embedding-huggingface/#usage-example","title":"Usage example","text":"<p>This example demonstrates creating a vector table, inserting documents, and performing similarity search using Hugging Face embedding models.</p>"},{"location":"ai/integrations/embedding-huggingface/#step-1-connect-to-the-database","title":"Step 1: Connect to the database","text":"PythonSQL <pre><code>from pytidb import TiDBClient\n\ntidb_client = TiDBClient.connect(\n    host=\"{gateway-region}.prod.aws.tidbcloud.com\",\n    port=4000,\n    username=\"{prefix}.root\",\n    password=\"{password}\",\n    database=\"{database}\",\n    ensure_db=True,\n)\n</code></pre> <pre><code>mysql -h {gateway-region}.prod.aws.tidbcloud.com \\\n    -P 4000 \\\n    -u {prefix}.root \\\n    -p{password} \\\n    -D {database}\n</code></pre>"},{"location":"ai/integrations/embedding-huggingface/#step-2-configure-the-api-key","title":"Step 2: Configure the API key","text":"<p>If you're using a private model or need higher rate limits, you can configure your Hugging Face API token. You can create your token from the Hugging Face Token Settings page:</p> PythonSQL <p>Configure the API token for Hugging Face models using the TiDB Client:</p> <pre><code>tidb_client.configure_embedding_provider(\n    provider=\"huggingface\",\n    api_key=\"{your-huggingface-token}\",\n)\n</code></pre> <p>Set the API token for Hugging Face models using SQL:</p> <pre><code>SET @@GLOBAL.TIDB_EXP_EMBED_HUGGINGFACE_API_KEY = \"{your-huggingface-token}\";\n</code></pre>"},{"location":"ai/integrations/embedding-huggingface/#step-3-create-a-vector-table","title":"Step 3: Create a vector table","text":"<p>Create a table with a vector field that uses a Hugging Face model to generate embeddings:</p> PythonSQL <pre><code>from pytidb.schema import TableModel, Field\nfrom pytidb.embeddings import EmbeddingFunction\nfrom pytidb.datatype import TEXT\n\nclass Document(TableModel):\n    __tablename__ = \"sample_documents\"\n    id: int = Field(primary_key=True)\n    content: str = Field(sa_type=TEXT)\n    embedding: list[float] = EmbeddingFunction(\n        model_name=\"huggingface/sentence-transformers/all-MiniLM-L6-v2\"\n    ).VectorField(source_field=\"content\")\n\ntable = tidb_client.create_table(schema=Document, if_exists=\"overwrite\")\n</code></pre> <pre><code>CREATE TABLE sample_documents (\n    `id`        INT PRIMARY KEY,\n    `content`   TEXT,\n    `embedding` VECTOR(384) GENERATED ALWAYS AS (EMBED_TEXT(\n        \"huggingface/sentence-transformers/all-MiniLM-L6-v2\",\n        `content`\n    )) STORED\n);\n</code></pre> <p>Tip</p> <p>The vector dimensions depend on the model you choose. For example, <code>huggingface/sentence-transformers/all-MiniLM-L6-v2</code> produces 384-dimensional vectors, while <code>huggingface/sentence-transformers/all-mpnet-base-v2</code> produces 768-dimensional vectors.</p>"},{"location":"ai/integrations/embedding-huggingface/#step-4-insert-data-into-the-table","title":"Step 4: Insert data into the table","text":"PythonSQL <p>Use the <code>table.insert()</code> or <code>table.bulk_insert()</code> API to add data:</p> <pre><code>documents = [\n    Document(id=1, content=\"Machine learning algorithms can identify patterns in data.\"),\n    Document(id=2, content=\"Deep learning uses neural networks with multiple layers.\"),\n    Document(id=3, content=\"Natural language processing helps computers understand text.\"),\n    Document(id=4, content=\"Computer vision enables machines to interpret images.\"),\n    Document(id=5, content=\"Reinforcement learning learns through trial and error.\"),\n]\ntable.bulk_insert(documents)\n</code></pre> <p>Insert data using the <code>INSERT INTO</code> statement:</p> <pre><code>INSERT INTO sample_documents (id, content)\nVALUES\n    (1, \"Machine learning algorithms can identify patterns in data.\"),\n    (2, \"Deep learning uses neural networks with multiple layers.\"),\n    (3, \"Natural language processing helps computers understand text.\"),\n    (4, \"Computer vision enables machines to interpret images.\"),\n    (5, \"Reinforcement learning learns through trial and error.\");\n</code></pre>"},{"location":"ai/integrations/embedding-huggingface/#step-5-search-for-similar-documents","title":"Step 5: Search for similar documents","text":"PythonSQL <p>Use the <code>table.search()</code> API to perform vector search:</p> <pre><code>results = table.search(\"How do neural networks work?\") \\\n    .limit(3) \\\n    .to_list()\n\nfor doc in results:\n    print(f\"ID: {doc.id}, Content: {doc.content}\")\n</code></pre> <p>Use the <code>VEC_EMBED_COSINE_DISTANCE</code> function to perform vector search with cosine distance:</p> <pre><code>SELECT\n    `id`,\n    `content`,\n    VEC_EMBED_COSINE_DISTANCE(embedding, \"How do neural networks work?\") AS _distance\nFROM sample_documents\nORDER BY _distance ASC\nLIMIT 3;\n</code></pre>"},{"location":"ai/integrations/embedding-jinaai/","title":"Integrate TiDB Vector Search with Jina AI Embeddings API","text":"<p>This tutorial demonstrates how to use Jina AI to generate embeddings for text and image data, store them in TiDB vector storage, and perform semantic search.</p> <p>Info</p> <p>Currently, Server-Side Auto Embedding is only available on TiDB Cloud Starter clusters in the following AWS regions:</p> <ul> <li><code>Frankfurt (eu-central-1)</code></li> <li><code>Oregon (us-west-2)</code></li> <li><code>N. Virginia (us-east-1)</code></li> </ul>"},{"location":"ai/integrations/embedding-jinaai/#jina-ai-embeddings","title":"Jina AI Embeddings","text":"<p>Jina AI provides high-performance, multimodal, and multilingual long-context embeddings for search, RAG, and agent applications.</p>"},{"location":"ai/integrations/embedding-jinaai/#supported-models","title":"Supported Models","text":"Model Name Dimensions Max Input Tokens Description <code>jina_ai/jina-embeddings-v4</code> 2048 32,768 Multimodal, multilingual, text and image embeddings <code>jina_ai/jina-clip-v2</code> 1024 8192 Multilingual multimodal embeddings for texts and images <code>jina_ai/jina-embeddings-v3</code> 1024 8192 Multilingual, text and code embeddings <p>For a complete list of supported models and detailed specifications, see the Jina AI Embeddings Documentation.</p>"},{"location":"ai/integrations/embedding-jinaai/#usage-example","title":"Usage example","text":"<p>This example demonstrates creating a vector table, inserting documents, and performing similarity search using Jina AI embedding models.</p>"},{"location":"ai/integrations/embedding-jinaai/#step-1-connect-to-the-database","title":"Step 1: Connect to the database","text":"PythonSQL <pre><code>from pytidb import TiDBClient\n\ntidb_client = TiDBClient.connect(\n    host=\"{gateway-region}.prod.aws.tidbcloud.com\",\n    port=4000,\n    username=\"{prefix}.root\",\n    password=\"{password}\",\n    database=\"{database}\",\n    ensure_db=True,\n)\n</code></pre> <pre><code>mysql -h {gateway-region}.prod.aws.tidbcloud.com \\\n    -P 4000 \\\n    -u {prefix}.root \\\n    -p{password} \\\n    -D {database}\n</code></pre>"},{"location":"ai/integrations/embedding-jinaai/#step-2-configure-the-api-key","title":"Step 2: Configure the API key","text":"<p>Create your API key from the Jina AI Platform and bring your own key (BYOK) to use the embedding service.</p> PythonSQL <p>Configure the API key for the Jina AI embedding provider using the TiDB Client:</p> <pre><code>tidb_client.configure_embedding_provider(\n    provider=\"jina_ai\",\n    api_key=\"{your-jina-api-key}\",\n)\n</code></pre> <p>Set the API key for the Jina AI embedding provider using SQL:</p> <pre><code>SET @@GLOBAL.TIDB_EXP_EMBED_JINA_AI_API_KEY = \"{your-jina-api-key}\";\n</code></pre>"},{"location":"ai/integrations/embedding-jinaai/#step-3-create-a-vector-table","title":"Step 3: Create a vector table","text":"<p>Create a table with a vector field that uses the <code>jina_ai/jina-embeddings-v4</code> model to generate 2048-dimensional vectors:</p> PythonSQL <pre><code>from pytidb.schema import TableModel, Field\nfrom pytidb.embeddings import EmbeddingFunction\nfrom pytidb.datatype import TEXT\n\nclass Document(TableModel):\n    __tablename__ = \"sample_documents\"\n    id: int = Field(primary_key=True)\n    content: str = Field(sa_type=TEXT)\n    embedding: list[float] = EmbeddingFunction(\n        model_name=\"jina_ai/jina-embeddings-v4\"\n    ).VectorField(source_field=\"content\")\n\ntable = tidb_client.create_table(schema=Document, if_exists=\"overwrite\")\n</code></pre> <pre><code>CREATE TABLE sample_documents (\n    `id`        INT PRIMARY KEY,\n    `content`   TEXT,\n    `embedding` VECTOR(2048) GENERATED ALWAYS AS (EMBED_TEXT(\n        \"jina_ai/jina-embeddings-v4\",\n        `content`\n    )) STORED\n);\n</code></pre>"},{"location":"ai/integrations/embedding-jinaai/#step-4-insert-data-into-the-table","title":"Step 4: Insert data into the table","text":"PythonSQL <p>Use the <code>table.insert()</code> or <code>table.bulk_insert()</code> API to add data:</p> <pre><code>documents = [\n    Document(id=1, content=\"Java: Object-oriented language for cross-platform development.\"),\n    Document(id=2, content=\"Java coffee: Bold Indonesian beans with low acidity.\"),\n    Document(id=3, content=\"Java island: Densely populated, home to Jakarta.\"),\n    Document(id=4, content=\"Java's syntax is used in Android apps.\"),\n    Document(id=5, content=\"Dark roast Java beans enhance espresso blends.\"),\n]\ntable.bulk_insert(documents)\n</code></pre> <p>Insert data using the <code>INSERT INTO</code> statement:</p> <pre><code>INSERT INTO sample_documents (id, content)\nVALUES\n    (1, \"Java: Object-oriented language for cross-platform development.\"),\n    (2, \"Java coffee: Bold Indonesian beans with low acidity.\"),\n    (3, \"Java island: Densely populated, home to Jakarta.\"),\n    (4, \"Java's syntax is used in Android apps.\"),\n    (5, \"Dark roast Java beans enhance espresso blends.\");\n</code></pre>"},{"location":"ai/integrations/embedding-jinaai/#step-5-search-for-similar-documents","title":"Step 5: Search for similar documents","text":"PythonSQL <p>Use the <code>table.search()</code> API to perform vector search:</p> <pre><code>results = table.search(\"How to start learning Java programming?\") \\\n    .limit(2) \\\n    .to_list()\nprint(results)\n</code></pre> <p>Use the <code>VEC_EMBED_COSINE_DISTANCE</code> function to perform vector search based on cosine distance metric:</p> <pre><code>SELECT\n    `id`,\n    `content`,\n    VEC_EMBED_COSINE_DISTANCE(embedding, \"How to start learning Java programming?\") AS _distance\nFROM sample_documents\nORDER BY _distance ASC\nLIMIT 2;\n</code></pre>"},{"location":"ai/integrations/embedding-nvidia-nim/","title":"Integrate TiDB Vector Search with NVIDIA NIM Embeddings","text":"<p>This tutorial demonstrates how to use NVIDIA NIM models to generate text embeddings, store them in TiDB vector storage, and perform semantic search.</p> <p>Info</p> <p>Currently, Server-Side Auto Embedding is only available on TiDB Cloud Starter clusters in the following AWS regions:</p> <ul> <li><code>Frankfurt (eu-central-1)</code></li> <li><code>Oregon (us-west-2)</code></li> <li><code>N. Virginia (us-east-1)</code></li> </ul>"},{"location":"ai/integrations/embedding-nvidia-nim/#nvidia-nim-embeddings","title":"NVIDIA NIM Embeddings","text":"<p>NVIDIA NIM\u2122 (NVIDIA Inference Microservices) provides containers to self-host GPU-accelerated inferencing microservices for pretrained and customized AI models across clouds, data centers, and RTX\u2122 AI PCs and workstations. NIM microservices expose industry-standard APIs for simple integration into AI applications, development frameworks, and workflows.</p> <p>You can integrate NVIDIA NIM embedding models with TiDB using the AI SDK, which enables automatic embedding generation from various transformer-based models.</p>"},{"location":"ai/integrations/embedding-nvidia-nim/#supported-models","title":"Supported Models","text":"<p>NVIDIA NIM supports a range of embedding models optimized for different use cases. Here are some popular examples:</p> Model Name Dimensions Max Input Tokens Description <code>nvidia/nv-embed-v1</code> 4096 32k High-quality general-purpose embeddings based on Mistral-7B-v0.1 with Latent-Attention pooling <code>nvidia/llama-3_2-nemoretriever-300m-embed-v1</code> 2048 8192 Multilingual embeddings using Llama 3.2 architecture, supporting 20+ languages and long-context reasoning <p>For a complete list of supported models and detailed specifications, see the NVIDIA Build Platform.</p>"},{"location":"ai/integrations/embedding-nvidia-nim/#usage-example","title":"Usage example","text":"<p>This example demonstrates creating a vector table, inserting documents, and performing similarity search using NVIDIA NIM embedding models.</p>"},{"location":"ai/integrations/embedding-nvidia-nim/#step-1-connect-to-the-database","title":"Step 1: Connect to the database","text":"PythonSQL <pre><code>from pytidb import TiDBClient\n\ntidb_client = TiDBClient.connect(\n    host=\"{gateway-region}.prod.aws.tidbcloud.com\",\n    port=4000,\n    username=\"{prefix}.root\",\n    password=\"{password}\",\n    database=\"{database}\",\n    ensure_db=True,\n)\n</code></pre> <pre><code>mysql -h {gateway-region}.prod.aws.tidbcloud.com \\\n    -P 4000 \\\n    -u {prefix}.root \\\n    -p{password} \\\n    -D {database}\n</code></pre>"},{"location":"ai/integrations/embedding-nvidia-nim/#step-2-configure-the-api-key","title":"Step 2: Configure the API key","text":"<p>If you're using NVIDIA NIM models that require authentication, you can configure your API key. You can get free access to NIM API endpoints through the NVIDIA Developer Program or create your API key from the NVIDIA Build Platform:</p> PythonSQL <p>Configure the API key for NVIDIA NIM models using the TiDB Client:</p> <pre><code>tidb_client.configure_embedding_provider(\n    provider=\"nvidia_nim\",\n    api_key=\"{your-nvidia-api-key}\",\n)\n</code></pre> <p>Set the API key for NVIDIA NIM models using SQL:</p> <pre><code>SET @@GLOBAL.TIDB_EXP_EMBED_NVIDIA_NIM_API_KEY = \"{your-nvidia-api-key}\";\n</code></pre>"},{"location":"ai/integrations/embedding-nvidia-nim/#step-3-create-a-vector-table","title":"Step 3: Create a vector table","text":"<p>Create a table with a vector field that uses an NVIDIA NIM model to generate embeddings:</p> PythonSQL <pre><code>from pytidb.schema import TableModel, Field\nfrom pytidb.embeddings import EmbeddingFunction\nfrom pytidb.datatype import TEXT\n\nclass Document(TableModel):\n    __tablename__ = \"sample_documents\"\n    id: int = Field(primary_key=True)\n    content: str = Field(sa_type=TEXT)\n    embedding: list[float] = EmbeddingFunction(\n        model_name=\"nvidia/nv-embed-v1\"\n    ).VectorField(source_field=\"content\")\n\ntable = tidb_client.create_table(schema=Document, if_exists=\"overwrite\")\n</code></pre> <pre><code>CREATE TABLE sample_documents (\n    `id`        INT PRIMARY KEY,\n    `content`   TEXT,\n    `embedding` VECTOR(4096) GENERATED ALWAYS AS (EMBED_TEXT(\n        \"nvidia/nv-embed-v1\",\n        `content`\n    )) STORED\n);\n</code></pre>"},{"location":"ai/integrations/embedding-nvidia-nim/#step-4-insert-data-into-the-table","title":"Step 4: Insert data into the table","text":"PythonSQL <p>Use the <code>table.insert()</code> or <code>table.bulk_insert()</code> API to add data:</p> <pre><code>documents = [\n    Document(id=1, content=\"Machine learning algorithms can identify patterns in data.\"),\n    Document(id=2, content=\"Deep learning uses neural networks with multiple layers.\"),\n    Document(id=3, content=\"Natural language processing helps computers understand text.\"),\n    Document(id=4, content=\"Computer vision enables machines to interpret images.\"),\n    Document(id=5, content=\"Reinforcement learning learns through trial and error.\"),\n]\ntable.bulk_insert(documents)\n</code></pre> <p>Insert data using the <code>INSERT INTO</code> statement:</p> <pre><code>INSERT INTO sample_documents (id, content)\nVALUES\n    (1, \"Machine learning algorithms can identify patterns in data.\"),\n    (2, \"Deep learning uses neural networks with multiple layers.\"),\n    (3, \"Natural language processing helps computers understand text.\"),\n    (4, \"Computer vision enables machines to interpret images.\"),\n    (5, \"Reinforcement learning learns through trial and error.\");\n</code></pre>"},{"location":"ai/integrations/embedding-nvidia-nim/#step-5-search-for-similar-documents","title":"Step 5: Search for similar documents","text":"PythonSQL <p>Use the <code>table.search()</code> API to perform vector search:</p> <pre><code>results = table.search(\"How do neural networks work?\") \\\n    .limit(3) \\\n    .to_list()\n\nfor doc in results:\n    print(f\"ID: {doc.id}, Content: {doc.content}\")\n</code></pre> <p>Use the <code>VEC_EMBED_COSINE_DISTANCE</code> function to perform vector search with cosine distance:</p> <pre><code>SELECT\n    `id`,\n    `content`,\n    VEC_EMBED_COSINE_DISTANCE(embedding, \"How do neural networks work?\") AS _distance\nFROM sample_documents\nORDER BY _distance ASC\nLIMIT 3;\n</code></pre>"},{"location":"ai/integrations/embedding-openai-compatible/","title":"Integrate TiDB Vector Search with OpenAI-Compatible Embedding API","text":"<p>This tutorial demonstrates how to use OpenAI-compatible embedding services to generate text embeddings, store them in TiDB vector storage, and perform semantic search.</p> <p>Info</p> <p>Currently, TiDB does not support native SQL functions for OpenAI-compatible embedding services, but you can integrate them using the AI SDK.</p>"},{"location":"ai/integrations/embedding-openai-compatible/#openai-like-embedding-services","title":"OpenAI-like embedding services","text":"<p>Because the OpenAI Embedding API is widely used, many AI service providers offer APIs that are compatible with the OpenAI Embedding API, such as:</p> <ul> <li>Ollama</li> <li>vLLM</li> </ul> <p>TiDB AI SDK provides an <code>EmbeddingFunction</code> class that can be used to integrate with OpenAI-compatible embedding services.</p>"},{"location":"ai/integrations/embedding-openai-compatible/#usage-example","title":"Usage example","text":"<p>This example demonstrates creating a vector table, inserting documents, and performing similarity search using OpenAI-compatible embedding models.</p>"},{"location":"ai/integrations/embedding-openai-compatible/#step-1-connect-to-the-database","title":"Step 1: Connect to the database","text":"Python <pre><code>from pytidb import TiDBClient\n\ntidb_client = TiDBClient.connect(\n    host=\"{gateway-region}.prod.aws.tidbcloud.com\",\n    port=4000,\n    username=\"{prefix}.root\",\n    password=\"{password}\",\n    database=\"{database}\",\n    ensure_db=True,\n)\n</code></pre>"},{"location":"ai/integrations/embedding-openai-compatible/#step-2-define-the-embedding-function","title":"Step 2: Define the embedding function","text":"Python <p>To integrate with OpenAI-compatible embedding services, initialize the <code>EmbeddingFunction</code> class and set the <code>model_name</code> parameter with the <code>openai/</code> prefix.</p> <pre><code>from pytidb.embeddings import EmbeddingFunction\n\nopenai_like_embed = EmbeddingFunction(\n    model_name=\"openai/{model_name}\",\n    api_base=\"{your-api-base}\",\n    api_key=\"{your-api-key}\",\n)\n</code></pre> <p>The parameters are:</p> <ul> <li><code>model_name</code>: Specifies the model to use. Use the format <code>openai/{model_name}</code>.</li> <li><code>api_base</code>: The base URL of your OpenAI compatible embedding API service.</li> <li><code>api_key</code>: The API key can be used to authenticate with the embedding API service.</li> </ul> <p>Example: Using Ollama and <code>nomic-embed-text</code> model</p> Python <pre><code>openai_like_embed = EmbeddingFunction(\n    model_name=\"openai/nomic-embed-text\",\n    api_base=\"http://localhost:11434/v1\"\n)\n</code></pre> <p>Example: Using vLLM and <code>nomic-embed-text</code> model</p> Python <pre><code>openai_like_embed = EmbeddingFunction(\n    model_name=\"openai/intfloat/e5-mistral-7b-instruct\",\n    api_base=\"http://localhost:8000/v1\"\n)\n</code></pre>"},{"location":"ai/integrations/embedding-openai-compatible/#step-3-create-a-vector-table","title":"Step 3: Create a vector table","text":"<p>Create a table with a vector field that uses Ollama platform and <code>nomic-embed-text</code> model.</p> Python <pre><code>from pytidb.schema import TableModel, Field\nfrom pytidb.embeddings import EmbeddingFunction\nfrom pytidb.datatype import TEXT\n\nopenai_like_embed = EmbeddingFunction(\n    model_name=\"openai/nomic-embed-text\",\n    api_base=\"{your-api-base}\",\n)\n\nclass Document(TableModel):\n    __tablename__ = \"sample_documents\"\n    id: int = Field(primary_key=True)\n    content: str = Field(sa_type=TEXT)\n    embedding: list[float] = openai_like_embed.VectorField(source_field=\"content\")\n\ntable = tidb_client.create_table(schema=Document, if_exists=\"overwrite\")\n</code></pre>"},{"location":"ai/integrations/embedding-openai-compatible/#step-4-insert-data-into-the-table","title":"Step 4: Insert data into the table","text":"Python <p>Use the <code>table.insert()</code> or <code>table.bulk_insert()</code> API to add data:</p> <pre><code>documents = [\n    Document(id=1, content=\"Java: Object-oriented language for cross-platform development.\"),\n    Document(id=2, content=\"Java coffee: Bold Indonesian beans with low acidity.\"),\n    Document(id=3, content=\"Java island: Densely populated, home to Jakarta.\"),\n    Document(id=4, content=\"Java's syntax is used in Android apps.\"),\n    Document(id=5, content=\"Dark roast Java beans enhance espresso blends.\"),\n]\ntable.bulk_insert(documents)\n</code></pre> <p>With Auto Embedding enabled, TiDB automatically generates vector field values when you insert data.</p>"},{"location":"ai/integrations/embedding-openai-compatible/#step-5-search-for-similar-documents","title":"Step 5: Search for similar documents","text":"Python <p>Use the <code>table.search()</code> API to perform vector search:</p> <pre><code>results = table.search(\"How to start learning Java programming?\") \\\n    .limit(2) \\\n    .to_list()\nprint(results)\n</code></pre> <p>With Auto Embedding enabled, TiDB automatically generates vector embeddings for query text during vector search.</p>"},{"location":"ai/integrations/embedding-openai/","title":"Integrate TiDB Vector Search with OpenAI Embeddings API","text":"<p>This tutorial demonstrates how to use OpenAI to generate text embeddings, store them in TiDB vector storage, and perform semantic search.</p> <p>Info</p> <p>Currently, Auto Embedding is only available on TiDB Cloud Starter clusters in the following AWS regions:</p> <ul> <li>Frankfurt (eu-central-1)</li> <li>Oregon (us-west-2)</li> <li>N. Virginia (us-east-1)</li> </ul>"},{"location":"ai/integrations/embedding-openai/#openai-embeddings","title":"OpenAI Embeddings","text":"<p>OpenAI offers cost-effective, high-performance embedding models. You can integrate the OpenAI Embeddings API with TiDB using the AI SDK or native SQL functions for automatic embedding generation.</p>"},{"location":"ai/integrations/embedding-openai/#supported-models","title":"Supported Models","text":"Model Name Dimensions Max Input Tokens <code>openai/text-embedding-3-small</code> 1536 8191 <code>openai/text-embedding-3-large</code> 3072 8191 <p>For a complete list of supported models, see the OpenAI Embedding API Reference.</p>"},{"location":"ai/integrations/embedding-openai/#usage-example","title":"Usage example","text":"<p>This example demonstrates creating a vector table, inserting documents, and performing similarity search using OpenAI embedding models.</p>"},{"location":"ai/integrations/embedding-openai/#step-1-connect-to-the-database","title":"Step 1: Connect to the database","text":"PythonSQL <pre><code>from pytidb import TiDBClient\n\ntidb_client = TiDBClient.connect(\n    host=\"{gateway-region}.prod.aws.tidbcloud.com\",\n    port=4000,\n    username=\"{prefix}.root\",\n    password=\"{password}\",\n    database=\"{database}\",\n    ensure_db=True,\n)\n</code></pre> <pre><code>mysql -h {gateway-region}.prod.aws.tidbcloud.com \\\n    -P 4000 \\\n    -u {prefix}.root \\\n    -p{password} \\\n    -D {database}\n</code></pre>"},{"location":"ai/integrations/embedding-openai/#step-2-configure-the-api-key","title":"Step 2: Configure the API key","text":"<p>Create your own API key from the OpenAI API Platform and bring your own key (BYOK) to use the embedding service.</p> PythonSQL <p>Configure the API key for the OpenAI embedding provider using the TiDB Client:</p> <pre><code>tidb_client.configure_embedding_provider(\n    provider=\"openai\",\n    api_key=\"{your-openai-api-key}\",\n)\n</code></pre> <p>Set the API key for the OpenAI embedding provider using SQL:</p> <pre><code>SET @@GLOBAL.TIDB_EXP_EMBED_OPENAI_API_KEY = \"{your-openai-api-key}\";\n</code></pre>"},{"location":"ai/integrations/embedding-openai/#step-3-create-a-vector-table","title":"Step 3: Create a vector table","text":"<p>Create a table with a vector field that uses the <code>openai/text-embedding-3-small</code> model to generate 1536-dimensional vectors:</p> PythonSQL <pre><code>from pytidb.schema import TableModel, Field\nfrom pytidb.embeddings import EmbeddingFunction\nfrom pytidb.datatype import TEXT\n\nclass Document(TableModel):\n    __tablename__ = \"sample_documents\"\n    id: int = Field(primary_key=True)\n    content: str = Field(sa_type=TEXT)\n    embedding: list[float] = EmbeddingFunction(\n        model_name=\"openai/text-embedding-3-small\"\n    ).VectorField(source_field=\"content\")\n\ntable = tidb_client.create_table(schema=Document, if_exists=\"overwrite\")\n</code></pre> <pre><code>CREATE TABLE sample_documents (\n    `id`        INT PRIMARY KEY,\n    `content`   TEXT,\n    `embedding` VECTOR(1536) GENERATED ALWAYS AS (EMBED_TEXT(\n        \"openai/text-embedding-3-small\",\n        `content`\n    )) STORED\n);\n</code></pre>"},{"location":"ai/integrations/embedding-openai/#step-4-insert-data-into-the-table","title":"Step 4: Insert data into the table","text":"PythonSQL <p>Use the <code>table.insert()</code> or <code>table.bulk_insert()</code> API to add data:</p> <pre><code>documents = [\n    Document(id=1, content=\"Java: Object-oriented language for cross-platform development.\"),\n    Document(id=2, content=\"Java coffee: Bold Indonesian beans with low acidity.\"),\n    Document(id=3, content=\"Java island: Densely populated, home to Jakarta.\"),\n    Document(id=4, content=\"Java's syntax is used in Android apps.\"),\n    Document(id=5, content=\"Dark roast Java beans enhance espresso blends.\"),\n]\ntable.bulk_insert(documents)\n</code></pre> <p>Insert data using the <code>INSERT INTO</code> statement:</p> <pre><code>INSERT INTO sample_documents (id, content)\nVALUES\n    (1, \"Java: Object-oriented language for cross-platform development.\"),\n    (2, \"Java coffee: Bold Indonesian beans with low acidity.\"),\n    (3, \"Java island: Densely populated, home to Jakarta.\"),\n    (4, \"Java's syntax is used in Android apps.\"),\n    (5, \"Dark roast Java beans enhance espresso blends.\");\n</code></pre>"},{"location":"ai/integrations/embedding-openai/#step-5-search-for-similar-documents","title":"Step 5: Search for similar documents","text":"PythonSQL <p>Use the <code>table.search()</code> API to perform vector search:</p> <pre><code>results = table.search(\"How to start learning Java programming?\") \\\n    .limit(2) \\\n    .to_list()\nprint(results)\n</code></pre> <p>Use the <code>VEC_EMBED_COSINE_DISTANCE</code> function to perform vector search with cosine distance:</p> <pre><code>SELECT\n    `id`,\n    `content`,\n    VEC_EMBED_COSINE_DISTANCE(embedding, \"How to start learning Java programming?\") AS _distance\nFROM sample_documents\nORDER BY _distance ASC\nLIMIT 2;\n</code></pre>"},{"location":"ai/integrations/embedding-overview/","title":"Embeddings Integration","text":""},{"location":"ai/integrations/embedding-overview/#overview","title":"Overview","text":"<p>TiDB provides a unified interface for integrating with various embedding providers and models:</p> <ul> <li>Programmatic use: Use the <code>EmbeddingFunction</code> class from the AI SDK to create embedding functions for specific providers or models.</li> <li>SQL use: Use the <code>EMBED_TEXT</code> function to generate embeddings directly from text data.</li> </ul>"},{"location":"ai/integrations/embedding-overview/#embedding-function","title":"Embedding Function","text":"Python <p>Use the <code>EmbeddingFunction</code> class to work with different embedding providers and models.</p> <pre><code>from pytidb.embeddings import EmbeddingFunction\n\nembed_func = EmbeddingFunction(\n    model_name=\"&lt;provider_name&gt;/&lt;model_name&gt;\",\n)\n</code></pre> <p>Parameters:</p> <ul> <li> <p><code>model_name</code> (required):     Specifies the embedding model to use, in the format <code>{provider_name}/{model_name}</code>.</p> </li> <li> <p><code>dimensions</code> (optional):     The dimensionality of output vector embeddings. If not provided and the model lacks a default dimension, a test string is embedded during initialization to determine the actual dimension automatically.</p> </li> <li> <p><code>api_key</code> (optional):      The API key for accessing the embedding service. If not explicitly set, retrieves the key from the provider's default environment variable.</p> </li> <li> <p><code>api_base</code> (optional):     The base URL of the embedding API service.</p> </li> <li> <p><code>use_server</code> (optional):     Whether to use TiDB Cloud's hosted embedding service. Defaults to <code>True</code> for TiDB Cloud Starter.</p> </li> <li> <p><code>multimodal</code> (optional):     Whether to use a multimodal embedding model. When enabled, <code>use_server</code> is automatically set to <code>False</code>, and the embedding service is called client-side.</p> </li> </ul> SQL <pre><code>SELECT EMBED_TEXT('{model_id}', '{text}', '{extra_params}');\n</code></pre> <p>Parameters:</p> <ul> <li> <p><code>model_id</code> (required):     The ID of the embedding model, in the format <code>{provider_name}/{model_name}</code>, for example, <code>tidbcloud_free/amazon/titan-embed-text-v2</code>.</p> </li> <li> <p><code>text</code> (required):     The text to generate embeddings from.</p> </li> <li> <p><code>extra_params</code> (optional):     Additional parameters sent to the embedding API. Refer to the embedding provider's documentation for supported parameters.</p> </li> </ul>"},{"location":"ai/integrations/embedding-overview/#supported-providers","title":"Supported Providers","text":"<p>The following embedding providers are supported. Click on the corresponding provider to learn how to integrate and enable automatic embedding for your data.</p> <ul> <li>TiDB Cloud Hosted</li> <li>OpenAI</li> <li>OpenAI Compatible </li> <li>Cohere</li> <li>Jina AI</li> <li>Google Gemini</li> <li>Hugging Face</li> <li>NVIDIA NIM</li> </ul>"},{"location":"ai/integrations/embedding-tidb-cloud-hosted/","title":"Integrate TiDB Vector Search with TiDB Cloud Hosted Embedding Models","text":"<p>This tutorial demonstrates how to use TiDB Cloud hosted embedding models to generate embeddings for text data, store them in TiDB vector storage, and perform semantic search.</p> <p>Info</p> <p>Currently, Server-Side Auto Embedding is only available on TiDB Cloud Starter clusters in the following AWS regions:</p> <ul> <li><code>Frankfurt (eu-central-1)</code></li> <li><code>Oregon (us-west-2)</code></li> <li><code>N. Virginia (us-east-1)</code></li> </ul>"},{"location":"ai/integrations/embedding-tidb-cloud-hosted/#tidb-cloud-hosted-embeddings","title":"TiDB Cloud Hosted Embeddings","text":"<p>TiDB Cloud provides hosted embedding models for generating text embeddings without requiring external API keys.</p>"},{"location":"ai/integrations/embedding-tidb-cloud-hosted/#supported-models","title":"Supported Models","text":"<p>TiDB Cloud currently supports the following hosted embedding models:</p> Model Name Dimensions Max Input Tokens Features <code>tidbcloud_free/amazon/titan-embed-text-v2</code> 1536 8192 Text, Multilingual <code>tidbcloud_free/cohere/embed-english-v3</code> 1024 512 Text, English-optimized <code>tidbcloud_free/cohere/embed-multilingual-v3</code> 1024 512 Text, Multilingual <p>Info</p> <p><code>tidbcloud_free</code> prefix models are provided by TiDB Cloud for free.</p>"},{"location":"ai/integrations/embedding-tidb-cloud-hosted/#usage-example","title":"Usage example","text":"<p>This example demonstrates creating a vector table, inserting documents, and performing similarity search using TiDB Cloud hosted embedding models.</p>"},{"location":"ai/integrations/embedding-tidb-cloud-hosted/#step-1-connect-to-the-database","title":"Step 1: Connect to the database","text":"PythonSQL <pre><code>from pytidb import TiDBClient\n\ntidb_client = TiDBClient.connect(\n    host=\"{gateway-region}.prod.aws.tidbcloud.com\",\n    port=4000,\n    username=\"{prefix}.root\",\n    password=\"{password}\",\n    database=\"{database}\",\n    ensure_db=True,\n)\n</code></pre> <pre><code>mysql -h {gateway-region}.prod.aws.tidbcloud.com \\\n    -P 4000 \\\n    -u {prefix}.root \\\n    -p{password} \\\n    -D {database}\n</code></pre>"},{"location":"ai/integrations/embedding-tidb-cloud-hosted/#step-2-create-a-vector-table","title":"Step 2: Create a vector table","text":"<p>Create a table with a vector field that uses the <code>tidbcloud_free/amazon/titan-embed-text-v2</code> model to generate 1536-dimensional vectors:</p> PythonSQL <pre><code>from pytidb.schema import TableModel, Field\nfrom pytidb.embeddings import EmbeddingFunction\nfrom pytidb.datatype import TEXT\n\nclass Document(TableModel):\n    __tablename__ = \"sample_documents\"\n    id: int = Field(primary_key=True)\n    content: str = Field(sa_type=TEXT)\n    embedding: list[float] = EmbeddingFunction(\n        model_name=\"tidbcloud_free/amazon/titan-embed-text-v2\"\n    ).VectorField(source_field=\"content\")\n\ntable = tidb_client.create_table(schema=Document, if_exists=\"overwrite\")\n</code></pre> <pre><code>CREATE TABLE sample_documents (\n    `id`        INT PRIMARY KEY,\n    `content`   TEXT,\n    `embedding` VECTOR(1536) GENERATED ALWAYS AS (EMBED_TEXT(\n        \"tidbcloud_free/amazon/titan-embed-text-v2\",\n        `content`\n    )) STORED\n);\n</code></pre> <p>Info</p> <p><code>tidbcloud_free</code> prefix models is not required to configure the API key.</p>"},{"location":"ai/integrations/embedding-tidb-cloud-hosted/#step-3-insert-data-into-the-table","title":"Step 3: Insert data into the table","text":"PythonSQL <p>Use the <code>table.insert()</code> or <code>table.bulk_insert()</code> API to add data:</p> <pre><code>documents = [\n    Document(id=1, content=\"Java: Object-oriented language for cross-platform development.\"),\n    Document(id=2, content=\"Java coffee: Bold Indonesian beans with low acidity.\"),\n    Document(id=3, content=\"Java island: Densely populated, home to Jakarta.\"),\n    Document(id=4, content=\"Java's syntax is used in Android apps.\"),\n    Document(id=5, content=\"Dark roast Java beans enhance espresso blends.\"),\n]\ntable.bulk_insert(documents)\n</code></pre> <p>Insert data using the <code>INSERT INTO</code> statement:</p> <pre><code>INSERT INTO sample_documents (id, content)\nVALUES\n    (1, \"Java: Object-oriented language for cross-platform development.\"),\n    (2, \"Java coffee: Bold Indonesian beans with low acidity.\"),\n    (3, \"Java island: Densely populated, home to Jakarta.\"),\n    (4, \"Java's syntax is used in Android apps.\"),\n    (5, \"Dark roast Java beans enhance espresso blends.\");\n</code></pre>"},{"location":"ai/integrations/embedding-tidb-cloud-hosted/#step-4-search-for-similar-documents","title":"Step 4: Search for similar documents","text":"PythonSQL <p>Use the <code>table.search()</code> API to perform vector search:</p> <pre><code>results = table.search(\"How to start learning Java programming?\") \\\n    .limit(2) \\\n    .to_list()\nprint(results)\n</code></pre> <p>Use the <code>VEC_EMBED_COSINE_DISTANCE</code> function to perform vector search based on cosine distance metric:</p> <pre><code>SELECT\n    `id`,\n    `content`,\n    VEC_EMBED_COSINE_DISTANCE(embedding, \"How to start learning Java programming?\") AS _distance\nFROM sample_documents\nORDER BY _distance ASC\nLIMIT 2;\n</code></pre>"},{"location":"ai/integrations/langchain/","title":"Integrate Vector Search with LangChain","text":"<p>This tutorial demonstrates how to integrate the vector search feature of TiDB with LangChain.</p> <p>Note</p> <p>The vector search feature is only available for TiDB Self-Managed clusters and TiDB Cloud Starter clusters.</p> <p>Tip</p> <p>You can view the complete sample code on Jupyter Notebook, or run the sample code directly in the Colab online environment.</p>"},{"location":"ai/integrations/langchain/#prerequisites","title":"Prerequisites","text":"<p>To complete this tutorial, you need:</p> <ul> <li>Python 3.8 or higher installed.</li> <li>Jupyter Notebook installed.</li> <li>Git installed.</li> <li>A TiDB cluster.</li> </ul>"},{"location":"ai/integrations/langchain/#get-started","title":"Get started","text":"<p>This section provides step-by-step instructions for integrating TiDB Vector Search with LangChain to perform semantic searches.</p>"},{"location":"ai/integrations/langchain/#step-1-create-a-new-jupyter-notebook-file","title":"Step 1. Create a new Jupyter Notebook file","text":"<p>In your preferred directory, create a new Jupyter Notebook file named <code>integrate_with_langchain.ipynb</code>:</p> <pre><code>touch integrate_with_langchain.ipynb\n</code></pre>"},{"location":"ai/integrations/langchain/#step-2-install-required-dependencies","title":"Step 2. Install required dependencies","text":"<p>In your project directory, run the following command to install the required packages:</p> <pre><code>!pip install langchain langchain-community\n!pip install langchain-openai\n!pip install pymysql\n!pip install tidb-vector\n</code></pre> <p>Open the <code>integrate_with_langchain.ipynb</code> file in Jupyter Notebook, and then add the following code to import the required packages:</p> <pre><code>from langchain_community.document_loaders import TextLoader\nfrom langchain_community.vectorstores import TiDBVectorStore\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import CharacterTextSplitter\n</code></pre>"},{"location":"ai/integrations/langchain/#step-3-set-up-your-environment","title":"Step 3. Set up your environment","text":"<p>Configure the environment variables depending on the TiDB deployment option you've selected.</p> <p>For a TiDB Cloud Starter cluster, take the following steps to obtain the cluster connection string and configure environment variables:</p> <ol> <li> <p>Navigate to the Clusters page, and then click the name of your target cluster to go to its overview page.</p> </li> <li> <p>Click Connect in the upper-right corner. A connection dialog is displayed.</p> </li> <li> <p>Ensure the configurations in the connection dialog match your operating environment.</p> <ul> <li>Connection Type is set to <code>Public</code>.</li> <li>Branch is set to <code>main</code>.</li> <li>Connect With is set to <code>SQLAlchemy</code>.</li> <li>Operating System matches your environment.</li> </ul> </li> <li> <p>Click the PyMySQL tab and copy the connection string.</p> <p>Tip:</p> <p>If you have not set a password yet, click Generate Password to generate a random password.</p> </li> <li> <p>Configure environment variables.</p> <p>This document uses OpenAI as the embedding model provider. In this step, you need to provide the connection string obtained from the previous step and your OpenAI API key.</p> <p>To configure the environment variables, run the following code. You will be prompted to enter your connection string and OpenAI API key:</p> <pre><code># Use getpass to securely prompt for environment variables in your terminal.\nimport getpass\nimport os\n\n# Copy your connection string from the TiDB Cloud console.\n# Connection string format: \"mysql+pymysql://&lt;USER&gt;:&lt;PASSWORD&gt;@&lt;HOST&gt;:4000/&lt;DB&gt;?ssl_ca=/etc/ssl/cert.pem&amp;ssl_verify_cert=true&amp;ssl_verify_identity=true\"\ntidb_connection_string = getpass.getpass(\"TiDB Connection String:\")\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n</code></pre> </li> </ol> <p>This document uses OpenAI as the embedding model provider. In this step, you need to provide the connection string obtained from the previous step and your OpenAI API key.</p> <p>To configure the environment variables, run the following code. You will be prompted to enter your connection string and OpenAI API key:</p> <pre><code># Use getpass to securely prompt for environment variables in your terminal.\nimport getpass\nimport os\n\n# Connection string format: \"mysql+pymysql://&lt;USER&gt;:&lt;PASSWORD&gt;@&lt;HOST&gt;:4000/&lt;DB&gt;?ssl_ca=/etc/ssl/cert.pem&amp;ssl_verify_cert=true&amp;ssl_verify_identity=true\"\ntidb_connection_string = getpass.getpass(\"TiDB Connection String:\")\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n</code></pre> <p>Taking macOS as an example, the cluster connection string is as follows:</p> <pre><code>TIDB_DATABASE_URL=\"mysql+pymysql://&lt;USERNAME&gt;:&lt;PASSWORD&gt;@&lt;HOST&gt;:&lt;PORT&gt;/&lt;DATABASE_NAME&gt;\"\n# For example: TIDB_DATABASE_URL=\"mysql+pymysql://root@127.0.0.1:4000/test\"\n</code></pre> <p>You need to modify the values of the connection parameters according to your TiDB cluster. If you are running TiDB on your local machine, <code>&lt;HOST&gt;</code> is <code>127.0.0.1</code> by default. The initial <code>&lt;PASSWORD&gt;</code> is empty, so if you are starting the cluster for the first time, you can omit this field.</p> <p>The following are descriptions for each parameter:</p> <ul> <li><code>&lt;USERNAME&gt;</code>: The username to connect to the TiDB cluster.</li> <li><code>&lt;PASSWORD&gt;</code>: The password to connect to the TiDB cluster.</li> <li><code>&lt;HOST&gt;</code>: The host of the TiDB cluster.</li> <li><code>&lt;PORT&gt;</code>: The port of the TiDB cluster.</li> <li><code>&lt;DATABASE&gt;</code>: The name of the database you want to connect to.</li> </ul>"},{"location":"ai/integrations/langchain/#step-4-load-the-sample-document","title":"Step 4. Load the sample document","text":""},{"location":"ai/integrations/langchain/#step-41-download-the-sample-document","title":"Step 4.1 Download the sample document","text":"<p>In your project directory, create a directory named <code>data/how_to/</code> and download the sample document <code>state_of_the_union.txt</code> from the langchain-ai/langchain GitHub repository.</p> <pre><code>!mkdir -p 'data/how_to/'\n!wget 'https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/how_to/state_of_the_union.txt' -O 'data/how_to/state_of_the_union.txt'\n</code></pre>"},{"location":"ai/integrations/langchain/#step-42-load-and-split-the-document","title":"Step 4.2 Load and split the document","text":"<p>Load the sample document from <code>data/how_to/state_of_the_union.txt</code> and split it into chunks of approximately 1,000 characters each using a <code>CharacterTextSplitter</code>.</p> <pre><code>loader = TextLoader(\"data/how_to/state_of_the_union.txt\")\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.split_documents(documents)\n</code></pre>"},{"location":"ai/integrations/langchain/#step-5-embed-and-store-document-vectors","title":"Step 5. Embed and store document vectors","text":"<p>TiDB vector store supports both cosine distance (<code>consine</code>) and Euclidean distance (<code>l2</code>) for measuring similarity between vectors. The default strategy is cosine distance.</p> <p>The following code creates a table named <code>embedded_documents</code> in TiDB, which is optimized for vector search.</p> <pre><code>embeddings = OpenAIEmbeddings()\nvector_store = TiDBVectorStore.from_documents(\n    documents=docs,\n    embedding=embeddings,\n    table_name=\"embedded_documents\",\n    connection_string=tidb_connection_string,\n    distance_strategy=\"cosine\",  # default, another option is \"l2\"\n)\n</code></pre> <p>Upon successful execution, you can directly view and access the <code>embedded_documents</code> table in your TiDB database.</p>"},{"location":"ai/integrations/langchain/#step-6-perform-a-vector-search","title":"Step 6. Perform a vector search","text":"<p>This step demonstrates how to query \"What did the president say about Ketanji Brown Jackson\" from the document <code>state_of_the_union.txt</code>.</p> <pre><code>query = \"What did the president say about Ketanji Brown Jackson\"\n</code></pre>"},{"location":"ai/integrations/langchain/#option-1-use-similarity_search_with_score","title":"Option 1: Use <code>similarity_search_with_score()</code>","text":"<p>The <code>similarity_search_with_score()</code> method calculates the vector space distance between the documents and the query. This distance serves as a similarity score, determined by the chosen <code>distance_strategy</code>. The method returns the top <code>k</code> documents with the lowest scores. A lower score indicates a higher similarity between a document and your query.</p> <pre><code>docs_with_score = vector_store.similarity_search_with_score(query, k=3)\nfor doc, score in docs_with_score:\n   print(\"-\" * 80)\n   print(\"Score: \", score)\n   print(doc.page_content)\n   print(\"-\" * 80)\n</code></pre> Expected output <pre><code>--------------------------------------------------------------------------------\nScore:  0.18472413652518527\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections.\n\nTonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nScore:  0.21757513022785557\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system.\n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.\n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.\n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster.\n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nScore:  0.22676987253721725\nAnd for our LGBTQ+ Americans, let\u2019s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong.\n\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential.\n\nWhile it often appears that we never agree, that isn\u2019t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice.\n\nAnd soon, we\u2019ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things.\n\nSo tonight I\u2019m offering a Unity Agenda for the Nation. Four big things we can do together.\n\nFirst, beat the opioid epidemic.\n--------------------------------------------------------------------------------\n</code></pre>"},{"location":"ai/integrations/langchain/#option-2-use-similarity_search_with_relevance_scores","title":"Option 2: Use <code>similarity_search_with_relevance_scores()</code>","text":"<p>The <code>similarity_search_with_relevance_scores()</code> method returns the top <code>k</code> documents with the highest relevance scores. A higher score indicates a higher degree of similarity between a document and your query.</p> <pre><code>docs_with_relevance_score = vector_store.similarity_search_with_relevance_scores(query, k=2)\nfor doc, score in docs_with_relevance_score:\n    print(\"-\" * 80)\n    print(\"Score: \", score)\n    print(doc.page_content)\n    print(\"-\" * 80)\n</code></pre> Expected output <pre><code>--------------------------------------------------------------------------------\nScore:  0.8152758634748147\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections.\n\nTonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nScore:  0.7824248697721444\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system.\n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.\n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.\n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster.\n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n--------------------------------------------------------------------------------\n</code></pre>"},{"location":"ai/integrations/langchain/#use-as-a-retriever","title":"Use as a retriever","text":"<p>In Langchain, a retriever is an interface that retrieves documents in response to an unstructured query, providing more functionality than a vector store. The following code demonstrates how to use TiDB vector store as a retriever.</p> <pre><code>retriever = vector_store.as_retriever(\n   search_type=\"similarity_score_threshold\",\n   search_kwargs={\"k\": 3, \"score_threshold\": 0.8},\n)\ndocs_retrieved = retriever.invoke(query)\nfor doc in docs_retrieved:\n   print(\"-\" * 80)\n   print(doc.page_content)\n   print(\"-\" * 80)\n</code></pre> <p>The expected output is as follows:</p> <pre><code>--------------------------------------------------------------------------------\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections.\n\nTonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n--------------------------------------------------------------------------------\n</code></pre>"},{"location":"ai/integrations/langchain/#remove-the-vector-store","title":"Remove the vector store","text":"<p>To remove an existing TiDB vector store, use the <code>drop_vectorstore()</code> method:</p> <pre><code>vector_store.drop_vectorstore()\n</code></pre>"},{"location":"ai/integrations/langchain/#search-with-metadata-filters","title":"Search with metadata filters","text":"<p>To refine your searches, you can use metadata filters to retrieve specific nearest-neighbor results that match the applied filters.</p>"},{"location":"ai/integrations/langchain/#supported-metadata-types","title":"Supported metadata types","text":"<p>Each document in the TiDB vector store can be paired with metadata, structured as key-value pairs within a JSON object. Keys are always strings, while values can be any of the following types:</p> <ul> <li>String</li> <li>Number: integer or floating point</li> <li>Boolean: <code>true</code> or <code>false</code></li> </ul> <p>For example, the following is a valid metadata payload:</p> <pre><code>{\n  \"page\": 12,\n  \"book_title\": \"Siddhartha\"\n}\n</code></pre>"},{"location":"ai/integrations/langchain/#metadata-filter-syntax","title":"Metadata filter syntax","text":"<p>Available filters include the following:</p> <ul> <li><code>$or</code>: Selects vectors that match any one of the specified conditions.</li> <li><code>$and</code>: Selects vectors that match all the specified conditions.</li> <li><code>$eq</code>: Equal to the specified value.</li> <li><code>$ne</code>: Not equal to the specified value.</li> <li><code>$gt</code>: Greater than the specified value.</li> <li><code>$gte</code>: Greater than or equal to the specified value.</li> <li><code>$lt</code>: Less than the specified value.</li> <li><code>$lte</code>: Less than or equal to the specified value.</li> <li><code>$in</code>: In the specified array of values.</li> <li><code>$nin</code>: Not in the specified array of values.</li> </ul> <p>If the metadata of a document is as follows:</p> <pre><code>{\n  \"page\": 12,\n  \"book_title\": \"Siddhartha\"\n}\n</code></pre> <p>The following metadata filters can match this document:</p> <pre><code>{ \"page\": 12 }\n</code></pre> <pre><code>{ \"page\": { \"$eq\": 12 } }\n</code></pre> <pre><code>{\n  \"page\": {\n    \"$in\": [11, 12, 13]\n  }\n}\n</code></pre> <pre><code>{ \"page\": { \"$nin\": [13] } }\n</code></pre> <pre><code>{ \"page\": { \"$lt\": 11 } }\n</code></pre> <pre><code>{\n  \"$or\": [{ \"page\": 11 }, { \"page\": 12 }],\n  \"$and\": [{ \"page\": 12 }, { \"page\": 13 }]\n}\n</code></pre> <p>In a metadata filter, TiDB treats each key-value pair as a separate filter clause and combines these clauses using the <code>AND</code> logical operator.</p>"},{"location":"ai/integrations/langchain/#example","title":"Example","text":"<p>The following example adds two documents to <code>TiDBVectorStore</code> and adds a <code>title</code> field to each document as the metadata:</p> <pre><code>vector_store.add_texts(\n    texts=[\n        \"TiDB Vector offers advanced, high-speed vector processing capabilities, enhancing AI workflows with efficient data handling and analytics support.\",\n        \"TiDB Vector, starting as low as $10 per month for basic usage\",\n    ],\n    metadatas=[\n        {\"title\": \"TiDB Vector functionality\"},\n        {\"title\": \"TiDB Vector Pricing\"},\n    ],\n)\n</code></pre> <p>The expected output is as follows:</p> <pre><code>[UUID('c782cb02-8eec-45be-a31f-fdb78914f0a7'),\n UUID('08dcd2ba-9f16-4f29-a9b7-18141f8edae3')]\n</code></pre> <p>Perform a similarity search with metadata filters:</p> <pre><code>docs_with_score = vector_store.similarity_search_with_score(\n    \"Introduction to TiDB Vector\", filter={\"title\": \"TiDB Vector functionality\"}, k=4\n)\nfor doc, score in docs_with_score:\n    print(\"-\" * 80)\n    print(\"Score: \", score)\n    print(doc.page_content)\n    print(\"-\" * 80)\n</code></pre> <p>The expected output is as follows:</p> <pre><code>--------------------------------------------------------------------------------\nScore:  0.12761409169211535\nTiDB Vector offers advanced, high-speed vector processing capabilities, enhancing AI workflows with efficient data handling and analytics support.\n--------------------------------------------------------------------------------\n</code></pre>"},{"location":"ai/integrations/langchain/#advanced-usage-example-travel-agent","title":"Advanced usage example: travel agent","text":"<p>This section demonstrates a use case of integrating vector search with Langchain for a travel agent. The goal is to create personalized travel reports for clients, helping them find airports with specific amenities, such as clean lounges and vegetarian options.</p> <p>The process involves two main steps:</p> <ol> <li>Perform a semantic search across airport reviews to identify airport codes that match the desired amenities.</li> <li>Execute a SQL query to merge these codes with route information, highlighting airlines and destinations that align with user's preferences.</li> </ol>"},{"location":"ai/integrations/langchain/#prepare-data","title":"Prepare data","text":"<p>First, create a table to store airport route data:</p> <pre><code># Create a table to store flight plan data.\nvector_store.tidb_vector_client.execute(\n    \"\"\"CREATE TABLE airplan_routes (\n        id INT AUTO_INCREMENT PRIMARY KEY,\n        airport_code VARCHAR(10),\n        airline_code VARCHAR(10),\n        destination_code VARCHAR(10),\n        route_details TEXT,\n        duration TIME,\n        frequency INT,\n        airplane_type VARCHAR(50),\n        price DECIMAL(10, 2),\n        layover TEXT\n    );\"\"\"\n)\n\n# Insert some sample data into airplan_routes and the vector table.\nvector_store.tidb_vector_client.execute(\n    \"\"\"INSERT INTO airplan_routes (\n        airport_code,\n        airline_code,\n        destination_code,\n        route_details,\n        duration,\n        frequency,\n        airplane_type,\n        price,\n        layover\n    ) VALUES\n    ('JFK', 'DL', 'LAX', 'Non-stop from JFK to LAX.', '06:00:00', 5, 'Boeing 777', 299.99, 'None'),\n    ('LAX', 'AA', 'ORD', 'Direct LAX to ORD route.', '04:00:00', 3, 'Airbus A320', 149.99, 'None'),\n    ('EFGH', 'UA', 'SEA', 'Daily flights from SFO to SEA.', '02:30:00', 7, 'Boeing 737', 129.99, 'None');\n    \"\"\"\n)\nvector_store.add_texts(\n    texts=[\n        \"Clean lounges and excellent vegetarian dining options. Highly recommended.\",\n        \"Comfortable seating in lounge areas and diverse food selections, including vegetarian.\",\n        \"Small airport with basic facilities.\",\n    ],\n    metadatas=[\n        {\"airport_code\": \"JFK\"},\n        {\"airport_code\": \"LAX\"},\n        {\"airport_code\": \"EFGH\"},\n    ],\n)\n</code></pre> <p>The expected output is as follows:</p> <pre><code>[UUID('6dab390f-acd9-4c7d-b252-616606fbc89b'),\n UUID('9e811801-0e6b-4893-8886-60f4fb67ce69'),\n UUID('f426747c-0f7b-4c62-97ed-3eeb7c8dd76e')]\n</code></pre>"},{"location":"ai/integrations/langchain/#perform-a-semantic-search","title":"Perform a semantic search","text":"<p>The following code searches for airports with clean facilities and vegetarian options:</p> <pre><code>retriever = vector_store.as_retriever(\n    search_type=\"similarity_score_threshold\",\n    search_kwargs={\"k\": 3, \"score_threshold\": 0.85},\n)\nsemantic_query = \"Could you recommend a US airport with clean lounges and good vegetarian dining options?\"\nreviews = retriever.invoke(semantic_query)\nfor r in reviews:\n    print(\"-\" * 80)\n    print(r.page_content)\n    print(r.metadata)\n    print(\"-\" * 80)\n</code></pre> <p>The expected output is as follows:</p> <pre><code>--------------------------------------------------------------------------------\nClean lounges and excellent vegetarian dining options. Highly recommended.\n{'airport_code': 'JFK'}\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nComfortable seating in lounge areas and diverse food selections, including vegetarian.\n{'airport_code': 'LAX'}\n--------------------------------------------------------------------------------\n</code></pre>"},{"location":"ai/integrations/langchain/#retrieve-detailed-airport-information","title":"Retrieve detailed airport information","text":"<p>Extract airport codes from the search results and query the database for detailed route information:</p> <pre><code># Extracting airport codes from the metadata\nairport_codes = [review.metadata[\"airport_code\"] for review in reviews]\n\n# Executing a query to get the airport details\nsearch_query = \"SELECT * FROM airplan_routes WHERE airport_code IN :codes\"\nparams = {\"codes\": tuple(airport_codes)}\n\nairport_details = vector_store.tidb_vector_client.execute(search_query, params)\nairport_details.get(\"result\")\n</code></pre> <p>The expected output is as follows:</p> <pre><code>[(1, 'JFK', 'DL', 'LAX', 'Non-stop from JFK to LAX.', datetime.timedelta(seconds=21600), 5, 'Boeing 777', Decimal('299.99'), 'None'),\n (2, 'LAX', 'AA', 'ORD', 'Direct LAX to ORD route.', datetime.timedelta(seconds=14400), 3, 'Airbus A320', Decimal('149.99'), 'None')]\n</code></pre>"},{"location":"ai/integrations/langchain/#streamline-the-process","title":"Streamline the process","text":"<p>Alternatively, you can streamline the entire process using a single SQL query:</p> <pre><code>search_query = f\"\"\"\n    SELECT\n        VEC_Cosine_Distance(se.embedding, :query_vector) as distance,\n        ar.*,\n        se.document as airport_review\n    FROM\n        airplan_routes ar\n    JOIN\n        {TABLE_NAME} se ON ar.airport_code = JSON_UNQUOTE(JSON_EXTRACT(se.meta, '$.airport_code'))\n    ORDER BY distance ASC\n    LIMIT 5;\n\"\"\"\nquery_vector = embeddings.embed_query(semantic_query)\nparams = {\"query_vector\": str(query_vector)}\nairport_details = vector_store.tidb_vector_client.execute(search_query, params)\nairport_details.get(\"result\")\n</code></pre> <p>The expected output is as follows:</p> <pre><code>[(0.1219207353407008, 1, 'JFK', 'DL', 'LAX', 'Non-stop from JFK to LAX.', datetime.timedelta(seconds=21600), 5, 'Boeing 777', Decimal('299.99'), 'None', 'Clean lounges and excellent vegetarian dining options. Highly recommended.'),\n (0.14613754359804654, 2, 'LAX', 'AA', 'ORD', 'Direct LAX to ORD route.', datetime.timedelta(seconds=14400), 3, 'Airbus A320', Decimal('149.99'), 'None', 'Comfortable seating in lounge areas and diverse food selections, including vegetarian.'),\n (0.19840519342700513, 3, 'EFGH', 'UA', 'SEA', 'Daily flights from SFO to SEA.', datetime.timedelta(seconds=9000), 7, 'Boeing 737', Decimal('129.99'), 'None', 'Small airport with basic facilities.')]\n</code></pre>"},{"location":"ai/integrations/langchain/#clean-up-data","title":"Clean up data","text":"<p>Finally, clean up the resources by dropping the created table:</p> <pre><code>vector_store.tidb_vector_client.execute(\"DROP TABLE airplan_routes\")\n</code></pre> <p>The expected output is as follows:</p> <pre><code>{'success': True, 'result': 0, 'error': None}\n</code></pre>"},{"location":"ai/integrations/llamaindex/","title":"Integrate Vector Search with LlamaIndex","text":"<p>This tutorial demonstrates how to integrate the vector search feature of TiDB with LlamaIndex.</p> <p>Note</p> <p>The vector search feature is only available for TiDB Self-Managed clusters and TiDB Cloud Starter clusters.</p> <p>Tip</p> <p>You can view the complete sample code on Jupyter Notebook, or run the sample code directly in the Colab online environment.</p>"},{"location":"ai/integrations/llamaindex/#prerequisites","title":"Prerequisites","text":"<p>To complete this tutorial, you need:</p> <ul> <li>Python 3.8 or higher installed.</li> <li>Jupyter Notebook installed.</li> <li>Git installed.</li> <li>A TiDB cluster.</li> </ul>"},{"location":"ai/integrations/llamaindex/#get-started","title":"Get started","text":"<p>This section provides step-by-step instructions for integrating TiDB Vector Search with LlamaIndex to perform semantic searches.</p>"},{"location":"ai/integrations/llamaindex/#step-1-create-a-new-jupyter-notebook-file","title":"Step 1. Create a new Jupyter Notebook file","text":"<p>In the root directory, create a new Jupyter Notebook file named <code>integrate_with_llamaindex.ipynb</code>:</p> <pre><code>touch integrate_with_llamaindex.ipynb\n</code></pre>"},{"location":"ai/integrations/llamaindex/#step-2-install-required-dependencies","title":"Step 2. Install required dependencies","text":"<p>In your project directory, run the following command to install the required packages:</p> <pre><code>pip install llama-index-vector-stores-tidbvector\npip install llama-index\n</code></pre> <p>Open the <code>integrate_with_llamaindex.ipynb</code> file in Jupyter Notebook and add the following code to import the required packages:</p> <pre><code>import textwrap\n\nfrom llama_index.core import SimpleDirectoryReader, StorageContext\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.vector_stores.tidbvector import TiDBVectorStore\n</code></pre>"},{"location":"ai/integrations/llamaindex/#step-3-configure-environment-variables","title":"Step 3. Configure environment variables","text":"<p>Configure the environment variables depending on the TiDB deployment option you've selected.</p> <p>For a TiDB Cloud Starter cluster, take the following steps to obtain the cluster connection string and configure environment variables:</p> <ol> <li> <p>Navigate to the Clusters page, and then click the name of your target cluster to go to its overview page.</p> </li> <li> <p>Click Connect in the upper-right corner. A connection dialog is displayed.</p> </li> <li> <p>Ensure the configurations in the connection dialog match your operating environment.</p> <ul> <li>Connection Type is set to <code>Public</code>.</li> <li>Branch is set to <code>main</code>.</li> <li>Connect With is set to <code>SQLAlchemy</code>.</li> <li>Operating System matches your environment.</li> </ul> </li> <li> <p>Click the PyMySQL tab and copy the connection string.</p> <p>Tip:</p> <p>If you have not set a password yet, click Generate Password to generate a random password.</p> </li> <li> <p>Configure environment variables.</p> <p>This document uses OpenAI as the embedding model provider. In this step, you need to provide the connection string obtained from the previous step and your OpenAI API key.</p> <p>To configure the environment variables, run the following code. You will be prompted to enter your connection string and OpenAI API key:</p> <pre><code># Use getpass to securely prompt for environment variables in your terminal.\nimport getpass\nimport os\n\n# Copy your connection string from the TiDB Cloud console.\n# Connection string format: \"mysql+pymysql://&lt;USER&gt;:&lt;PASSWORD&gt;@&lt;HOST&gt;:4000/&lt;DB&gt;?ssl_ca=/etc/ssl/cert.pem&amp;ssl_verify_cert=true&amp;ssl_verify_identity=true\"\ntidb_connection_string = getpass.getpass(\"TiDB Connection String:\")\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n</code></pre> </li> </ol> <p>This document uses OpenAI as the embedding model provider. In this step, you need to provide the connection string of your TiDB cluster and your OpenAI API key.</p> <p>To configure the environment variables, run the following code. You will be prompted to enter your connection string and OpenAI API key:</p> <pre><code># Use getpass to securely prompt for environment variables in your terminal.\nimport getpass\nimport os\n\n# Connection string format: \"mysql+pymysql://&lt;USER&gt;:&lt;PASSWORD&gt;@&lt;HOST&gt;:4000/&lt;DB&gt;?ssl_ca=/etc/ssl/cert.pem&amp;ssl_verify_cert=true&amp;ssl_verify_identity=true\"\ntidb_connection_string = getpass.getpass(\"TiDB Connection String:\")\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n</code></pre> <p>Taking macOS as an example, the cluster connection string is as follows:</p> <pre><code>TIDB_DATABASE_URL=\"mysql+pymysql://&lt;USERNAME&gt;:&lt;PASSWORD&gt;@&lt;HOST&gt;:&lt;PORT&gt;/&lt;DATABASE_NAME&gt;\"\n# For example: TIDB_DATABASE_URL=\"mysql+pymysql://root@127.0.0.1:4000/test\"\n</code></pre> <p>You need to modify the parameters in the connection string according to your TiDB cluster. If you are running TiDB on your local machine, <code>&lt;HOST&gt;</code> is <code>127.0.0.1</code> by default. The initial <code>&lt;PASSWORD&gt;</code> is empty, so if you are starting the cluster for the first time, you can omit this field.</p> <p>The following are descriptions for each parameter:</p> <ul> <li><code>&lt;USERNAME&gt;</code>: The username to connect to the TiDB cluster.</li> <li><code>&lt;PASSWORD&gt;</code>: The password to connect to the TiDB cluster.</li> <li><code>&lt;HOST&gt;</code>: The host of the TiDB cluster.</li> <li><code>&lt;PORT&gt;</code>: The port of the TiDB cluster.</li> <li><code>&lt;DATABASE&gt;</code>: The name of the database you want to connect to.</li> </ul>"},{"location":"ai/integrations/llamaindex/#step-4-load-the-sample-document","title":"Step 4. Load the sample document","text":""},{"location":"ai/integrations/llamaindex/#step-41-download-the-sample-document","title":"Step 4.1 Download the sample document","text":"<p>In your project directory, create a directory named <code>data/paul_graham/</code> and download the sample document <code>paul_graham_essay.txt</code> from the run-llama/llama_index GitHub repository.</p> <pre><code>!mkdir -p 'data/paul_graham/'\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'\n</code></pre>"},{"location":"ai/integrations/llamaindex/#step-42-load-the-document","title":"Step 4.2 Load the document","text":"<p>Load the sample document from <code>data/paul_graham/paul_graham_essay.txt</code> using the <code>SimpleDirectoryReader</code> class.</p> <pre><code>documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()\nprint(\"Document ID:\", documents[0].doc_id)\n\nfor index, document in enumerate(documents):\n   document.metadata = {\"book\": \"paul_graham\"}\n</code></pre>"},{"location":"ai/integrations/llamaindex/#step-5-embed-and-store-document-vectors","title":"Step 5. Embed and store document vectors","text":""},{"location":"ai/integrations/llamaindex/#step-51-initialize-the-tidb-vector-store","title":"Step 5.1 Initialize the TiDB vector store","text":"<p>The following code creates a table named <code>paul_graham_test</code> in TiDB, which is optimized for vector search.</p> <pre><code>tidbvec = TiDBVectorStore(\n   connection_string=tidb_connection_url,\n   table_name=\"paul_graham_test\",\n   distance_strategy=\"cosine\",\n   vector_dimension=1536,\n   drop_existing_table=False,\n)\n</code></pre> <p>Upon successful execution, you can directly view and access the <code>paul_graham_test</code> table in your TiDB database.</p>"},{"location":"ai/integrations/llamaindex/#step-52-generate-and-store-embeddings","title":"Step 5.2 Generate and store embeddings","text":"<p>The following code parses the documents, generates embeddings, and stores them in the TiDB vector store.</p> <pre><code>storage_context = StorageContext.from_defaults(vector_store=tidbvec)\nindex = VectorStoreIndex.from_documents(\n   documents, storage_context=storage_context, show_progress=True\n)\n</code></pre> <p>The expected output is as follows:</p> <pre><code>Parsing nodes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  8.76it/s]\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 21/21 [00:02&lt;00:00,  8.22it/s]\n</code></pre>"},{"location":"ai/integrations/llamaindex/#step-6-perform-a-vector-search","title":"Step 6. Perform a vector search","text":"<p>The following creates a query engine based on the TiDB vector store and performs a semantic similarity search.</p> <pre><code>query_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do?\")\nprint(textwrap.fill(str(response), 100))\n</code></pre> <p>Note</p> <p><code>TiDBVectorStore</code> only supports the <code>default</code> query mode.</p> <p>The expected output is as follows:</p> <pre><code>The author worked on writing, programming, building microcomputers, giving talks at conferences,\npublishing essays online, developing spam filters, painting, hosting dinner parties, and purchasing\na building for office use.\n</code></pre>"},{"location":"ai/integrations/llamaindex/#step-7-search-with-metadata-filters","title":"Step 7. Search with metadata filters","text":"<p>To refine your searches, you can use metadata filters to retrieve specific nearest-neighbor results that match the applied filters.</p>"},{"location":"ai/integrations/llamaindex/#query-with-book-paul_graham-filter","title":"Query with <code>book != \"paul_graham\"</code> filter","text":"<p>The following example excludes results where the <code>book</code> metadata field is <code>\"paul_graham\"</code>:</p> <pre><code>from llama_index.core.vector_stores.types import (\n   MetadataFilter,\n   MetadataFilters,\n)\n\nquery_engine = index.as_query_engine(\n   filters=MetadataFilters(\n      filters=[\n         MetadataFilter(key=\"book\", value=\"paul_graham\", operator=\"!=\"),\n      ]\n   ),\n   similarity_top_k=2,\n)\nresponse = query_engine.query(\"What did the author learn?\")\nprint(textwrap.fill(str(response), 100))\n</code></pre> <p>The expected output is as follows:</p> <pre><code>Empty Response\n</code></pre>"},{"location":"ai/integrations/llamaindex/#query-with-book-paul_graham-filter_1","title":"Query with <code>book == \"paul_graham\"</code> filter","text":"<p>The following example filters results to include only documents where the <code>book</code> metadata field is <code>\"paul_graham\"</code>:</p> <pre><code>from llama_index.core.vector_stores.types import (\n   MetadataFilter,\n   MetadataFilters,\n)\n\nquery_engine = index.as_query_engine(\n   filters=MetadataFilters(\n      filters=[\n         MetadataFilter(key=\"book\", value=\"paul_graham\", operator=\"==\"),\n      ]\n   ),\n   similarity_top_k=2,\n)\nresponse = query_engine.query(\"What did the author learn?\")\nprint(textwrap.fill(str(response), 100))\n</code></pre> <p>The expected output is as follows:</p> <pre><code>The author learned programming on an IBM 1401 using an early version of Fortran in 9th grade, then\nlater transitioned to working with microcomputers like the TRS-80 and Apple II. Additionally, the\nauthor studied philosophy in college but found it unfulfilling, leading to a switch to studying AI.\nLater on, the author attended art school in both the US and Italy, where they observed a lack of\nsubstantial teaching in the painting department.\n</code></pre>"},{"location":"ai/integrations/llamaindex/#step-8-delete-documents","title":"Step 8. Delete documents","text":"<p>Delete the first document from the index:</p> <pre><code>tidbvec.delete(documents[0].doc_id)\n</code></pre> <p>Check whether the documents had been deleted:</p> <pre><code>query_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author learn?\")\nprint(textwrap.fill(str(response), 100))\n</code></pre> <p>The expected output is as follows:</p> <pre><code>Empty Response\n</code></pre>"},{"location":"ai/integrations/tidb-mcp-claude-desktop/","title":"Integrate TiDB MCP Server with Claude Desktop","text":"<p>This guide shows you how to configure the TiDB MCP Server in Claude Desktop.</p>"},{"location":"ai/integrations/tidb-mcp-claude-desktop/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>Claude Desktop: Download and install Claude Desktop from claude.ai.</li> <li>Python (&gt;=3.10) and uv: Ensure Python (version 3.10 or later) and uv is installed. Follow the installation guide to install uv.</li> <li>A TiDB Cloud Starter Cluster: You can create a free TiDB cluster here tidbcloud.com.</li> </ul>"},{"location":"ai/integrations/tidb-mcp-claude-desktop/#setup-steps","title":"Setup steps","text":"<p>You can follow the steps below to set up the TiDB MCP Server in Claude Desktop:</p> <ol> <li>Open the Settings dialog.</li> <li>Click the Developers tab in the dialog.</li> <li>Click the Edit Config button to open the MCP config file <code>claude_desktop_config.json</code>.</li> <li> <p>Copy the following configuration into the <code>claude_desktop_config.json</code> file.</p> <pre><code>{\n  \"mcpServers\": {\n    \"TiDB\": {\n      \"command\": \"uvx --from pytidb[mcp] tidb-mcp-server\",\n      \"env\": {\n        \"TIDB_HOST\": \"localhost\",\n        \"TIDB_PORT\": \"4000\",\n        \"TIDB_USERNAME\": \"root\",\n        \"TIDB_PASSWORD\": \"\",\n        \"TIDB_DATABASE\": \"test\"\n      }\n    }\n  }\n}\n</code></pre> </li> <li> <p>Go to the TiDB Cloud cluster page and navigate to the cluster you want to connect to.</p> </li> <li>Click the Connect button to get the connection parameters, and replace the <code>TIDB_HOST</code>, <code>TIDB_PORT</code>, <code>TIDB_USERNAME</code>, <code>TIDB_PASSWORD</code>, and <code>TIDB_DATABASE</code> values with your own.</li> <li>Restart Claude Desktop.</li> </ol> <p>For more details, please refer to the quickstart guide to learn how to configure the MCP server in Claude Desktop.</p>"},{"location":"ai/integrations/tidb-mcp-cursor/","title":"Get started with Cursor and TiDB MCP Server","text":"<p>This guide shows you how to configure the TiDB MCP Server in the Cursor editor.</p> <p>For one-click installation, you can click the following button:</p> <p></p>"},{"location":"ai/integrations/tidb-mcp-cursor/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>Cursor Editor: Download and install Cursor from cursor.com.</li> <li>Python (&gt;=3.10) and uv: Ensure Python (version 3.10 or later) and uv is installed. Follow the installation guide to install uv.</li> <li>A TiDB Cloud Starter Cluster: You can create a free TiDB cluster here tidbcloud.com.</li> </ul>"},{"location":"ai/integrations/tidb-mcp-cursor/#setup-steps","title":"Setup steps","text":"<p>You can follow the steps below to set up the TiDB MCP Server in the Cursor editor:</p> <ol> <li>Click the Open Cursor Settings button in the top right corner of the editor.</li> <li>On the Cursor Settings page, click the Tools &amp; Integrations tab.</li> <li>Click the New MCP Server button.</li> <li> <p>Copy the following configuration into the <code>.cursor/mcp.json</code> file.</p> <pre><code>{\n  \"mcpServers\": {\n    \"TiDB\": {\n      \"command\": \"uvx --from pytidb[mcp] tidb-mcp-server\",\n      \"env\": {\n        \"TIDB_HOST\": \"localhost\",\n        \"TIDB_PORT\": \"4000\",\n        \"TIDB_USERNAME\": \"root\",\n        \"TIDB_PASSWORD\": \"\",\n        \"TIDB_DATABASE\": \"test\"\n      }\n    }\n  }\n}\n</code></pre> </li> <li> <p>Go to the TiDB Cloud cluster page and navigate to the cluster you want to connect to.</p> </li> <li>Click the Connect button to get the connection parameters, and replace the <code>TIDB_HOST</code>, <code>TIDB_PORT</code>, <code>TIDB_USERNAME</code>, <code>TIDB_PASSWORD</code>, and <code>TIDB_DATABASE</code> values with your own.</li> </ol> <p>For more details, please refer to the Model Context Protocol documentation to learn how to configure the MCP server in the Cursor editor.</p>"},{"location":"ai/integrations/tidb-mcp-cursor/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues installing the TiDB MCP Server, please check the MCP logs in the Cursor editor.</p> <ol> <li>Click View &gt; Output in the main menu at the top of the editor.</li> <li>Select MCP from the dropdown menu in the Output panel.</li> <li>If you find error messages like <code>[error] Could not start MCP server tidb-mcp-server: Error: spawn uvx ENOENT</code>, it means the <code>uvx</code> command may not exist in your <code>$PATH</code> system variable. For macOS users, you can install <code>uvx</code> by running <code>brew install uv</code>.</li> </ol>"},{"location":"ai/integrations/tidb-mcp-server/","title":"TiDB MCP Server","text":"<p>The TiDB MCP Server is an open-source tool that enables you to interact with TiDB databases using natural language instructions.</p>"},{"location":"ai/integrations/tidb-mcp-server/#understanding-mcp-and-tidb-mcp-server","title":"Understanding MCP and TiDB MCP Server","text":"<p>The Model Context Protocol (MCP) is a protocol standardizes communication between LLMs and external tools.</p> <p>MCP adopts a client-server architecture, allowing a host application to connect to multiple external servers:</p> <ul> <li> <p>Hosts: AI-powered applications, such as Claude Desktop or IDEs like Cursor, that initiate connections to MCP servers.</p> </li> <li> <p>Clients: Components embedded within host applications that establish one-to-one connections with individual MCP servers.</p> </li> <li> <p>Servers: External services, such as the TiDB MCP Server, which provide tools, context, and prompts to clients for interacting with external systems.</p> </li> </ul> <p>The TiDB MCP Server is an MCP-compatible server that provides tools, context to MCP clients for interacting with TiDB databases.</p>"},{"location":"ai/integrations/tidb-mcp-server/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>An MCP-compatible client: For example, Cursor or Claude Desktop.</li> <li>Python (&gt;=3.10) and uv: Ensure Python (version 3.10 or later) and uv is installed. Follow the installation guide to install uv.</li> <li>A TiDB Cloud Starter Cluster: You can create a free TiDB cluster here tidbcloud.com.</li> </ul>"},{"location":"ai/integrations/tidb-mcp-server/#supported-mcp-clients","title":"Supported MCP Clients","text":"<p>Refer to the following guides for detailed examples of using the TiDB MCP Server with specific MCP clients:</p> <ul> <li>Cursor</li> <li>Claude Desktop</li> </ul> <p>If your MCP client is not listed above, please follow the setup steps below.</p>"},{"location":"ai/integrations/tidb-mcp-server/#setup-steps","title":"Setup steps","text":"<p>The TiDB MCP Server supports two modes to integrate with MCP clients:</p> <ul> <li>Standard Input/Output (STDIO) mode (default)</li> <li>Server-Sent Events (SSE) mode</li> </ul> <p>TiDB MCP Server uses STDIO mode by default, which is not required to start up a standalone server in advance.</p> <p>You can choose one of them to set up the TiDB MCP Server in your MCP client.</p>"},{"location":"ai/integrations/tidb-mcp-server/#stdio-mode","title":"STDIO Mode","text":"<p>To set up the TiDB MCP Server in your MCP client using STDIO mode, follow these steps:</p> <ol> <li> <p>Refer to your MCP client\u2019s documentation to learn how to configure the MCP Server.</p> </li> <li> <p>Go to your TiDB Cloud clusters page and navigate to your cluster.</p> </li> <li> <p>Click the Connect button in the cluster page to get the connection parameters.</p> </li> <li> <p>Configure the TiDB MCP Server with your connection parameters in the <code>mcpServers</code> section of your AI application\u2019s configuration file.</p> <p>Example MCP configuration file:</p> <pre><code>{\n  \"mcpServers\": {\n    \"TiDB\": {\n      \"command\": \"uvx --from pytidb[mcp] tidb-mcp-server\",\n      \"env\": {\n        \"TIDB_HOST\": \"localhost\",\n        \"TIDB_PORT\": \"4000\",\n        \"TIDB_USERNAME\": \"root\",\n        \"TIDB_PASSWORD\": \"\",\n        \"TIDB_DATABASE\": \"test\"\n      }\n    }\n  }\n}\n</code></pre> </li> </ol>"},{"location":"ai/integrations/tidb-mcp-server/#server-sent-events-sse-mode","title":"Server-Sent Events (SSE) Mode","text":"<p>To set up the TiDB MCP Server in your MCP client using SSE mode, follow these steps:</p> <ol> <li> <p>Refer to your MCP client\u2019s documentation to learn how to configure the MCP Server.</p> </li> <li> <p>Go to your TiDB Cloud clusters page and navigate to your cluster.</p> </li> <li> <p>Click the Connect button in the cluster page to get the connection parameters.</p> </li> <li> <p>Create a <code>.env</code> file with your own connection parameters.</p> <p>Example <code>.env</code> file:</p> <pre><code>cat &gt; .env &lt;&lt;EOF\nTIDB_HOST={gateway-region}.prod.aws.tidbcloud.com\nTIDB_PORT=4000\nTIDB_USERNAME={prefix}.root\nTIDB_PASSWORD={password}\nTIDB_DATABASE=test\nEOF\n</code></pre> </li> <li> <p>Start the TiDB MCP Server with the <code>--transport sse</code> option.</p> <pre><code>uvx --from \"pytidb[mcp]\" tidb-mcp-server --transport sse\n</code></pre> </li> <li> <p>Add the <code>TiDB</code> MCP server configuration to the <code>mcpServers</code> section of your AI application\u2019s configuration file.</p> <pre><code>{\n  \"mcpServers\": {\n    \"TiDB\": {\n      \"url\": \"http://localhost:8000/sse\"\n    }\n  }\n}\n</code></pre> </li> </ol>"},{"location":"ai/integrations/tidb-mcp-server/#supported-actions-tools","title":"Supported actions (tools)","text":"<p>The TiDB MCP Server provides the following actions (tools) to MCP clients. You can use these tools to interact with your TiDB projects and databases using natural language instructions.</p> <p>Database Management</p> <ul> <li> <p><code>show_databases</code> - Show all databases in the TiDB cluster</p> <ul> <li><code>username</code>: Database username (string, optional)</li> <li><code>password</code>: Database password (string, optional)</li> </ul> </li> <li> <p><code>switch_database</code> - Switch to a specific database</p> <ul> <li><code>db_name</code>: Database name to switch to (string, required)</li> <li><code>username</code>: Database username (string, optional)</li> <li><code>password</code>: Database password (string, optional)</li> </ul> </li> <li> <p><code>show_tables</code> - Show all tables in the current database</p> </li> </ul> <p>SQL query and execution</p> <ul> <li> <p><code>db_query</code> - Execute read-only SQL queries</p> <ul> <li><code>sql_stmt</code>: SQL query statement (string, required)</li> </ul> </li> <li> <p><code>db_execute</code> - Execute data modification SQL statements</p> <ul> <li><code>sql_stmts</code>: A single SQL statement or an array of SQL statements (string or array, required)</li> </ul> </li> </ul> <p>User Management</p> <ul> <li> <p><code>db_create_user</code> - Create a new database user</p> <ul> <li><code>username</code>: Name for the new user (string, required)</li> <li><code>password</code>: Password for the new user (string, required)</li> </ul> </li> <li> <p><code>db_remove_user</code> - Remove an existing database user</p> <ul> <li><code>username</code>: Name of the user to remove (string, required)</li> </ul> </li> </ul>"}]}